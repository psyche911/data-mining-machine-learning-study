{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e6e5dd2",
   "metadata": {},
   "source": [
    "### Neural networks for time series and regression\n",
    "- Let's start with the implementation of a neural network. Note that the implementation of the forward and backward pass remains the same. You used the same code for classification problems in the previous lessons and you will find that some of the variables report the classification performance. This is a regression problem. You can ignore those variables and take note of the RMSE. \n",
    "\n",
    "- The below code helps you to predict the performance. You can see the prediction graphs discussed in the previous lesson(for Sunspot, Lazer and Mackey-Glass).\n",
    "\n",
    "- Challenge: Run the code and show results for different problems and compare the results with scikit-learn implementation given in the previous lesson. You can post your results in a discussion thread to discuss the results with your peers. Note that you should use the same training time. You can also change the number of hidden neurons. You can add more hidden layers in the scikit-learn version and evaluate the prediction performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3164742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sources: https://github.com/rohitash-chandra\n",
    "# http://iamtrask.github.io/2015/07/12/basic-python-network/  \n",
    " \n",
    "# Sigmoid units used in hidden and output  \n",
    "# Numpy: http://cs231n.github.io/python-numpy-tutorial/#numpy-arrays\n",
    " \n",
    "# This version will demonstrate momemntum and stocastic gradient descent\n",
    "# FNN for Time Series prediction and Regression\n",
    " \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import random\n",
    "import time\n",
    " \n",
    "class Network:\n",
    "\n",
    "    def __init__(self, Topo, Train, Test, MaxTime, MinPer, learnRate, use_stocasticGD, use_vanillalearning, momentum_rate): \n",
    "        self.Top = Topo      # NN topology [input, hidden, output]\n",
    "        self.Max = MaxTime   # Max epocs\n",
    "        self.TrainData = Train\n",
    "        self.TestData = Test\n",
    "        self.NumSamples = Train.shape[0]\n",
    "\n",
    "        self.learn_rate = learnRate\n",
    "        self.minPerf = MinPer\n",
    "\n",
    "        #Initialise weights (W1 W2) and bias (b1 b2) of the network\n",
    "        np.random.seed() \n",
    "        self.W1 = np.random.uniform(-0.5, 0.5, (self.Top[0], self.Top[1]))  \n",
    "        #Print(self.W1,  ' self.W1')\n",
    "        self.B1 = np.random.uniform(-0.5, 0.5, (1, self.Top[1])) # bias first layer\n",
    "        #Print(self.B1, ' self.B1')\n",
    "        self.BestB1 = self.B1\n",
    "        self.BestW1 = self.W1 \n",
    "        self.W2 = np.random.uniform(-0.5, 0.5, (self.Top[1], self.Top[2]))   \n",
    "        self.B2 = np.random.uniform(-0.5,0.5, (1,self.Top[2]))   # bias second layer\n",
    "        self.BestB2 = self.B2\n",
    "        self.BestW2 = self.W2 \n",
    "        self.hidout = np.zeros(self.Top[1])  # output of first hidden layer\n",
    "        self.out = np.zeros(self.Top[2])      # output last layer\n",
    "\n",
    "        self.hid_delta = np.zeros(self.Top[1]) # output of first hidden layer\n",
    "        self.out_delta = np.zeros(self.Top[2])  # output last layer\n",
    "\n",
    "        self.vanilla = use_vanillalearning    # canonical batch training mode - use full data set - no SGD  \n",
    "\n",
    "        self.momenRate = momentum_rate\n",
    "\n",
    "        self.stocasticGD = use_stocasticGD\n",
    "        \n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "    def softmax(self, x):\n",
    "        # Numerically stable with large exponentials\n",
    "        exps = np.exp(x - x.max())\n",
    "        return exps / np.sum(exps, axis=0)\n",
    "    \n",
    "\n",
    "    def sampleEr(self, actualout):\n",
    "        error = np.subtract(self.out, actualout)\n",
    "        sqerror = np.sum(np.square(error))/self.Top[2] \n",
    "\n",
    "        return sqerror\n",
    "    \n",
    "\n",
    "    def ForwardPass(self, X ): \n",
    "        z1 = X.dot(self.W1) - self.B1  \n",
    "        self.hidout = self.sigmoid(z1) # output of first hidden layer   \n",
    "        z2 = self.hidout.dot(self.W2)-self.B2 \n",
    "        self.out = self.sigmoid(z2)   # output second hidden layer\n",
    "\n",
    "\n",
    "    def BackwardPass(self, input_vec, desired):   \n",
    "        out_delta = (desired - self.out)*(self.out*(1-self.out))  \n",
    "        hid_delta = out_delta.dot(self.W2.T) * (self.hidout * (1-self.hidout)) \n",
    "        # https://www.tutorialspoint.com/numpy/numpy_dot.htm  https://www.geeksforgeeks.org/numpy-dot-python/\n",
    "\n",
    "        if self.vanilla == True: #No momentum \n",
    "            self.W2 += self.hidout.T.dot(out_delta) * self.learn_rate\n",
    "            self.B2 += (-1 * self.learn_rate * out_delta)\n",
    "\n",
    "            self.W1 += (input_vec.T.dot(hid_delta) * self.learn_rate) \n",
    "            self.B1 += (-1 * self.learn_rate * hid_delta) \n",
    "        else:     # Use momentum\n",
    "            v2 = self.W2.copy()   # Save previous weights http://cs231n.github.io/neural-networks-3/#sgd\n",
    "            v1 = self.W1.copy()\n",
    "            b2 = self.B2.copy()\n",
    "            b1 = self.B1.copy()\n",
    "\n",
    "            self.W2 += (v2 *self.momenRate) + (self.hidout.T.dot(out_delta) * self.learn_rate)  # velocity update\n",
    "            self.W1 += (v1 *self.momenRate) + (input_vec.T.dot(hid_delta) * self.learn_rate)   \n",
    "            self.B2 += (b2 *self.momenRate) + (-1 * self.learn_rate * out_delta)     # velocity update\n",
    "            self.B1 += (b1 *self.momenRate) + (-1 * self.learn_rate * hid_delta)   \n",
    "\n",
    "\n",
    "    def TestNetworkRegression(self, Data,  erTolerance):\n",
    "        Input = np.zeros((1, self.Top[0]))    # Temp hold input\n",
    "        Desired = np.zeros((1, self.Top[2])) \n",
    "        nOutput = np.zeros((1, self.Top[2]))\n",
    "        testSize = Data.shape[0] \n",
    "        sse = 0  \n",
    "        Input = np.zeros((1, self.Top[0]))    # Temp hold input\n",
    "\n",
    "        predicted = np.zeros(testSize)\n",
    "\n",
    "        self.W1 = self.BestW1\n",
    "        self.W2 = self.BestW2    # Load best knowledge\n",
    "        self.B1 = self.BestB1\n",
    "        self.B2 = self.BestB2    # Load best knowledge\n",
    "\n",
    "        for s in range(0, testSize):\n",
    "            Input[:] = Data[s, 0:self.Top[0]] \n",
    "            Desired[:] = Data[s, self.Top[0]:] \n",
    "\n",
    "            self.ForwardPass(Input) \n",
    "            predicted[s] = self.out\n",
    "\n",
    "            sse += self.sampleEr(Desired)   \n",
    "\n",
    "        actual = Data[:, self.Top[0]:]\n",
    "        \n",
    "        return np.sqrt(sse/testSize), actual, predicted\n",
    "\n",
    "\n",
    "    def saveKnowledge(self):\n",
    "        self.BestW1 = self.W1\n",
    "        self.BestW2 = self.W2\n",
    "        self.BestB1 = self.B1\n",
    "        self.BestB2 = self.B2 \n",
    "\n",
    "        #Print (self.BestW1, self.BestW2, self.BestB1, self.BestB2)\n",
    "        \n",
    "\n",
    "    def BP_GD(self, trainTolerance):  \n",
    "        Input = np.zeros((1, self.Top[0]))  # Temp hold input\n",
    "        Desired = np.zeros((1, self.Top[2])) \n",
    "\n",
    "        #minibatchsize = int(0.1* self.TrainData.shape[0]) \n",
    "        # Choose a mini-batch size for SGD - optional exercise to implement this\n",
    "\n",
    "        Er = [] \n",
    "        epoch = 0\n",
    "        bestRMSE = 10000    # Assign a large number in begining to maintain best (lowest RMSE)\n",
    "        bestTrain = 0\n",
    "        while  epoch < self.Max and bestTrain < self.minPerf:\n",
    "            sse = 0\n",
    "\n",
    "            for s in range(0, self.TrainData.shape[0]):\n",
    "                if(self.stocasticGD==True):   \n",
    "                    pat = random.randint(0, self.TrainData.shape[0]-1) \n",
    "                    # Data shuffle in SGD:\n",
    "                    # https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.random.randint.html\n",
    "                else:\n",
    "                    pat = s # no data shuffle in SGD\n",
    "\n",
    "                Input[:] = self.TrainData[pat, 0:self.Top[0]]  \n",
    "                Desired[:] = self.TrainData[pat, self.Top[0]:]  \n",
    "\n",
    "                self.ForwardPass(Input)  \n",
    "                self.BackwardPass(Input, Desired)\n",
    "                sse += self.sampleEr(Desired)\n",
    "\n",
    "            rmse = np.sqrt(sse/self.TrainData.shape[0]*self.Top[2])\n",
    "\n",
    "            if rmse < bestRMSE:\n",
    "                 bestRMSE = rmse\n",
    "                 self.saveKnowledge() \n",
    "                 bestRMSE, actual, predicted = self.TestNetworkRegression(self.TrainData, trainTolerance)\n",
    "\n",
    "            Er = np.append(Er, rmse)\n",
    "\n",
    "            epoch += 1\n",
    "\n",
    "        return (Er, bestRMSE, bestTrain, epoch, predicted) \n",
    "\n",
    "    def normalisedata(data, inputsize, outsize):   # Normalise the data between [0,1]\n",
    "        traindt = data[:,np.array(range(0,inputsize))]\n",
    "        dt = np.amax(traindt, axis=0)\n",
    "        tds = abs(traindt/dt) \n",
    "        \n",
    "        return np.concatenate((tds[:,range(0,inputsize)], data[:,range(inputsize,inputsize+outsize)]), axis=1)\n",
    "\n",
    "    \n",
    "    def main():\n",
    "        problem = 3 # [1,2,3] choose your problem\n",
    "        \n",
    "        if problem == 1:\n",
    "            TrainData = np.loadtxt(\"data/Sunspot/train.txt\") \n",
    "            TestData = np.loadtxt(\"data/Sunspot/test.txt\") \n",
    "            Hidden = 5\n",
    "            Input = 4\n",
    "            Output = 1 \n",
    "            MaxTime = 1000\n",
    "\n",
    "        elif problem == 2:\n",
    "            TrainData = np.loadtxt(\"data/Lazer/train.txt\") \n",
    "            TestData    = np.loadtxt(\"data/Lazer/test.txt\") \n",
    "            Hidden = 5\n",
    "            Input = 4\n",
    "            Output = 1 \n",
    "            MaxTime = 1000\n",
    "            \n",
    "        elif problem == 3:\n",
    "            TrainData = np.loadtxt(\"data/Mackey/train.txt\") \n",
    "            TestData = np.loadtxt(\"data/Mackey/test.txt\") \n",
    "            Hidden = 5\n",
    "            Input = 4\n",
    "            Output = 1 \n",
    "            MaxTime = 1000\n",
    "            \n",
    "        MinCriteria = 100 \n",
    "        #Stop when learn 100 percent - to ensure it does not stop (does not apply for time series - regression problems)\n",
    "\n",
    "        Topo = [Input, Hidden, Output] \n",
    "        MaxRun = 5    # Number of experimental runs \n",
    "        \n",
    "        trainTolerance = 0.2      # [eg 0.15 would be seen as 0] [ 0.81 would be seen as 1]\n",
    "        testTolerance = 0.49\n",
    "\n",
    "        learnRate = 0.1\n",
    "        useStocastic = True   # False for vanilla BP with SGD (no shuffle of data).\n",
    "                              # True for BP with SGD (shuffle of data at every epoch)\n",
    "        updateStyle = True    # True for Vanilla SGD, False for momentum SGD\n",
    "        momentum_rate = 0.001   # 0.1 ends up having very large weights\n",
    "\n",
    "        trainRMSE = np.zeros(MaxRun)\n",
    "        testRMSE = np.zeros(MaxRun)\n",
    "        Epochs = np.zeros(MaxRun)\n",
    "        Time = np.zeros(MaxRun)\n",
    "\n",
    "        for run in range(0, MaxRun  ):\n",
    "            print(run, ' is experimental run') \n",
    "\n",
    "            fnn = Network(Topo, TrainData, TestData, MaxTime, MinCriteria, \n",
    "                          learnRate, useStocastic, updateStyle, momentum_rate)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            (erEp, trainRMSE[run], Epochs[run], actual, predicted) = fnn.BP_GD(trainTolerance)   \n",
    "\n",
    "            Time[run] = time.time() - start_time\n",
    "            (testRMSE[run], actual, predicted) = fnn.TestNetworkRegression(TestData, testTolerance)\n",
    "\n",
    "        print('RMSE performance for each experimental run') \n",
    "        print(trainRMSE)\n",
    "        print(testRMSE)\n",
    "        \n",
    "        print('Epocs and Time taken for each experimental run') \n",
    "        print(Epochs)\n",
    "        print(Time)\n",
    "        \n",
    "        print('mean and std of classification performance') \n",
    "        print(np.mean(trainRMSE), np.std(trainRMSE))\n",
    "        print(np.mean(testRMSE), np.std(testRMSE))\n",
    "\n",
    "        print('mean and std of computational time taken') \n",
    "        print(np.mean(Time), np.std(Time))\n",
    "\n",
    "        # fig of last run\n",
    "        plt.figure()\n",
    "        plt.plot(erEp)\n",
    "        plt.ylabel('error')  \n",
    "        plt.savefig('figures/out.png')\n",
    "        plt.clf()\n",
    "\n",
    "        y_train = np.linspace(0, actual.shape[0], num=actual.shape[0])\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(y_train, actual, label='actual')\n",
    "        plt.plot(y_train, predicted, label='predicted')\n",
    "        plt.ylabel('RMSE')  \n",
    "        plt.xlabel('Time (samples)')  \n",
    "        plt.savefig('figures/pred_timeseries'+str(problem)+'.png')\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\": \n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
