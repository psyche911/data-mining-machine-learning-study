{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1286f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance:  90.78341013824885\n",
      "Confusion Matrix: \n",
      " [[99 11]\n",
      " [ 9 98]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "def I(flag):\n",
    "    return 1 if flag else 0\n",
    "\n",
    "def sign(x):\n",
    "    return abs(x)/x if x!=0 else 1       \n",
    "\n",
    "class AdaBoost:\n",
    "    \n",
    "    def __init__(self,n_estimators=50):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.models = [None]*n_estimators\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        \n",
    "        X = np.float64(X)\n",
    "        N = len(y)\n",
    "        w = np.array([1/N for i in range(N)])\n",
    "        \n",
    "        for m in range(self.n_estimators):\n",
    "            \n",
    "            Gm = DecisionTreeClassifier(max_depth=1).fit(X,y,sample_weight=w).predict\n",
    "                        \n",
    "            errM = sum([w[i]*I(y[i]!=Gm(X[i].reshape(1,-1))) for i in range(N)])/sum(w)\n",
    " \n",
    "            AlphaM = np.log((1-errM)/errM)\n",
    "            \n",
    "            w = [w[i]*np.exp(AlphaM*I(y[i]!=Gm(X[i].reshape(1,-1)))) for i in range(N)] \n",
    "            \n",
    "            \n",
    "            self.models[m] = (AlphaM,Gm)\n",
    "\n",
    "    def predict(self,X):\n",
    "        \n",
    "        y = 0\n",
    "        for m in range(self.n_estimators):\n",
    "            AlphaM,Gm = self.models[m]\n",
    "            y += AlphaM*Gm(X)\n",
    "        signA = np.vectorize(sign)\n",
    "        y = np.where(signA(y)==-1,-1,1)\n",
    "        return y\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import confusion_matrix as CM\n",
    "\n",
    "x,y = make_classification(n_samples=217)\n",
    "'''\n",
    "As for our implementaion of AdaBoost \n",
    "y needs to be in {-1,1}\n",
    "'''\n",
    "y = np.where(y==0,-1,1)\n",
    "\n",
    "clf = AdaBoost(n_estimators=5) # try 5 10 50 and press Run over and over again\n",
    "clf.fit(x,y)\n",
    "y_pred = clf.predict(x)\n",
    "\n",
    "\n",
    "print(\"Performance: \", 100*sum(y_pred==y)/len(y))\n",
    "print(\"Confusion Matrix: \\n\", CM(y,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfe126db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regression  is our problem\n",
      "0  is our exp run\n",
      "0  is our exp run\n",
      "|--- feature_0 <= 0.75\n",
      "|   |--- feature_6 <= 0.18\n",
      "|   |   |--- value: [11.32]\n",
      "|   |--- feature_6 >  0.18\n",
      "|   |   |--- value: [14.82]\n",
      "|--- feature_0 >  0.75\n",
      "|   |--- feature_1 <= 624.75\n",
      "|   |   |--- value: [28.77]\n",
      "|   |--- feature_1 >  624.75\n",
      "|   |   |--- value: [37.14]\n",
      "\n",
      "0  is our exp run\n",
      "0  is our exp run\n",
      "0  is our exp run\n",
      "1  is our exp run\n",
      "1  is our exp run\n",
      "|--- feature_1 <= 673.75\n",
      "|   |--- feature_1 <= 624.75\n",
      "|   |   |--- value: [28.51]\n",
      "|   |--- feature_1 >  624.75\n",
      "|   |   |--- value: [37.22]\n",
      "|--- feature_1 >  673.75\n",
      "|   |--- feature_2 <= 330.75\n",
      "|   |   |--- value: [12.12]\n",
      "|   |--- feature_2 >  330.75\n",
      "|   |   |--- value: [15.55]\n",
      "\n",
      "1  is our exp run\n",
      "1  is our exp run\n",
      "1  is our exp run\n",
      "2  is our exp run\n",
      "2  is our exp run\n",
      "|--- feature_1 <= 673.75\n",
      "|   |--- feature_1 <= 624.75\n",
      "|   |   |--- value: [28.57]\n",
      "|   |--- feature_1 >  624.75\n",
      "|   |   |--- value: [37.18]\n",
      "|--- feature_1 >  673.75\n",
      "|   |--- feature_6 <= 0.18\n",
      "|   |   |--- value: [11.08]\n",
      "|   |--- feature_6 >  0.18\n",
      "|   |   |--- value: [14.58]\n",
      "\n",
      "2  is our exp run\n",
      "2  is our exp run\n",
      "2  is our exp run\n",
      "3  is our exp run\n",
      "3  is our exp run\n",
      "|--- feature_3 <= 183.75\n",
      "|   |--- feature_1 <= 624.75\n",
      "|   |   |--- value: [28.30]\n",
      "|   |--- feature_1 >  624.75\n",
      "|   |   |--- value: [36.21]\n",
      "|--- feature_3 >  183.75\n",
      "|   |--- feature_6 <= 0.18\n",
      "|   |   |--- value: [11.15]\n",
      "|   |--- feature_6 >  0.18\n",
      "|   |   |--- value: [14.57]\n",
      "\n",
      "3  is our exp run\n",
      "3  is our exp run\n",
      "3  is our exp run\n",
      "4  is our exp run\n",
      "4  is our exp run\n",
      "|--- feature_3 <= 183.75\n",
      "|   |--- feature_0 <= 0.81\n",
      "|   |   |--- value: [37.54]\n",
      "|   |--- feature_0 >  0.81\n",
      "|   |   |--- value: [28.43]\n",
      "|--- feature_3 >  183.75\n",
      "|   |--- feature_6 <= 0.18\n",
      "|   |   |--- value: [11.09]\n",
      "|   |--- feature_6 >  0.18\n",
      "|   |   |--- value: [14.53]\n",
      "\n",
      "4  is our exp run\n",
      "4  is our exp run\n",
      "4  is our exp run\n",
      "[3.93795227 5.25462052 4.08233133 4.20982183 4.14286469]  nn_all\n",
      "4.325518127887034  mean nn_all\n",
      "0.47314401388089583  std nn_all\n",
      "[3.60336885 3.52448101 3.42211998 3.19720914 3.37391663]  tree_all\n",
      "3.4242191221768494  tree _all\n",
      "0.13871939041164552  tree _all\n",
      "[3.51001022 3.37307022 3.36895941 3.10321828 3.2835344 ] 8  forest_all\n",
      "3.3277585073203797  forest _all\n",
      "0.13367156565544347  forest _all\n",
      "[1.98423879 1.79019302 1.78146875 1.76467151 1.56630548] adaboost_all\n",
      "1.7773755109472138  adaboost _all\n",
      "0.1324276482039962  adaboost_all\n",
      "[4.12817229 3.99071287 3.9370843  4.28371682 3.88833761] gb_all\n",
      "4.045604777375567  gb _all\n",
      "0.14355313744542464  gb_all\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import *\n",
    "from sklearn import datasets \n",
    "from sklearn.metrics import mean_squared_error \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import random\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_text\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "\n",
    "def read_data(run_num, prob):\n",
    "\n",
    "    normalise = False\n",
    "    \n",
    "    if prob == 'classifification': #Source: Pima-Indian diabetes dataset: https://www.kaggle.com/kumargh/pimaindiansdiabetescsv\n",
    "        data_in = genfromtxt(\"datasets/pima.csv\", delimiter=\",\")\n",
    "        data_inputx = data_in[:, 0:8] # all features 0, 1, 2, 3, 4, 5, 6, 7 \n",
    "        data_inputy = data_in[:, -1]  # this is target - so that last col is selected from data\n",
    "\n",
    "    elif prob == 'regression': # energy - regression prob\n",
    "        data_in = genfromtxt('datasets/energy/ENB2012_data.csv', delimiter=\",\")  \n",
    "        data_inputx = data_in[:, 0:8] # all features 0, - 7\n",
    "        data_inputy = data_in[:, 8]   # this is target - just the heating load selected from data\n",
    "  \n",
    "\n",
    "    if normalise == True:\n",
    "        transformer = Normalizer().fit(data_inputx)  # fit does nothing.\n",
    "        data_inputx = transformer.transform(data_inputx)\n",
    " \n",
    "    x_train, x_test, y_train, y_test = train_test_split(data_inputx, data_inputy, test_size=0.40, random_state=run_num)\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    " \n",
    "    \n",
    "def scipy_models(x_train, x_test, y_train, y_test, type_model, hidden, learn_rate, run_num, problem):\n",
    "\n",
    "    print(run_num, ' is our exp run')\n",
    "\n",
    "    tree_depth = 2\n",
    " \n",
    "    if problem == 'classifification':\n",
    "        if type_model == 0:  # SGD \n",
    "            model = MLPClassifier(hidden_layer_sizes=(hidden,), random_state=run_num, \n",
    "                                  max_iter=100,solver='sgd', learning_rate_init=learn_rate )  \n",
    "        elif type_model == 1: # https://scikit-learn.org/stable/modules/tree.html  (see how tree can be visualised)\n",
    "            model = DecisionTreeClassifier(random_state=0, max_depth=tree_depth) \n",
    "        elif type_model == 2:\n",
    "            model = RandomForestClassifier(n_estimators=100, max_depth=tree_depth, random_state=run_num)\n",
    "            \n",
    "        elif type_model == 3:\n",
    "            model = AdaBoostClassifier(n_estimators=100,  random_state=run_num)\n",
    "\n",
    "        elif type_model == 4:\n",
    "            model = GradientBoostingClassifier(n_estimators=10,  random_state=run_num)\n",
    "\n",
    "    elif problem == 'regression':\n",
    "        if type_model ==0: #SGD  \n",
    "            model = MLPRegressor(hidden_layer_sizes=(hidden*3,), random_state=run_num, \n",
    "                                 max_iter=500, solver='adam',learning_rate_init=learn_rate) \n",
    "        elif type_model == 1:  \n",
    "            model = DecisionTreeRegressor(random_state=0, max_depth=tree_depth)\n",
    "        elif type_model == 2: \n",
    "            model = RandomForestRegressor(n_estimators=100, max_depth=tree_depth, random_state=run_num)\n",
    "        elif type_model == 3: \n",
    "            model = AdaBoostRegressor(n_estimators=100, random_state=run_num)\n",
    "        elif type_model == 4:\n",
    "            model = GradientBoostingRegressor(n_estimators=10, random_state=run_num)\n",
    "   \n",
    "    # Train the model using the training sets\n",
    "    model.fit(x_train, y_train)   \n",
    "\n",
    "    if type_model == 1:\n",
    "        r = export_text(model)\n",
    "        print(r)\n",
    "\n",
    "    # Make predictions using the testing set\n",
    "    y_pred_test = model.predict(x_test)\n",
    "    y_pred_train = model.predict(x_train) \n",
    "\n",
    "    if problem == 'regression':\n",
    "        perf_test = np.sqrt(mean_squared_error(y_test, y_pred_test)) \n",
    "        perf_train = np.sqrt(mean_squared_error(y_train, y_pred_train)) \n",
    "\n",
    "    if problem == 'classifification': \n",
    "        perf_test = accuracy_score(y_pred_test, y_test) \n",
    "        perf_train = accuracy_score(y_pred_train, y_train) \n",
    "        cm = confusion_matrix(y_pred_test, y_test) \n",
    "        #print(cm, 'is confusion matrix')\n",
    "        #auc = roc_auc_score(y_pred, y_test, average=None) \n",
    "\n",
    "    return perf_test #,perf_train\n",
    "\n",
    "\n",
    "def main(): \n",
    "\n",
    "    max_expruns = 5\n",
    "\n",
    "    SGD_all = np.zeros(max_expruns) \n",
    "    forest_all = np.zeros(max_expruns) \n",
    "    tree_all = np.zeros(max_expruns) \n",
    "    adaboost_all = np.zeros(max_expruns)  \n",
    "\n",
    "    gb_all = np.zeros(max_expruns)  \n",
    " \n",
    "    learn_rate = 0.01\n",
    "    hidden = 8\n",
    "\n",
    "    #prob = 'classifification' # classification  or regression \n",
    "    prob = 'regression'        # classification  or regression \n",
    "\n",
    "    # classifcation accurary is reported for classification and RMSE for regression\n",
    "\n",
    "    print(prob, ' is our problem') \n",
    " \n",
    "    for run_num in range(0,max_expruns): \n",
    "\n",
    "        x_train, x_test, y_train, y_test = read_data(run_num, prob)   \n",
    "        \n",
    "        acc_sgd = scipy_models(x_train, x_test, y_train, y_test, 0, hidden, learn_rate, run_num, prob)   #SGD \n",
    "        acc_tree = scipy_models(x_train, x_test, y_train, y_test, 1, hidden, learn_rate, run_num, prob)  #Decision Tree\n",
    "        acc_forest = scipy_models(x_train, x_test, y_train, y_test, 2, hidden, learn_rate, run_num, prob)  #Random Forests\n",
    "        acc_adaboost = scipy_models(x_train, x_test, y_train, y_test, 3, hidden, learn_rate, run_num, prob) #adaboost\n",
    "        acc_gb = scipy_models(x_train, x_test, y_train, y_test, 4, hidden, learn_rate, run_num, prob)       #adaboost\n",
    "       \n",
    "        SGD_all[run_num] = acc_sgd \n",
    "        tree_all[run_num] = acc_tree\n",
    "        forest_all[run_num] = acc_forest\n",
    "        adaboost_all[run_num] = acc_adaboost\n",
    "        gb_all[run_num] = acc_gb\n",
    "\n",
    "    print(SGD_all,' nn_all')\n",
    "    print(np.mean(SGD_all), ' mean nn_all')\n",
    "    print(np.std(SGD_all), ' std nn_all')\n",
    " \n",
    "    print(tree_all, ' tree_all')\n",
    "    print(np.mean(tree_all), ' tree _all')\n",
    "    print(np.std(tree_all), ' tree _all')\n",
    "\n",
    "    print(forest_all, hidden, ' forest_all')\n",
    "    print(np.mean(forest_all), ' forest _all')\n",
    "    print(np.std(forest_all), ' forest _all')\n",
    "\n",
    "    print(adaboost_all, 'adaboost_all')\n",
    "    print(np.mean(adaboost_all), ' adaboost _all')\n",
    "    print(np.std(adaboost_all), ' adaboost_all')\n",
    "\n",
    "       \n",
    "    print(gb_all, 'gb_all')\n",
    "    print(np.mean(gb_all), ' gb _all')\n",
    "    print(np.std(gb_all), ' gb_all')\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "     main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e39ebd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifification  is our problem\n",
      "0  is our exp run\n",
      "0  is our exp run\n",
      "|--- feature_1 <= 154.50\n",
      "|   |--- feature_7 <= 28.50\n",
      "|   |   |--- class: 0.0\n",
      "|   |--- feature_7 >  28.50\n",
      "|   |   |--- class: 0.0\n",
      "|--- feature_1 >  154.50\n",
      "|   |--- feature_5 <= 29.95\n",
      "|   |   |--- class: 0.0\n",
      "|   |--- feature_5 >  29.95\n",
      "|   |   |--- class: 1.0\n",
      "\n",
      "0  is our exp run\n",
      "0  is our exp run\n",
      "0  is our exp run\n",
      "0  is our exp run\n",
      "[23:55:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\di_ma\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  is our exp run\n",
      "1  is our exp run\n",
      "|--- feature_1 <= 130.50\n",
      "|   |--- feature_7 <= 27.50\n",
      "|   |   |--- class: 0.0\n",
      "|   |--- feature_7 >  27.50\n",
      "|   |   |--- class: 0.0\n",
      "|--- feature_1 >  130.50\n",
      "|   |--- feature_5 <= 33.25\n",
      "|   |   |--- class: 0.0\n",
      "|   |--- feature_5 >  33.25\n",
      "|   |   |--- class: 1.0\n",
      "\n",
      "1  is our exp run\n",
      "1  is our exp run\n",
      "1  is our exp run\n",
      "1  is our exp run\n",
      "[23:55:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "2  is our exp run\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\di_ma\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2  is our exp run\n",
      "|--- feature_1 <= 127.50\n",
      "|   |--- feature_0 <= 4.50\n",
      "|   |   |--- class: 0.0\n",
      "|   |--- feature_0 >  4.50\n",
      "|   |   |--- class: 0.0\n",
      "|--- feature_1 >  127.50\n",
      "|   |--- feature_1 <= 165.50\n",
      "|   |   |--- class: 1.0\n",
      "|   |--- feature_1 >  165.50\n",
      "|   |   |--- class: 1.0\n",
      "\n",
      "2  is our exp run\n",
      "2  is our exp run\n",
      "2  is our exp run\n",
      "2  is our exp run\n",
      "[23:55:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "3  is our exp run\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\di_ma\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3  is our exp run\n",
      "|--- feature_1 <= 144.50\n",
      "|   |--- feature_7 <= 28.50\n",
      "|   |   |--- class: 0.0\n",
      "|   |--- feature_7 >  28.50\n",
      "|   |   |--- class: 0.0\n",
      "|--- feature_1 >  144.50\n",
      "|   |--- feature_1 <= 166.50\n",
      "|   |   |--- class: 1.0\n",
      "|   |--- feature_1 >  166.50\n",
      "|   |   |--- class: 1.0\n",
      "\n",
      "3  is our exp run\n",
      "3  is our exp run\n",
      "3  is our exp run\n",
      "3  is our exp run\n",
      "[23:55:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "4  is our exp run\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\di_ma\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4  is our exp run\n",
      "|--- feature_1 <= 127.50\n",
      "|   |--- feature_7 <= 28.50\n",
      "|   |   |--- class: 0.0\n",
      "|   |--- feature_7 >  28.50\n",
      "|   |   |--- class: 0.0\n",
      "|--- feature_1 >  127.50\n",
      "|   |--- feature_5 <= 29.95\n",
      "|   |   |--- class: 0.0\n",
      "|   |--- feature_5 >  29.95\n",
      "|   |   |--- class: 1.0\n",
      "\n",
      "4  is our exp run\n",
      "4  is our exp run\n",
      "4  is our exp run\n",
      "4  is our exp run\n",
      "[23:55:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.66558442 0.6461039  0.66558442 0.59415584 0.66558442]  nn_all\n",
      "0.6474025974025974  mean nn_all\n",
      "0.02767178669176953  std nn_all\n",
      "[0.72402597 0.70454545 0.73051948 0.71428571 0.78896104]  tree_all\n",
      "0.7324675324675325  tree _all\n",
      "0.029586456493232507  tree _all\n",
      "[0.74025974 0.75       0.74025974 0.71753247 0.78896104] 8  forest_all\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\di_ma\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7474025974025974  forest _all\n",
      "0.02335857889429213  forest _all\n",
      "[0.76298701 0.77597403 0.74025974 0.74675325 0.77272727] adaboost_all\n",
      "0.7597402597402597  adaboost _all\n",
      "0.014077586616025192  adaboost_all\n",
      "[0.74675325 0.74675325 0.74675325 0.72402597 0.80519481] gb_all\n",
      "0.753896103896104  gb _all\n",
      "0.02711767924392353  gb_all\n",
      "[0.75974026 0.76948052 0.76623377 0.74025974 0.78571429] xg_all\n",
      "0.7642857142857142  xg _all\n",
      "0.014750411287792906  xg_all\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import *\n",
    "from sklearn import datasets \n",
    "from sklearn.metrics import mean_squared_error \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import random\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_text\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def read_data(run_num, prob):\n",
    "\n",
    "    normalise = False\n",
    "    \n",
    "    if prob == 'classifification': #Source: Pima-Indian diabetes dataset: https://www.kaggle.com/kumargh/pimaindiansdiabetescsv\n",
    "        data_in = genfromtxt(\"datasets/pima.csv\", delimiter=\",\")\n",
    "        data_inputx = data_in[:, 0:8]  # all features 0, 1, 2, 3, 4, 5, 6, 7 \n",
    "        data_inputy = data_in[:, -1]   # this is target - so that last col is selected from data\n",
    "\n",
    "    elif prob == 'regression': # energy - regression prob\n",
    "        data_in = genfromtxt('datasets/energy/ENB2012_data.csv', delimiter=\",\")  \n",
    "        data_inputx = data_in[:, 0:8]  # all features 0 - 7\n",
    "        data_inputy = data_in[:, 8]    # this is target - just the heating load selected from data  \n",
    "\n",
    "    if normalise == True:\n",
    "        transformer = Normalizer().fit(data_inputx)  # fit does nothing.\n",
    "        data_inputx = transformer.transform(data_inputx)\n",
    " \n",
    "    x_train, x_test, y_train, y_test = train_test_split(data_inputx, data_inputy, test_size=0.40, random_state=run_num)\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    " \n",
    "    \n",
    "def scipy_models(x_train, x_test, y_train, y_test, type_model, hidden, learn_rate, run_num, problem):\n",
    "\n",
    "    print(run_num, ' is our exp run')\n",
    "\n",
    "    tree_depth = 2\n",
    " \n",
    "    if problem == 'classifification':\n",
    "        if type_model == 0:  # SGD \n",
    "            model = MLPClassifier(hidden_layer_sizes=(hidden,), random_state=run_num, \n",
    "                                  max_iter=100,solver='sgd', learning_rate_init=learn_rate)  \n",
    "        elif type_model == 1: #https://scikit-learn.org/stable/modules/tree.html (see how tree can be visualised)\n",
    "            model = DecisionTreeClassifier(random_state=0, max_depth=tree_depth) \n",
    "        elif type_model == 2:\n",
    "            model = RandomForestClassifier(n_estimators=100, max_depth=tree_depth, random_state=run_num)\n",
    "            \n",
    "        elif type_model == 3:\n",
    "            model = AdaBoostClassifier(n_estimators=100, random_state=run_num)\n",
    "\n",
    "        elif type_model == 4:\n",
    "            model = GradientBoostingClassifier(n_estimators=10, random_state=run_num)\n",
    "\n",
    "    elif problem == 'regression':\n",
    "        if type_model == 0: #SGD  \n",
    "            model = MLPRegressor(hidden_layer_sizes=(hidden*3,), random_state=run_num, \n",
    "                                 max_iter=500, solver='adam',learning_rate_init=learn_rate) \n",
    "        elif type_model == 1:  \n",
    "            model = DecisionTreeRegressor(random_state=0, max_depth=tree_depth)\n",
    "            \n",
    "        elif type_model == 2: \n",
    "            model = RandomForestRegressor(n_estimators=100, max_depth=tree_depth, random_state=run_num)\n",
    "            \n",
    "        elif type_model == 3: \n",
    "            model = AdaBoostRegressor(n_estimators=100, random_state=run_num)\n",
    "            \n",
    "        elif type_model == 4:\n",
    "            model = GradientBoostingRegressor(n_estimators=10, random_state=run_num)            \n",
    "   \n",
    "    # Train the model using the training sets\n",
    "    model.fit(x_train, y_train)   \n",
    "\n",
    "    if type_model ==1:\n",
    "        r = export_text(model)\n",
    "        print(r)\n",
    "\n",
    "    # Make predictions using the testing set\n",
    "    y_pred_test = model.predict(x_test)\n",
    "    y_pred_train = model.predict(x_train) \n",
    "\n",
    "    if problem == 'regression':\n",
    "        perf_test = np.sqrt(mean_squared_error(y_test, y_pred_test)) \n",
    "        perf_train = np.sqrt(mean_squared_error(y_train, y_pred_train)) \n",
    "\n",
    "    if problem == 'classifification': \n",
    "        perf_test = accuracy_score(y_pred_test, y_test) \n",
    "        perf_train = accuracy_score(y_pred_train, y_train) \n",
    "        cm = confusion_matrix(y_pred_test, y_test) \n",
    "        #print(cm, 'is confusion matrix')\n",
    "        #auc = roc_auc_score(y_pred, y_test, average=None) \n",
    "\n",
    "    return perf_test #,perf_train\n",
    "\n",
    "\n",
    "def xgboost_models(x_train, x_test, y_train, y_test, type_model, hidden, learn_rate, run_num, problem):\n",
    "\n",
    "    print(run_num, ' is our exp run')\n",
    "\n",
    "    tree_depth = 2\n",
    " \n",
    "    if problem == 'classifification':\n",
    "        if type_model == 0:  \n",
    "            model = xgb.XGBClassifier(colsample_bytree = 0.3, learning_rate = 0.1,\n",
    "                                      max_depth = 5, alpha = 5, n_estimators = 100)            \n",
    "\n",
    "    elif problem == 'regression':\n",
    "        if type_model == 0:    # SGD  \n",
    "            model = xgb.XGBRegressor(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1, \n",
    "                                     max_depth = 5, alpha = 5, n_estimators = 100)            \n",
    "   \n",
    "    # Train the model using the training sets\n",
    "    model.fit(x_train, y_train)   \n",
    "\n",
    "    if type_model == 1:\n",
    "        r = export_text(model)\n",
    "        print(r)\n",
    "\n",
    "    # Make predictions using the testing set\n",
    "    y_pred_test = model.predict(x_test)\n",
    "    y_pred_train = model.predict(x_train) \n",
    "\n",
    "    if problem == 'regression':\n",
    "        perf_test = np.sqrt(mean_squared_error(y_test, y_pred_test)) \n",
    "        perf_train = np.sqrt(mean_squared_error(y_train, y_pred_train)) \n",
    "\n",
    "    if problem == 'classifification': \n",
    "        perf_test = accuracy_score(y_pred_test, y_test) \n",
    "        perf_train = accuracy_score(y_pred_train, y_train) \n",
    "        cm = confusion_matrix(y_pred_test, y_test) \n",
    "        #print(cm, 'is confusion matrix')\n",
    "        #auc = roc_auc_score(y_pred, y_test, average=None) \n",
    "\n",
    "    return perf_test #,perf_train\n",
    "\n",
    "\n",
    "def main(): \n",
    "\n",
    "    max_expruns = 5\n",
    "\n",
    "    SGD_all = np.zeros(max_expruns) \n",
    "    forest_all = np.zeros(max_expruns) \n",
    "    tree_all = np.zeros(max_expruns) \n",
    "    adaboost_all = np.zeros(max_expruns)  \n",
    "    xg_all = np.zeros(max_expruns)  \n",
    "    gb_all = np.zeros(max_expruns)  \n",
    " \n",
    "    learn_rate = 0.01\n",
    "    hidden = 8\n",
    "\n",
    "    prob = 'classifification' # classification or regression \n",
    "    #prob = 'regression'      # classification or regression \n",
    "\n",
    "    # classifcation accurary is reported for classification and RMSE for regression\n",
    "\n",
    "    print(prob, ' is our problem') \n",
    " \n",
    "    for run_num in range(0,max_expruns): \n",
    "\n",
    "        x_train, x_test, y_train, y_test = read_data(run_num, prob)   \n",
    "        \n",
    "        acc_sgd = scipy_models(x_train, x_test, y_train, y_test, 0, hidden, learn_rate, run_num, prob) #SGD \n",
    "        acc_tree = scipy_models(x_train, x_test, y_train, y_test, 1, hidden, learn_rate, run_num, prob) #Decision Tree\n",
    "        acc_forest = scipy_models(x_train, x_test, y_train, y_test, 2, hidden, learn_rate, run_num, prob) #Random Forests\n",
    "        acc_adaboost = scipy_models(x_train, x_test, y_train, y_test, 3, hidden, learn_rate, run_num, prob) #adaboost\n",
    "        acc_gb = scipy_models(x_train, x_test, y_train, y_test, 4, hidden, learn_rate, run_num, prob) #gboost\n",
    "        acc_xg = xgboost_models(x_train, x_test, y_train, y_test, 0, hidden, learn_rate, run_num, prob) #adaboost\n",
    "       \n",
    "        SGD_all[run_num] = acc_sgd \n",
    "        tree_all[run_num] = acc_tree\n",
    "        forest_all[run_num] = acc_forest\n",
    "        adaboost_all[run_num] = acc_adaboost\n",
    "        gb_all[run_num] = acc_gb\n",
    "        xg_all[run_num] = acc_xg\n",
    "\n",
    "    print(SGD_all,' nn_all')\n",
    "    print(np.mean(SGD_all), ' mean nn_all')\n",
    "    print(np.std(SGD_all), ' std nn_all')\n",
    " \n",
    "    print(tree_all, ' tree_all')\n",
    "    print(np.mean(tree_all), ' tree _all')\n",
    "    print(np.std(tree_all), ' tree _all')\n",
    "\n",
    "    print(forest_all, hidden, ' forest_all')\n",
    "    print(np.mean(forest_all), ' forest _all')\n",
    "    print(np.std(forest_all), ' forest _all')\n",
    "\n",
    "    print(adaboost_all, 'adaboost_all')\n",
    "    print(np.mean(adaboost_all), ' adaboost _all')\n",
    "    print(np.std(adaboost_all), ' adaboost_all')\n",
    "       \n",
    "    print(gb_all, 'gb_all')\n",
    "    print(np.mean(gb_all), ' gb _all')\n",
    "    print(np.std(gb_all), ' gb_all')\n",
    " \n",
    "    print(xg_all, 'xg_all')\n",
    "    print(np.mean(xg_all), ' xg _all')\n",
    "    print(np.std(xg_all), ' xg_all')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "     main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c10a1aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-1.5.0-py3-none-win_amd64.whl (106.6 MB)\n",
      "Requirement already satisfied: scipy in c:\\users\\di_ma\\anaconda3\\lib\\site-packages (from xgboost) (1.6.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\di_ma\\anaconda3\\lib\\site-packages (from xgboost) (1.20.1)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee35a82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
