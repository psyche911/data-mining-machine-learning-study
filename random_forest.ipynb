{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8f5ee6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifification  is our problem\n",
      "0  is our exp run\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'model' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f5a84de0a1a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m      \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-f5a84de0a1a4>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[1;31m#acc_sgd = scipy_models(x_train, x_test, y_train, y_test, 0, hidden, learn_rate, run_num, prob)  # SGD\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[1;31m#acc_tree = scipy_models(x_train, x_test, y_train, y_test, 1, hidden, learn_rate, run_num, prob) # Decision Tree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m         \u001b[0macc_forest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscipy_models\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearn_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Random Forests\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m         \u001b[1;31m#acc_extratree = scipy_models(x_train, x_test, y_train, y_test, 3, hidden, learn_rate, run_num, prob) # Extra Trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-f5a84de0a1a4>\u001b[0m in \u001b[0;36mscipy_models\u001b[1;34m(x_train, x_test, y_train, y_test, type_model, hidden, learn_rate, run_num, problem)\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;31m# Train the model using the training sets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtype_model\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'model' referenced before assignment"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import *\n",
    "from sklearn import datasets \n",
    "from sklearn.metrics import mean_squared_error \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import random\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_text\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "\n",
    "def read_data(run_num, prob):\n",
    "\n",
    "    normalise = False\n",
    "    \n",
    "    if prob == 'classifification': \n",
    "        # Source: Pima-Indian diabetes dataset: https://www.kaggle.com/kumargh/pimaindiansdiabetescsv\n",
    "        data_in = genfromtxt(\"datasets/pima.csv\", delimiter=\",\")\n",
    "        data_inputx = data_in[:, 0:8]  # all features 0 - 7 \n",
    "        data_inputy = data_in[:, -1]   # this is target - so that last col is selected from data\n",
    "\n",
    "    elif prob == 'regression': # energy - regression prob\n",
    "        data_in = genfromtxt('datasets/energy/ENB2012_data.csv', delimiter=\",\")  \n",
    "        data_inputx = data_in[:, 0:8]  # all features 0 - 7\n",
    "        data_inputy = data_in[:, 8]    # this is target - just the heating load selected from data\n",
    "  \n",
    "\n",
    "    if normalise == True:\n",
    "        transformer = Normalizer().fit(data_inputx)  # fit does nothing.\n",
    "        data_inputx = transformer.transform(data_inputx)\n",
    " \n",
    "    x_train, x_test, y_train, y_test = train_test_split(data_inputx, data_inputy, test_size=0.40, random_state=run_num)\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "    \n",
    "def scipy_models(x_train, x_test, y_train, y_test, type_model, hidden, learn_rate, run_num, problem):\n",
    "\n",
    "    print(run_num, ' is our exp run')\n",
    "\n",
    "    tree_depth = 2\n",
    " \n",
    "    if problem == 'classifification':\n",
    "        if type_model == 0:  # SGD \n",
    "            model = MLPClassifier(hidden_layer_sizes=(hidden,), random_state=run_num, \n",
    "                                  max_iter=100, solver='sgd', learning_rate_init=learn_rate)\n",
    "            \n",
    "        elif type_model == 1: \n",
    "            # https://scikit-learn.org/stable/modules/tree.html (see how tree can be visualised)\n",
    "            model = DecisionTreeClassifier(random_state=0, max_depth=tree_depth)\n",
    "            \n",
    "        elif type_model == 3:\n",
    "            model = RandomForestClassifier(n_estimators=100, max_depth=tree_depth, random_state=run_num)\n",
    "            \n",
    "        elif type_model == 4:\n",
    "            model = ExtraTreesClassifier(n_estimators=100, max_depth=tree_depth, random_state=run_num)\n",
    "\n",
    "    elif problem == 'regression':\n",
    "        if type_model == 0:  # SGD  \n",
    "            model = MLPRegressor(hidden_layer_sizes=(hidden*3,), random_state=run_num, \n",
    "                                 max_iter=500, solver='adam', learning_rate_init=learn_rate) \n",
    "        elif type_model == 1:  \n",
    "            model = DecisionTreeRegressor(random_state=0, max_depth=tree_depth)\n",
    "            \n",
    "        elif type_model == 3: \n",
    "            model = RandomForestRegressor(n_estimators=100, max_depth=tree_depth, random_state=run_num)\n",
    "            \n",
    "        elif type_model == 4: \n",
    "            model = ExtraTreeRegressor(n_estimators=100, max_depth=tree_depth, random_state=run_num)\n",
    "            \n",
    "    # Train the model using the training sets\n",
    "\n",
    "    model.fit(x_train, y_train)   \n",
    "\n",
    "    if type_model == 1:\n",
    "        r = export_text(model)\n",
    "        print(r)\n",
    "\n",
    "    # Make predictions using the testing set\n",
    "    y_pred_test = model.predict(x_test)\n",
    "    y_pred_train = model.predict(x_train) \n",
    "\n",
    "    if problem == 'regression':\n",
    "        perf_test = np.sqrt(mean_squared_error(y_test, y_pred_test)) \n",
    "        perf_train = np.sqrt(mean_squared_error(y_train, y_pred_train)) \n",
    "\n",
    "    if problem == 'classifification': \n",
    "        perf_test = accuracy_score(y_pred_test, y_test) \n",
    "        perf_train = accuracy_score(y_pred_train, y_train) \n",
    "        cm = confusion_matrix(y_pred_test, y_test) \n",
    "        #print(cm, 'is confusion matrix')\n",
    "        #auc = roc_auc_score(y_pred, y_test, average=None) \n",
    "\n",
    "    return perf_test #,perf_train\n",
    "\n",
    "\n",
    "def main(): \n",
    "\n",
    "    max_expruns = 5\n",
    "\n",
    "    SGD_all = np.zeros(max_expruns) \n",
    "    forest_all = np.zeros(max_expruns) \n",
    "    tree_all = np.zeros(max_expruns) \n",
    "    extratree_all = np.zeros(max_expruns)  \n",
    " \n",
    "    learn_rate = 0.01\n",
    "    hidden = 8\n",
    "\n",
    "    prob = 'classifification' #  classification  or regression \n",
    "    #prob = 'regression' #  classification  or regression \n",
    "\n",
    "    # classifcation accurary is reported for classification, and RMSE for regression\n",
    "\n",
    "    print(prob, ' is our problem') \n",
    " \n",
    "    for run_num in range(0, max_expruns): \n",
    "\n",
    "        x_train, x_test, y_train, y_test = read_data(run_num, prob)   \n",
    "        \n",
    "        #acc_sgd = scipy_models(x_train, x_test, y_train, y_test, 0, hidden, learn_rate, run_num, prob)  # SGD \n",
    "        #acc_tree = scipy_models(x_train, x_test, y_train, y_test, 1, hidden, learn_rate, run_num, prob) # Decision Tree\n",
    "        acc_forest = scipy_models(x_train, x_test, y_train, y_test, 2, hidden, learn_rate, run_num, prob) # Random Forests\n",
    "        #acc_extratree = scipy_models(x_train, x_test, y_train, y_test, 3, hidden, learn_rate, run_num, prob) # Extra Trees\n",
    "       \n",
    "        SGD_all[run_num] = acc_sgd \n",
    "        tree_all[run_num] = acc_tree\n",
    "        forest_all[run_num] = acc_forest\n",
    "        extratree_all[run_num] = acc_extratree\n",
    "    \n",
    "    print(SGD_all, ' SGD_all')\n",
    "    print(np.mean(SGD_all), ' mean SGD_all')\n",
    "    print(np.std(SGD_all), ' std SGD_all')\n",
    " \n",
    "    print(tree_all,  ' tree_all')\n",
    "    print(np.mean(tree_all), ' tree _all')\n",
    "    print(np.std(tree_all), ' tree _all')\n",
    "\n",
    "    print(forest_all, hidden, ' forest_all')\n",
    "    print(np.mean(forest_all), ' forest _all')\n",
    "    print(np.std(forest_all), ' forest _all')\n",
    "\n",
    "    print(extratree_all, ' extra tree_all')\n",
    "    print(np.mean(extratree_all), ' extra tree _all')\n",
    "    print(np.std(extrratree_all), ' extra tree_all')\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "     main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cc2757",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
