{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "545c0c7f",
   "metadata": {},
   "source": [
    "### Exercise 3.1\n",
    "#### Python Challenge  \n",
    "\n",
    "- Try training the FNN model with two selected classification datasets from the UCI machine learning repository. Evaluate the effect of number of hidden neurons, and learning rate [0.1, 0.2 ...1]. Try 10 experiments with different random state in data split (60/40 train/test) or different initial weights for each run and report the mean and standard deviation for each experiment and plot your results. \n",
    "\n",
    "Optional (this will require more time and not part of the course assessment)\n",
    "\n",
    "- Try to add another hidden later and update the forward and backward pass. \n",
    "\n",
    "- Can you generalise to any user-selected number of hidden layers? Discuss how you will do this and implement it if possible. \n",
    "\n",
    "Resources\n",
    "\n",
    "Example datasets  http://archive.ics.uci.edu/ml/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0cfbfb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD_all:  [0.66233766 0.66233766 0.66558442 0.66233766 0.66233766] 6\n",
      "Mean SGD_all:  0.662987012987013 6\n",
      "std SGD_all:  0.0012987012987013102 6\n",
      "Adam_all:  [0.76623377 0.68831169 0.66558442 0.64935065 0.66558442] 6\n",
      "Adam_all:  0.6870129870129871 6\n",
      "Adam_all:  0.04150768003765426 6\n",
      "SGD2_all:  [0.65909091 0.66233766 0.66558442 0.66233766 0.71103896] 6\n",
      "SGD_all:  [0.66558442 0.66558442 0.66233766 0.66558442 0.66233766] 8\n",
      "Mean SGD_all:  0.6642857142857144 8\n",
      "std SGD_all:  0.001590577755054026 8\n",
      "Adam_all:  [0.71428571 0.66883117 0.73376623 0.66883117 0.72727273] 8\n",
      "Adam_all:  0.7025974025974027 8\n",
      "Adam_all:  0.028274728645554883 8\n",
      "SGD2_all:  [0.66558442 0.66558442 0.66233766 0.69805195 0.66558442] 8\n",
      "SGD_all:  [0.66558442 0.66233766 0.66558442 0.66558442 0.66233766] 10\n",
      "Mean SGD_all:  0.6642857142857144 10\n",
      "std SGD_all:  0.001590577755054026 10\n",
      "Adam_all:  [0.65909091 0.69480519 0.75974026 0.72402597 0.7012987 ] 10\n",
      "Adam_all:  0.7077922077922079 10\n",
      "Adam_all:  0.033300990662228795 10\n",
      "SGD2_all:  [0.68506494 0.66558442 0.66558442 0.66558442 0.66558442] 10\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from numpy import *\n",
    "from sklearn import datasets \n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "def read_data(run_num):\n",
    "    #Source: Pima-Indian diabetes dataset: https://www.kaggle.com/kumargh/pimaindiansdiabetescsv\n",
    "    data = genfromtxt(\"datasets/pima.csv\", delimiter=\",\") \n",
    "    data_X = data[:, 0:8]    # all features 0-7 \n",
    "\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html\n",
    "    # sklearn.preprocessing.Normalizer\n",
    "    #transformer = Normalizer().fit(data_inputx)  # fit does nothing\n",
    "    #data_inputx = transformer.transform(data_inputx)\n",
    "\n",
    "    data_y = data[:, -1]   # this is target - so that last col is selected from data\n",
    " \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data_X, data_y, test_size=0.4, random_state=run_num)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    " \n",
    "    \n",
    "def scipy_nn(X_train, X_test, y_train, y_test, type_model, hidden, learn_rate, run_num):\n",
    "    # Source: https://scikit-learn.org/stable/modules/neural_networks_supervised.html\n",
    "    # random_stateint, RandomState instance, default=None Determines random number generation\n",
    "    # for weights and bias initialization, train-test split if early stopping is used, and \n",
    "    # batch sampling when solver=’sgd’(stochastic gradient descent) or ‘adam’(stochastic gradient-based optimizer).\n",
    "    # Pass an int for reproducible results across multiple function calls.\n",
    "    # learning_rate_initdouble, default=0.001\n",
    "    # The initial learning rate used. It controls the step-size in updating the weights.\n",
    "    # Only used when solver=’sgd’ or ‘adam’.\n",
    "\n",
    "    # Note: Adam does not need momentum and constant learning rate since they are adjusted in Adam itself\n",
    "\n",
    "    if type_model == 0:   # SGD\n",
    "        # https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    "        nn = MLPClassifier(hidden_layer_sizes=(hidden,), random_state=run_num, \n",
    "                           max_iter=1000, solver='sgd', learning_rate_init=learn_rate)\n",
    "        \n",
    "    elif type_model == 1: # Adam\n",
    "        # https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    "        nn = MLPClassifier(hidden_layer_sizes=(hidden,), random_state=run_num, \n",
    "                           max_iter=1000, solver='adam', learning_rate_init=learn_rate)\n",
    "        \n",
    "    elif type_model == 2: # SGD with 2 hidden layers\n",
    "        nn = MLPClassifier(hidden_layer_sizes=(hidden, hidden), random_state=run_num, max_iter=1000,\n",
    "                           solver='sgd', learning_rate='constant', learning_rate_init=learn_rate)\n",
    "        #hidden_layer_sizes = (hidden, hidden, hidden) would implement 3 hidden layers\n",
    "    else:\n",
    "        print('no model')    \n",
    " \n",
    "    # Train the model using the training sets\n",
    "    nn.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions using the testing set\n",
    "    y_pred_test = nn.predict(X_test)\n",
    "    y_pred_train = nn.predict(X_train)\n",
    "\n",
    "    #print('weights shape: ', [coef.shape for coef in nn.coefs_]) \n",
    "    #print(\"RMSE: %.2f\" % np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "    # or, mean_squared_error(y_test, y_pred, squared=False)\n",
    "    \n",
    "    acc_test = accuracy_score(y_pred_test, y_test) \n",
    "    acc_train = accuracy_score(y_pred_train, y_train) \n",
    "\n",
    "    cm = confusion_matrix(y_pred_test, y_test) \n",
    "    #print(cm, 'is confusion matrix')\n",
    "\n",
    "    #auc = roc_auc_score(y_pred, y_test, average=None) \n",
    "    return acc_test #,acc_train\n",
    "\n",
    "\n",
    "def main(): \n",
    "    max_expruns = 5\n",
    "    SGD_all = np.zeros(max_expruns) \n",
    "    Adam_all = np.zeros(max_expruns)     \n",
    "    SGD2_all = np.zeros(max_expruns)  \n",
    "    max_hidden = 12\n",
    "    learn_rate = 0.01\n",
    "    #hidden = 8\n",
    "    \n",
    "    #for learn_rate in range(0.1, 1, 0.2):\n",
    "    \n",
    "    for hidden in range(6, max_hidden, 2): \n",
    "        for run_num in range(max_expruns):     \n",
    "            x_train, x_test, y_train, y_test = read_data(0)\n",
    "            \n",
    "            acc_sgd = scipy_nn(x_train, x_test, y_train, y_test, 0, hidden, learn_rate, run_num)   # SGD\n",
    "            acc_adam = scipy_nn(x_train, x_test, y_train, y_test, 1, hidden, learn_rate, run_num)  # Adam \n",
    "            acc_sgd2 = scipy_nn(x_train, x_test, y_train, y_test, 2, hidden, learn_rate,  run_num) # SGD2\n",
    "           \n",
    "            SGD_all[run_num] = acc_sgd\n",
    "            Adam_all[run_num] = acc_adam\n",
    "\n",
    "            SGD2_all[run_num] = acc_sgd2   # two hidden layers\n",
    "        \n",
    "        print('SGD_all: ', SGD_all, hidden)\n",
    "        print('Mean SGD_all: ', np.mean(SGD_all), hidden)\n",
    "        print('std SGD_all: ', np.std(SGD_all), hidden)\n",
    "\n",
    "        print('Adam_all: ', Adam_all, hidden)\n",
    "        print('Adam_all: ', np.mean(Adam_all), hidden)\n",
    "        print('Adam_all: ', np.std(Adam_all), hidden)\n",
    "\n",
    "        print('SGD2_all: ', SGD2_all, hidden)\n",
    "\n",
    "        # print for Adam\n",
    "    # next try a paragraph to describe your results and discuss which models are better to use.\n",
    "    # repeat for another dataset\n",
    "    # you can save results to a file as well\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "     main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be46b6e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
