{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "184bbfb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.46391225 0.81681318 0.9856427  0.99566879 0.18224726]\n",
      "  [0.19485853 0.18646619 0.95706079 0.41282122 0.25447514]\n",
      "  [0.88389975 0.23759714 0.59183858 0.75197443 0.93150512]\n",
      "  [0.04709183 0.00130502 0.15083255 0.51619873 0.38410871]\n",
      "  [0.22399013 0.82668565 0.2677109  0.31410795 0.78524972]]\n",
      "\n",
      " [[0.22399013 0.82668565 0.2677109  0.31410795 0.78524972]\n",
      "  [0.35185803 0.64860544 0.06151891 0.9797043  0.82781468]\n",
      "  [0.46391225 0.81681318 0.9856427  0.99566879 0.18224726]\n",
      "  [0.19485853 0.18646619 0.95706079 0.41282122 0.25447514]\n",
      "  [0.15984876 0.40287915 0.42810361 0.703509   0.80318853]]]\n",
      "\n",
      "Input sequence has shape (2, 5, 5)\n"
     ]
    }
   ],
   "source": [
    "# Word Vector Representation\n",
    "import numpy as np\n",
    "\n",
    "# Use null token to ensure each input sentence has the same length\n",
    "raw_txt_inputs = ['hello world i am calvin', 'calvin says hello world <null>']\n",
    "word_to_idx = dict()\n",
    "\n",
    "idx = 0\n",
    "for sentence in raw_txt_inputs:\n",
    "    for word in sentence.split():\n",
    "        if word_to_idx.get(word) is None:\n",
    "            word_to_idx[word] = idx\n",
    "            idx += 1\n",
    "\n",
    "# Create a weight matrix for mapping word to its word vector representation\n",
    "vocab_size = len(word_to_idx)\n",
    "word_vec_dim = 5\n",
    "word_embedding_weight = np.random.rand(vocab_size, word_vec_dim)\n",
    "\n",
    "# Convert raw_txt_input to tensor representation\n",
    "index_sequences = []\n",
    "for sentence in raw_txt_inputs:\n",
    "    seq = []\n",
    "    for word in sentence.split():\n",
    "        seq.append(word_to_idx[word])\n",
    "    index_sequences.append(seq)\n",
    "\n",
    "input_sequences = word_embedding_weight[np.array(index_sequences)]\n",
    "\n",
    "print(input_sequences)\n",
    "print('\\nInput sequence has shape', input_sequences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9638781",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rnn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-12ba6272272b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm_recurrent_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLSTMRecurrentModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm_solver\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLSTMSolver\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_util\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_word_based_text_input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'rnn'"
     ]
    }
   ],
   "source": [
    "# Forward Step & Sequence Example\n",
    "\n",
    "def _forward_step(self, x, prev_hidden_state, prev_cell_state):\n",
    "    \"\"\"Forward pass for a single time step of the LSTM layer.\n",
    "\n",
    "    :param np.array x: Input data of shape (N, D)\n",
    "    :param np.array prev_hidden_state: Previous hidden state of shape (N, H)\n",
    "    :param np.array prev_cell_state: Previous cell state of shape (N, H)\n",
    "\n",
    "    Returns tuple:\n",
    "        - next_hidden_state: Next hidden state, of shape (N, H)\n",
    "        - next_cell_state: Next cell state, of shape (N, H)\n",
    "        - cache: Tuple of values needed for back-propagation\n",
    "    \"\"\"\n",
    "    _, H = prev_hidden_state.shape\n",
    "\n",
    "    # Compute activations\n",
    "    acts = np.dot(x, self.Wx) + np.dot(prev_hidden_state, self.Wh) + self.b\n",
    "\n",
    "    # Compute the internal gates\n",
    "    input_gate = sigmoid(acts[:, 0:H])\n",
    "    forget_gate = sigmoid(acts[:, H:2*H])\n",
    "    output_gate = sigmoid(acts[:, 2*H:3*H])\n",
    "    gain_gate = np.tanh(acts[:, 3*H:4*H])\n",
    "\n",
    "    # Compute next states\n",
    "    next_cell_state = forget_gate * prev_cell_state + input_gate * gain_gate\n",
    "    next_hidden_state = output_gate * np.tanh(next_cell_state)\n",
    "\n",
    "    # Cache the results\n",
    "    cache = {\n",
    "        'x': x,\n",
    "        'next-c': next_hidden_state,\n",
    "        'next-h': next_cell_state,\n",
    "        'i-gate': input_gate,\n",
    "        'f-gate': forget_gate,\n",
    "        'o-gate': output_gate,\n",
    "        'g-gate': gain_gate,\n",
    "        'prev-h': prev_hidden_state,\n",
    "        'prev-c': prev_cell_state\n",
    "    }\n",
    "\n",
    "    return next_hidden_state, next_cell_state, cache\n",
    "\n",
    "\n",
    "def forward(self, input_sequence, h0, Wx=None, Wh=None, b=None):\n",
    "    \"\"\"Forward pass for a LSTM layer over an entire sequence of data.\n",
    "    This assumes an input sequence composed of T vectors, each of dimension D.\n",
    "    The LSTM uses a hidden size of H, and it works over a mini-batch containing N sequences.\n",
    "\n",
    "    :param np.array input_sequence: Input data of shape (N, T, D)\n",
    "    :param np.array h0: Initial hidden state of shape (N, H)\n",
    "    :param np.array Wx: Optional input-to-hidden weight matrix, of shape (D, 4H)\n",
    "    :param np.array Wh: Optional hidden-to-hidden weight matrix, of shape (H, 4H)\n",
    "    :param np.array b: Optional bias vector, of shape (4H,)\n",
    "\n",
    "    Returns np.array:\n",
    "        Hidden state over time of shape (N, T, H)\n",
    "    \"\"\"\n",
    "    if Wx is not None and Wh is not None and b is not None:\n",
    "        self.Wx, self.Wh, self.b = Wx, Wh, b\n",
    "\n",
    "    N, T, D = input_sequence.shape\n",
    "    _, H = h0.shape\n",
    "\n",
    "    # Cache the inputs and create time series variables,\n",
    "    # i.e. hidden states over time and cell states over time.\n",
    "    self.input_sequence = input_sequence\n",
    "    self.h0 = h0\n",
    "\n",
    "    self.hidden_states_over_t = np.zeros((N, T, H))\n",
    "    self.cell_states_over_t = np.zeros((N, T, H))\n",
    "    self.caches = dict()\n",
    "\n",
    "    # Run the sequence\n",
    "    prev_hidden_state = h0\n",
    "    prev_cell_state = np.zeros(h0.shape)\n",
    "    for t in range(T):\n",
    "        hidden_state, cell_state, self.caches[t] = self._forward_step(input_sequence[:, t, :],\n",
    "                                                                     prev_hidden_state,\n",
    "                                                                     prev_cell_state)\n",
    "        self.hidden_states_over_t[:, t, :] = hidden_state\n",
    "        self.cell_states_over_t[:, t, :] = cell_state\n",
    "\n",
    "        prev_hidden_state, prev_cell_state = hidden_state, cell_state\n",
    "\n",
    "    return self.hidden_states_over_t\n",
    "\n",
    "\n",
    "# Backprop Step & Sequence\n",
    "\n",
    "def _backward_step(self, grad_next_hidden_state, grad_next_cell_state, cache):\n",
    "    \"\"\"Backward pass for a single time step of the LSTM layer.\n",
    "\n",
    "    Args:\n",
    "        grad_next_hidden_state (np.array): Gradient of next hidden state, of shape (N, H)\n",
    "        grad_next_cell_state (np.array): Gradient of next cell state, of shape (N, H)\n",
    "        cache (tuple): Cache object from the forward pass\n",
    "\n",
    "    Returns tuple:\n",
    "        - grad_x: Gradients of time step input, of shape (N, D)\n",
    "        - grad_prev_hidden_state: Gradients of previous hidden state, of shape (N, H)\n",
    "        - grad_prev_cell_state: Gradients of previous cell state, of shape (N, H)\n",
    "        - grad_Wx: Gradients of input-to-hidden weights, of shape (D, 4H)\n",
    "        - grad_Wh: Gradients of hidden-to-hidden weights, of shape (H, 4H)\n",
    "        - grad_b: Gradients of bias, of shape (4H,)\n",
    "    \"\"\"\n",
    "    x, _, next_c, i_gate, f_gate, o_gate, g_gate, prev_h, prev_c = cache\n",
    "\n",
    "    # Note that grad_prev_c has two contributions, one from grad_next_cell_state and another one from\n",
    "    # grad_next_hidden_state\n",
    "    grad_next_h_next_c = o_gate * ( 1 - (np.tanh(next_c) * np.tanh(next_c)))\n",
    "\n",
    "    grad_prev_cell_state = (grad_next_hidden_state * grad_next_h_next_c * f_gate) + (grad_next_cell_state * f_gate)\n",
    "\n",
    "    # Each gate needs to go through the derivative of non-linearity\n",
    "    grad_i_gate = (grad_next_hidden_state * grad_next_h_next_c * g_gate) + (grad_next_cell_state * g_gate)\n",
    "    grad_i_gate = grad_i_gate * i_gate * (1 - i_gate)\n",
    "\n",
    "    grad_f_gate = (grad_next_hidden_state * grad_next_h_next_c * prev_c) + (grad_next_cell_state * prev_c)\n",
    "    grad_f_gate = grad_f_gate * f_gate * (1 - f_gate)\n",
    "\n",
    "    grad_o_gate = grad_next_hidden_state * np.tanh(next_c)\n",
    "    grad_o_gate = grad_o_gate * o_gate * (1 - o_gate)\n",
    "\n",
    "    grad_g_gate = (grad_next_hidden_state * grad_next_h_next_c * i_gate) + (grad_next_cell_state * i_gate)\n",
    "    grad_g_gate = grad_g_gate * (1 - g_gate * g_gate)\n",
    "\n",
    "    # Now stack them\n",
    "    grad_act = np.concatenate((grad_i_gate, grad_f_gate, grad_o_gate, grad_g_gate), axis=1)\n",
    "\n",
    "    # And then do the same ol' gradient calculations\n",
    "    grad_x = np.dot(grad_act, self.Wx.T)\n",
    "    grad_prev_hidden_state = np.dot(grad_act, self.Wh.T)\n",
    "    grad_Wx = np.dot(x.T, grad_act)\n",
    "    grad_Wh = np.dot(prev_h.T, grad_act)\n",
    "    grad_b = np.sum(grad_act, axis=0)\n",
    "\n",
    "    return grad_x, grad_prev_hidden_state, grad_prev_cell_state, grad_Wx, grad_Wh, grad_b\n",
    "\n",
    "\n",
    "def backward(self, grad_hidden_state_over_t):\n",
    "    \"\"\"Backward pass for a LSTM layer over an entire sequence of data.\n",
    "\n",
    "    Args:\n",
    "        grad_hidden_state (np.array): Upstream gradients of hidden states, of shape (N, T, H)\n",
    "\n",
    "    Returns tuple:\n",
    "        - grad_input_seq: Gradient of the input data, of shape (N, T, D)\n",
    "        - grad_h0: Gradient of the initial hidden state, of shape (N, H)\n",
    "        - grad_Wx: Gradient of input-to-hidden weight matrix, of shape (D, 4H)\n",
    "        - grad_Wh: Gradient of hidden-to-hidden weight matrix, of shape (H, 4H)\n",
    "        - grad_b: Gradient of biases, of shape (4H,)\n",
    "    \"\"\"\n",
    "    N, T, H = grad_hidden_state_over_t.shape\n",
    "    # grad_cell_state_over_t = np.zeros((N, T, H))\n",
    "\n",
    "    grad_input_seq = np.zeros(self.input_sequence.shape)\n",
    "    grad_Wx, grad_Wh, grad_b = np.zeros(self.Wx.shape), np.zeros(self.Wh.shape), np.zeros(self.b.shape)\n",
    "    grad_prev_hidden_state = np.zeros((N, H))\n",
    "    grad_prev_cell_state = np.zeros((N, H))\n",
    "\n",
    "    for t in reversed(range(T)):\n",
    "        time_step_result = self._backward_step(grad_hidden_state_over_t[:, t, :] + grad_prev_hidden_state,\n",
    "                                               grad_prev_cell_state,\n",
    "                                               self.caches[t])\n",
    "        grad_input_seq[:, t, :] = time_step_result[0]\n",
    "        grad_prev_hidden_state = time_step_result[1]\n",
    "        grad_prev_cell_state = time_step_result[2]\n",
    "\n",
    "        # Accumulate\n",
    "        grad_Wx += time_step_result[3]\n",
    "        grad_Wh += time_step_result[4]\n",
    "        grad_b += time_step_result[5]\n",
    "\n",
    "    # Gradient of the initial hidden state is the last grad_prev_hidden_state\n",
    "    grad_h0 = grad_prev_hidden_state\n",
    "\n",
    "    return grad_input_seq, grad_h0, grad_Wx, grad_Wh, grad_b\n",
    "\n",
    "\n",
    "# The softmax score is used to classify word vectors into the appropriate word index.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from rnn.lstm_recurrent_model import LSTMRecurrentModel\n",
    "from rnn.lstm_solver import LSTMSolver\n",
    "from rnn.data_util import load_word_based_text_input\n",
    "\n",
    "x_filepath = 'rnn/datasets/questions.txt'\n",
    "y_filepath = 'rnn/datasets/answers.txt'\n",
    "seq_length = 30\n",
    "\n",
    "questions, answers, word_to_idx, idx_to_word = load_word_based_text_input(x_filepath, y_filepath, seq_length)\n",
    "\n",
    "feed_dict = {\n",
    "    'training_x': questions,\n",
    "    'training_y': answers\n",
    "}\n",
    "\n",
    "model = LSTMRecurrentModel(word_to_idx, idx_to_word)\n",
    "\n",
    "\n",
    "# run a solver with Adam optimizer to train the model\n",
    "\n",
    "solver = LSTMSolver(model, feed_dict=feed_dict,\n",
    "                           batch_size=10,\n",
    "                           num_epochs=300,\n",
    "                           print_every=10,\n",
    "                           learning_rate_decay=0.99,\n",
    "                           update_rule='adam',\n",
    "                           verbose=False)\n",
    "iters, losses = solver.train()\n",
    "\n",
    "plt.plot(iters, losses)\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "\n",
    "from rnn.data_util import convert_string_to_index_matrix\n",
    "\n",
    "input_sequence = convert_string_to_index_matrix(\"What did you eat?\", word_to_idx)\n",
    "model.sample(input_sequence)\n",
    "\n",
    "input_sequence = convert_string_to_index_matrix(\"Where is the sky?\", word_to_idx)\n",
    "model.sample(input_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c57566",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
