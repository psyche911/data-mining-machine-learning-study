{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93c85c95",
   "metadata": {},
   "source": [
    "### Exercise 4.3\n",
    "#### Use either R or Python, with Keras and see the effect of Adam vs SGD for any Classification and Regression problem selected from UCI ML repository. \n",
    "\n",
    "- Then, apply dropouts and compare the generalisation performance. \n",
    "\n",
    "- Compare the performance of dropouts with weight decay (L2 Regularization or Ridge Regression). \n",
    "\n",
    "- Discuss the major similarities and differences between, weight decay and L1/L2 regularisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12822579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.66233766 0.64935064] 6  SGD_all\n",
      "0.6558441519737244 6  mean SGD_all\n",
      "0.006493508815765381 6  std SGD_all\n",
      "[0.72077924 0.63961041] 6  Adam_all\n",
      "0.6801948249340057 6  Adam _all\n",
      "0.040584415197372437 6  Adam _all\n",
      "[0.6785714  0.66558444] 6  SGD2_all\n",
      "[0.66558444 0.64285713] 8  SGD_all\n",
      "0.6542207896709442 8  mean SGD_all\n",
      "0.01136365532875061 8  std SGD_all\n",
      "[0.6785714  0.62012988] 8  Adam_all\n",
      "0.649350643157959 8  Adam _all\n",
      "0.029220759868621826 8  Adam _all\n",
      "[0.66883117 0.64610392] 8  SGD2_all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import *\n",
    "from sklearn import datasets \n",
    "from sklearn.metrics import mean_squared_error \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "#keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dropout\n",
    "from keras.regularizers import l2\n",
    " \n",
    "\n",
    "def read_data(run_num):\n",
    "    # Source: Pima-Indian diabetes dataset: https://www.kaggle.com/kumargh/pimaindiansdiabetescsv\n",
    "    data = genfromtxt(\"datasets/pima.csv\", delimiter=\",\")\n",
    "    data_X = data[:, 0:8]     # all features 0, 1, 2, 3, 4, 5, 6, 7 \n",
    "\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html\n",
    "    # sklearn.preprocessing.Normalizer\n",
    "    #transformer = Normalizer().fit(data_X)  # fit does nothing\n",
    "    #data_X = transformer.transform(data_X)\n",
    "    \n",
    "    data_y = data[:, -1]      # this is target - so that last col is selected from data\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data_X, data_y, test_size=0.40, random_state=run_num)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "    \n",
    "def keras_nn(X_train, X_test, y_train, y_test, type_model, hidden, dropout_rate, learn_rate, run_num):\n",
    " \n",
    "    # https://keras.io/api/models/model_training_apis/\n",
    "    # note that keras model on own ensures that every run begins with different initial \n",
    "    # weights so run_num is not needed \n",
    "\n",
    "    if type_model==0:    # SGD\n",
    "        #nn = MLPClassifier(hidden_layer_sizes=(hidden,), random_state=run_num, \n",
    "                           #max_iter=100,solver='sgd', learning_rate_init=learn_rate )\n",
    "        model = Sequential()\n",
    "        model.add(Dense(hidden, input_dim=X_train.shape[1], activation='relu'))\n",
    "        model.add(Dropout(dropout_rate))   # create a dropout layer with a 40% chance of setting inputs to zero\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.compile(loss='binary_crossentropy', optimizer='sgd',  metrics=['accuracy'])\n",
    "    \n",
    "    elif type_model==1:   # Adam\n",
    "        #nn = MLPClassifier(hidden_layer_sizes=(hidden,), random_state=run_num, \n",
    "                           #max_iter=100,solver='adam', learning_rate_init=learn_rate)\n",
    "        model = Sequential()\n",
    "        model.add(Dense(hidden, input_dim=X_train.shape[1], activation='sigmoid'))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    elif type_model==2:    # Adam with l2 regularisation\n",
    "        model = Sequential()\n",
    "        model.add(Dense(hidden, input_dim=X_train.shape[1], activation='sigmoid', kernel_regularizer=l2(0.001)))\n",
    "        #model.add(Dropout(dropout_rate))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    else:\n",
    "        print('no model')\n",
    "    \n",
    "    # Fit model\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=500, batch_size=10, verbose=0)\n",
    "\n",
    "    # Evaluate the model\n",
    "    # https://keras.io/api/models/model_training_apis/\n",
    "    _, acc_train = model.evaluate(X_train, y_train, verbose=0)\n",
    "    _, acc_test = model.evaluate(X_test, y_test, verbose=0)\n",
    "    #print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "\n",
    "    # Plot history\n",
    "    plt.plot(history.history['accuracy'], label='train')\n",
    "    plt.plot(history.history['val_accuracy'], label='test')\n",
    "    plt.legend()\n",
    "    plt.savefig('figures/'+str(type_model)+'nodp.png') \n",
    "    plt.clf()\n",
    "   \n",
    "    #auc = roc_auc_score(y_pred, y_test, average=None) \n",
    "    return acc_test #,acc_train\n",
    "\n",
    "\n",
    "def main(): \n",
    "\n",
    "    max_expruns = 2\n",
    "\n",
    "    SGD_all = np.zeros(max_expruns) \n",
    "    Adam_all = np.zeros(max_expruns) \n",
    "    \n",
    "    Adam2_all = np.zeros(max_expruns)  \n",
    "    max_hidden = 10 \n",
    "\n",
    "    learn_rate = 0.01\n",
    "\n",
    "    #for learn_rate in range(0.1, 1, 0.2):\n",
    "    \n",
    "    for hidden in range(6, max_hidden, 2):     # only cover 6 hidden neurons for now\n",
    " \n",
    "        for run_num in range(0, max_expruns): \n",
    "    \n",
    "            X_train, X_test, y_train, y_test = read_data(run_num)   \n",
    "            \n",
    "            acc_sgd = keras_nn(X_train, X_test, y_train, y_test, 0, hidden, 0.4, learn_rate, run_num) # SGD with dropout\n",
    "            acc_adam = keras_nn(X_train, X_test, y_train, y_test, 1, hidden, 0.4, learn_rate, run_num) # Adam with dropout\n",
    "            acc_adam2 = keras_nn(X_train, X_test, y_train, y_test, 2, hidden, 0.4, learn_rate, run_num) # Adam with l2\n",
    "           \n",
    "            SGD_all[run_num] = acc_sgd\n",
    "            Adam_all[run_num] = acc_adam\n",
    "\n",
    "            Adam2_all[run_num] = acc_adam2   # two hidden layers\n",
    "        \n",
    "        print(SGD_all, hidden,' SGD_all')\n",
    "        print(np.mean(SGD_all), hidden, ' mean SGD_all')\n",
    "        print(np.std(SGD_all), hidden, ' std SGD_all')\n",
    "\n",
    "        print(Adam_all, hidden,' Adam_all')\n",
    "        print(np.mean(Adam_all), hidden, ' Adam _all')\n",
    "        print(np.std(Adam_all), hidden, ' Adam _all')\n",
    "\n",
    "        print(Adam2_all, hidden,' SGD2_all')\n",
    "\n",
    "        # you can also print  for Adam \n",
    " \n",
    "    # next try a paragraph to describe your results and discuss which models are better to use\n",
    "    # repeat for another dataset\n",
    "    # you can save results to a file as well\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8677a007",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981739f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
