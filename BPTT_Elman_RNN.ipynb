{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f951dd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ForwardPass(self, sample, slide):\n",
    "    sample_time = sample[slide]\n",
    "    layer = 0 \n",
    "    weightsum = 0.0\n",
    "    StateWeightSum = 0.0\n",
    "    forwardout = 0.0 \n",
    "    \n",
    "    for row in range(0, self.Top[0]):\n",
    "        self.InlayerOutL0[slide+1][row] = sample_time[row]\n",
    "\n",
    "    for y in range(0, self.Top[1]):\n",
    "        for x in range(0, self.Top[0]):\n",
    "            weightsum += self.InlayerOutL0[slide+1][x] * self.W1[x,y]\n",
    "\n",
    "        for x in range(0, self.Top[1]):\n",
    "            StateWeightSum += self.OutputSlideL1[slide][x] * self.StateW[x,y]\n",
    "            \n",
    "        forwardout = (weightsum + StateWeightSum) - self.B1[y]\n",
    "\n",
    "        self.OutputSlideL1[slide+1][y] = self.sigmoid(forwardout)\n",
    "        \n",
    "        weightsum = 0\n",
    "        StateWeightSum = 0\n",
    "\n",
    "    layer = 1 #   hidden layer to output\n",
    "    weightsum = 0.0\n",
    "    #print(self.out, end=' ')\n",
    "    \n",
    "    for y in range(0, self.Top[layer+1]):\n",
    "        for x in range(0, self.Top[layer]):\n",
    "            weightsum += self.OutputSlideL1[slide+1][x] * self.W2[x,y]\n",
    "            forwardout = (weightsum - self.B2[y])\n",
    "            \n",
    "        self.OutputSlideL2[slide+1][y] = self.sigmoid(forwardout) \n",
    "        \n",
    "        weightsum = 0.0\n",
    "        StateWeightSum = 0.0\n",
    "\n",
    "\n",
    "def BPTT(self):  \n",
    "    for i,sample in enumerate(self.Train_x):\n",
    "        self.StateOut = np.ones(self.Top[1])\n",
    "        self.ErL1 = np.zeros((len(sample)+1, self.Top[1]))   # need to modify for multiple layers\n",
    "        self.OutputSlideL1 = np.zeros((len(sample)+1, self.Top[1]))\n",
    "        \n",
    "        for x in range(0,self.Top[1]):\n",
    "            self.OutputSlideL1[0][x] = self.StateOut[x]\n",
    "            \n",
    "        self.InlayerOutL0 = np.zeros((len(sample)+1, self.Top[0]))\n",
    "        self.OutputSlideL2 = np.zeros((len(sample)+1, self.Top[2]))\n",
    "        \n",
    "        for slide in range(0, len(sample)):\n",
    "            self.ForwardPass(sample, slide)\n",
    "            \n",
    "        for slide in range(len(sample),0,-1):\n",
    "            self.BackwardPass(sample, self.learn_rate, self.Train_y[i-1], slide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28a6a248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]]  input vec\n",
      "[[-1.73665315 -2.40366542 -2.72344361]\n",
      " [ 1.61591482  1.45557046  1.13262256]\n",
      " [ 1.68977504  1.54059305  1.21757531]]\n",
      "[[-2.15023381 -2.41205828 -2.71701457]\n",
      " [ 1.71962883  1.45767515  1.13101034]\n",
      " [ 1.80488553  1.542929    1.21578594]]\n",
      "[[-2.15024751 -2.41207375 -2.720968  ]\n",
      " [ 1.71963227  1.45767903  1.13200175]\n",
      " [ 1.80488935  1.54293331  1.21688628]]\n"
     ]
    }
   ],
   "source": [
    "# one to many RNN model\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "class RecurrentNetwork(object):\n",
    "    \"\"\"When we say W_hh, it means a weight matrix that accepts a hidden state and produce a new hidden state.\n",
    "    Similarly, W_xh represents a weight matrix that accepts an input vector and produce a new hidden state. This\n",
    "    notation can get messy as we get more variables later on with LSTM and I simplify the notation a little bit in\n",
    "    LSTM notes.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.hidden_state = np.zeros((3, 3))\n",
    "        self.W_hh = np.random.randn(3, 3)\n",
    "        self.W_xh = np.random.randn(3, 3)\n",
    "        self.W_hy = np.random.randn(3, 3)\n",
    "        self.Bh = np.random.randn(3,)\n",
    "        self.By = np.random.rand(3,)\n",
    "\n",
    "    def forward_prop(self, x):\n",
    "        # The order of which you do dot product is entirely up to you.\n",
    "        # The gradient updates will take care itself as long as the matrix dimension matches up.\n",
    "        self.hidden_state = np.tanh(np.dot(self.hidden_state, self.W_hh) + np.dot(x, self.W_xh) + self.Bh)\n",
    "        \n",
    "        return self.W_hy.dot(self.hidden_state) + self.By\n",
    "\n",
    "\n",
    "input_vector = np.ones((3, 3))\n",
    "print(input_vector, ' input vec')\n",
    "silly_network = RecurrentNetwork()\n",
    "\n",
    "# Notice that same input, but leads to different ouptut at every single time step.\n",
    "print(silly_network.forward_prop(input_vector))\n",
    "print(silly_network.forward_prop(input_vector))\n",
    "print(silly_network.forward_prop(input_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867b483f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
