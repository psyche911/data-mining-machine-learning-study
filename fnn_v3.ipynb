{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c832273c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://github.com/rohitash-chandra\n",
    "# Sigmoid units used in hidden and output   \n",
    "# This version will demonstrate momemntum and stocastic gradient descent \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import random\n",
    "import time\n",
    " \n",
    "class Network:\n",
    "    def __init__(self, Topo, Train, Test, MaxTime, MinPer, learnRate, use_stocasticGD, use_vanillalearning, momentum_rate): \n",
    "        self.Top = Topo     # NN topology [input, hidden, output]\n",
    "        self.Max = MaxTime  # Max epocs\n",
    "        self.TrainData = Train\n",
    "        self.TestData = Test\n",
    "        self.NumSamples = Train.shape[0]\n",
    "        self.learn_rate = learnRate\n",
    "        self.minPerf = MinPer\n",
    "        \n",
    "        # Initialise weights (W1 W2) and bias (b1 b2) of the network\n",
    "        np.random.seed() \n",
    "        self.W1 = np.random.uniform(-0.5, 0.5, (self.Top[0], self.Top[1]))  \n",
    "        #print(self.W1, ' self.W1')\n",
    "        self.B1 = np.random.uniform(-0.5, 0.5, (1, self.Top[1]))   # Bias first layer\n",
    "        #print(self.B1, ' self.B1')\n",
    "        self.BestB1 = self.B1\n",
    "        self.BestW1 = self.W1 \n",
    "        self.W2 = np.random.uniform(-0.5, 0.5, (self.Top[1], self.Top[2]))   \n",
    "        self.B2 = np.random.uniform(-0.5, 0.5, (1,self.Top[2]))    # Bias second layer\n",
    "        self.BestB2 = self.B2\n",
    "        self.BestW2 = self.W2\n",
    "        \n",
    "        self.hidout = np.zeros(self.Top[1]) # Output of first hidden layer\n",
    "        self.out = np.zeros(self.Top[2])    # Output last layer\n",
    "\n",
    "        self.hid_delta = np.zeros(self.Top[1])  # Output of first hidden layer\n",
    "        self.out_delta = np.zeros(self.Top[2])  # Output last layer\n",
    "\n",
    "        self.vanilla = use_vanillalearning     # Canonical batch training mode - use full data set - no SGD\n",
    "        self.momenRate = momentum_rate\n",
    "        self.stocasticGD = use_stocasticGD\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def softmax(self, x):\n",
    "        # Numerically stable with large exponentials\n",
    "        exps = np.exp(x - x.max())\n",
    "        return exps / np.sum(exps, axis=0)\n",
    "\n",
    "    def sampleEr(self, actualout):\n",
    "        error = np.subtract(self.out, actualout)\n",
    "        sqerror = np.sum(np.square(error))/self.Top[2] \n",
    "        return sqerror\n",
    "\n",
    "    def ForwardPass(self, X): \n",
    "        z1 = X.dot(self.W1) - self.B1  \n",
    "        self.hidout = self.sigmoid(z1)   # Output of first hidden layer   \n",
    "        z2 = self.hidout.dot(self.W2) - self.B2 \n",
    "        self.out = self.sigmoid(z2)      # Output second hidden layer\n",
    "\n",
    "    def BackwardPass(self, input_vec, desired):   \n",
    "        out_delta = (desired - self.out)*(self.out*(1-self.out))  \n",
    "        hid_delta = out_delta.dot(self.W2.T) * (self.hidout * (1-self.hidout)) \n",
    "\n",
    "        if self.vanilla == True:    # No momentum \n",
    "            self.W2 += self.hidout.T.dot(out_delta) * self.learn_rate\n",
    "            self.B2 += (-1 * self.learn_rate * out_delta)\n",
    "            self.W1 += (input_vec.T.dot(hid_delta) * self.learn_rate) \n",
    "            self.B1 += (-1 * self.learn_rate * hid_delta) \n",
    "        else:    # Use momentum\n",
    "            v2 = self.W2.copy()    # Save previous weights http://cs231n.github.io/neural-networks-3/#sgd\n",
    "            v1 = self.W1.copy()\n",
    "            b2 = self.B2.copy()\n",
    "            b1 = self.B1.copy()\n",
    "\n",
    "            self.W2 += (v2 *self.momenRate) + (self.hidout.T.dot(out_delta) * self.learn_rate)# velocity update\n",
    "            self.W1 += (v1 *self.momenRate) + (input_vec.T.dot(hid_delta) * self.learn_rate)   \n",
    "            self.B2 += (b2 *self.momenRate) + (-1 * self.learn_rate * out_delta)    # velocity update\n",
    "            self.B1 += (b1 *self.momenRate) + (-1 * self.learn_rate * hid_delta)\n",
    "\n",
    "    def TestNetwork(self, Data, tolerance):\n",
    "        Input = np.zeros((1, self.Top[0]))    # temp hold input\n",
    "        Desired = np.zeros((1, self.Top[2])) \n",
    "        nOutput = np.zeros((1, self.Top[2]))\n",
    "        testSize = Data.shape[0]\n",
    "        \n",
    "        sse = 0\n",
    "        clasPerf = 0\n",
    "        \n",
    "        self.W1 = self.BestW1\n",
    "        self.W2 = self.BestW2   # load best knowledge\n",
    "        self.B1 = self.BestB1\n",
    "        self.B2 = self.BestB2   # load best knowledge\n",
    "\n",
    "        for s in range(0, testSize):\n",
    "            Input = Data[s, 0:self.Top[0]] \n",
    "            Desired = Data[s, self.Top[0]:] \n",
    "\n",
    "            self.ForwardPass(Input) \n",
    "            sse += self.sampleEr(Desired)\n",
    "\n",
    "            pred_binary = np.where(self.out > (1 - tolerance), 1, 0)\n",
    "\n",
    "            if((Desired==pred_binary).all()):\n",
    "                clasPerf += 1   \n",
    "\n",
    "            #if(np.isclose(self.out, Desired, atol=erTolerance).any()):\n",
    "                #clasPerf += 1  \n",
    "\n",
    "        return (sse/testSize, float(clasPerf)/testSize * 100)\n",
    "\n",
    "\n",
    "    def saveKnowledge(self):\n",
    "        self.BestW1 = self.W1\n",
    "        self.BestW2 = self.W2\n",
    "        self.BestB1 = self.B1\n",
    "        self.BestB2 = self.B2 \n",
    "\n",
    "        #print (self.BestW1, self.BestW2, self.BestB1, self.BestB2)\n",
    "\n",
    "    def BP_GD(self, trainTolerance):  \n",
    "        Input = np.zeros((1, self.Top[0])) # Temp hold input\n",
    "        Desired = np.zeros((1, self.Top[2])) \n",
    "\n",
    "        #minibatchsize = int(0.1* self.TrainData.shape[0]) \n",
    "        # Choose a mini-batch size for SGD - optional exercise to implement this\n",
    "\n",
    "        Er = [] \n",
    "        epoch = 0\n",
    "        bestRMSE = 10000 # Assign a large number in begining to maintain best (lowest RMSE)\n",
    "        bestTrain = 0\n",
    "        while  epoch < self.Max and bestTrain < self.minPerf :\n",
    "            sse = 0\n",
    "\n",
    "            for s in range(0, self.TrainData.shape[0]):\n",
    "                if(self.stocasticGD==True):   \n",
    "                    pat = random.randint(0, self.TrainData.shape[0]-1) \n",
    "                    # Data shuffle in SGD:\n",
    "                    # https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.random.randint.html\n",
    "                else:\n",
    "                    pat = s # no data shuffle in SGD\n",
    "\n",
    "                Input[:] = self.TrainData[pat, 0:self.Top[0]]  \n",
    "                Desired[:] = self.TrainData[pat, self.Top[0]:]  \n",
    "\n",
    "                self.ForwardPass(Input)  \n",
    "                self.BackwardPass(Input, Desired)\n",
    "                sse += self.sampleEr(Desired)\n",
    "\n",
    "            rmse = np.sqrt(sse/self.TrainData.shape[0]*self.Top[2])\n",
    "\n",
    "            if rmse < bestRMSE: \n",
    "                self.saveKnowledge()\n",
    "                (bestRMSE, bestTrain) = self.TestNetwork(self.TrainData, trainTolerance)\n",
    "                 #print(bestRMSE, bestTrain)\n",
    "\n",
    "            Er = np.append(Er, rmse)\n",
    "\n",
    "            epoch += 1  \n",
    "\n",
    "        return (Er, bestRMSE, bestTrain, epoch) \n",
    "\n",
    "def normalisedata(data, inputsize, outsize): # Normalise the data between [0,1]\n",
    "    traindt = data[:, np.array(range(0,inputsize))]\t\n",
    "    dt = np.amax(traindt, axis=0)\n",
    "    tds = abs(traindt/dt) \n",
    "    return np.concatenate((tds[:, range(0,inputsize)], data[:, range(inputsize, inputsize+outsize)]), axis=1)\n",
    "\n",
    "def main():\n",
    "    problem = 1    # [1,2,3] choose your problem  \n",
    "\n",
    "    if problem == 1:\n",
    "        TrDat = np.loadtxt(\"data/train.csv\", delimiter=',')   # Iris classification problem (UCI dataset)\n",
    "        TesDat = np.loadtxt(\"data/test.csv\", delimiter=',')  \n",
    "        Hidden = 6\n",
    "        Input = 4\n",
    "        Output = 2 \n",
    "        #https://stats.stackexchange.com/questions/207049/neural-network-for-binary-classification-use-1-or-2-output-neurons\n",
    "\n",
    "        TrainData = normalisedata(TrDat, Input, Output) \n",
    "        TestData = normalisedata(TesDat, Input, Output)\n",
    "        MaxTime = 1000\n",
    "        MinCriteria = 95 #Stop when learn 95 percent\n",
    "\n",
    "    elif problem == 2:\n",
    "        import sklearn  \n",
    "        from sklearn import datasets \n",
    "        from sklearn.model_selection import train_test_split \n",
    "        import pandas as pd\n",
    "\n",
    "        df = pd.read_csv('datasets/diabetes.csv') \n",
    "        #https://www.kaggle.com/uciml/pima-indians-diabetes-database/data?select=diabetes.csv\n",
    "        #Print(df.shape)\n",
    "        print(df.describe().transpose())\n",
    "        #https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.transpose.html\n",
    "\n",
    "        target_column = ['Outcome'] \n",
    "        predictors = list(set(list(df.columns))-set(target_column))\n",
    "        df[predictors] = df[predictors]/df[predictors].max() \n",
    "        #https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html\n",
    "\n",
    "        X = df[predictors].values\n",
    "        y = df[target_column].values\n",
    "\n",
    "        #https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=40)\n",
    "        print(X_train.shape); print(X_test.shape)\n",
    "\n",
    "        TrainData = np.hstack((X_train, y_train))\n",
    "        TestData = np.hstack((X_test, y_test))\n",
    "\n",
    "        Hidden = 20\n",
    "        Input = 8\n",
    "        Output = 1  \n",
    "\n",
    "        MaxTime = 5000\n",
    "        MinCriteria = 95     # Stop when learn 95 percent \n",
    "\n",
    "    Topo = [Input, Hidden, Output] \n",
    "    MaxRun = 5     # Number of experimental runs \n",
    "\n",
    "    trainTolerance = 0.25   # [eg 0.15 would be seen as 0] [ 0.81 would be seen as 1]\n",
    "    testTolerance = 0.49\n",
    "    learnRate = 0.1  \n",
    "\n",
    "    useStocastic = True   # False for vanilla BP with SGD (no shuffle of data).\n",
    "                          # True for BP with SGD (shuffle of data at every epoch)\n",
    "    updateStyle = True    # True for Vanilla SGD, False for momentum  SGD\n",
    "\n",
    "    momentum_rate = 0.001   # 0.1 ends up having very large weights. you can try and see\n",
    "\n",
    "    trainPerf = np.zeros(MaxRun)\n",
    "    testPerf = np.zeros(MaxRun)\n",
    "\n",
    "    trainMSE = np.zeros(MaxRun)\n",
    "    testMSE = np.zeros(MaxRun)\n",
    "    Epochs = np.zeros(MaxRun)\n",
    "    Time = np.zeros(MaxRun)\n",
    "\n",
    "    for run in range(0, MaxRun):\n",
    "        print(run, ' is experimental run') \n",
    "\n",
    "        fnn = Network(Topo, TrainData, TestData, MaxTime, MinCriteria, learnRate, useStocastic, updateStyle, momentum_rate)\n",
    "        start_time = time.time()\n",
    "        (erEp, trainMSE[run], trainPerf[run], Epochs[run]) = fnn.BP_GD(trainTolerance)\n",
    "\n",
    "        Time[run] = time.time() - start_time\n",
    "        (testMSE[run], testPerf[run]) = fnn.TestNetwork(TestData, testTolerance)\n",
    "        print(trainMSE[run], trainPerf[run], testMSE[run], testPerf[run])\n",
    "\n",
    "    print('classification performance for each experimental run') \n",
    "    print(trainPerf)\n",
    "    print(testPerf)\n",
    "\n",
    "    print('RMSE performance for each experimental run') \n",
    "    print(trainMSE)\n",
    "    print(testMSE)\n",
    "\n",
    "    print('Epocs and Time taken for each experimental run') \n",
    "    print(Epochs)\n",
    "    print(Time)\n",
    "\n",
    "    print('mean and std of classification performance')\n",
    "    print(np.mean(trainPerf), np.std(trainPerf))\n",
    "    print(np.mean(testPerf), np.std(testPerf))\n",
    "\n",
    "    print('mean and std of computational time taken')\n",
    "    print(np.mean(Time), np.std(Time))\n",
    "\n",
    "    # Fig of last run\n",
    "    plt.figure()\n",
    "    plt.plot(erEp )\n",
    "    plt.ylabel('error')  \n",
    "    plt.savefig('out.png')\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\": \n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
