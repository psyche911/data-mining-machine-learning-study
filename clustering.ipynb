{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8183d484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifification  is our problem\n",
      " silhouette_scores: \n",
      " [0.5687897205830246, 0.5104287492214447, 0.425157227030241, 0.3953787487100022, 0.33365576673579317, 0.33359227428351756, 0.358176918357721]\n",
      "k_max:  2 \n",
      "score_max:  0.5687897205830246\n",
      " silhouette_scores: \n",
      " [0.5687897205830247, 0.5104287492214447, 0.425157227030241, 0.39516590580935257, 0.3336557667357931, 0.328626219441354, 0.35757978037085847]\n",
      "k_max:  2 \n",
      "score_max:  0.5687897205830247\n",
      " silhouette_scores: \n",
      " [0.5687897205830247, 0.5104287492214447, 0.4277493768982135, 0.39516590580935257, 0.3292568959620405, 0.3592462543688715, 0.35817691835772086]\n",
      "k_max:  2 \n",
      "score_max:  0.5687897205830247\n",
      " silhouette_scores: \n",
      " [0.5687897205830247, 0.5104287492214447, 0.425157227030241, 0.3969180143549049, 0.3292568959620405, 0.33359227428351756, 0.358176918357721]\n",
      "k_max:  2 \n",
      "score_max:  0.5687897205830247\n",
      " silhouette_scores: \n",
      " [0.5687897205830247, 0.5104287492214447, 0.425157227030241, 0.39365397479545655, 0.3336557667357931, 0.3592462543688715, 0.358176918357721]\n",
      "k_max:  2 \n",
      "score_max:  0.5687897205830247\n",
      "[0.66558442 0.6461039  0.66558442 0.59415584 0.66558442]  nn_all\n",
      "0.6474025974025974  mean nn_all\n",
      "0.02767178669176953  std nn_all\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import *\n",
    "import random\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.tree import export_text\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "def read_data(run_num, prob):\n",
    "\n",
    "    normalise = False\n",
    "    \n",
    "    if prob == 'classifification': \n",
    "        # Source: Pima-Indian diabetes dataset: https://www.kaggle.com/kumargh/pimaindiansdiabetescsv\n",
    "        data_in = genfromtxt(\"datasets/pima.csv\", delimiter=\",\")\n",
    "        data_inputx = data_in[:, 0:8]  # all features 0-7 \n",
    "        data_inputy = data_in[:, -1]   # this is target - so that last col is selected from data\n",
    "\n",
    "    elif prob == 'regression': # energy - regression prob\n",
    "        data_in = genfromtxt('datasets/energy/ENB2012_data.csv', delimiter=\",\")  # you can replace this with Abalone\n",
    "        data_inputx = data_in[:, 0:8]  # all features 0-7\n",
    "        data_inputy = data_in[:, 8]    # this is target - just the heating load selected from data  \n",
    "\n",
    "    if normalise == True:\n",
    "        transformer = Normalizer().fit(data_inputx)  # fit does nothing\n",
    "        data_inputx = transformer.transform(data_inputx) \n",
    " \n",
    "    x_train, x_test, y_train, y_test = train_test_split(data_inputx, data_inputy, test_size=0.40, random_state=run_num)\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\n",
    "def dimen_reduction(x_train, x_test, type_model):\n",
    "    # Scikit-learn is using SVD for PCA\n",
    "    if type_model == 0:    # SVD solver - full\n",
    "        pca = PCA(n_components=3, svd_solver='full')   # SVD LAPACK Solver\n",
    "        \n",
    "    elif type_model == 1:  # SVD \n",
    "        pca = PCA(n_components=3, svd_solver='arpack') # SVD AROACK Solver\n",
    "\n",
    "    # note number of components can be changed to 0.95\n",
    "    # but since we have different train and test data, that can create problems\n",
    "    # it is best to combine both train and test data and then split them back again\n",
    "\n",
    "    #data = np.vstack((x_train,x_test)) # something along these lines\n",
    "\n",
    "    #print(data.shape, ' * ')\n",
    "\n",
    "    reduced_datatrain = pca.fit_transform(x_train)\n",
    "    train_varianceratio = pca.explained_variance_ratio_\n",
    "\n",
    "    reduced_datatest = pca.fit_transform(x_test)\n",
    "    test_varianceratio = pca.explained_variance_ratio_\n",
    "\n",
    "    return reduced_datatrain, reduced_datatest, test_varianceratio, train_varianceratio\n",
    " \n",
    "\n",
    "def kmeans_clustering(x_train, x_test, type_model): \n",
    "\n",
    "    X = np.vstack((x_train, x_test)) # combining data - can be also separate \n",
    "\n",
    "    if type_model == 0: \n",
    "\n",
    "        scores = []\n",
    "        for k in range(2, 9):\n",
    "            kmeans = KMeans(n_clusters=k)\n",
    "            y_pred = kmeans.fit_predict(X)\n",
    "            score = silhouette_score(X, kmeans.labels_)\n",
    "            scores.append(score)\n",
    "\n",
    "        print(' silhouette_scores: \\n', scores)\n",
    "        score_max = np.max(scores)\n",
    "        k_max = np.argmax(scores)  + 2\n",
    "        print('k_max: ', k_max, '\\nscore_max: ' , score_max)\n",
    "\n",
    "    elif type_model == 1: \n",
    "        print('aglo clustering ')\n",
    "\n",
    "        # get dendogram\n",
    "        # todo\n",
    "\n",
    "    elif type_model == 2: \n",
    "        print('DBSCAN clustering ')\n",
    "        #todo\n",
    "\n",
    "    return scores, k_max  # need to organise what to return\n",
    "\n",
    "    \n",
    "def scipy_models(x_train, x_test, y_train, y_test, type_model, hidden, learn_rate, run_num, problem): \n",
    "\n",
    "    if problem == 'classifification':\n",
    "        if type_model == 0:   # SGD \n",
    "            model = MLPClassifier(hidden_layer_sizes=(hidden,), random_state=run_num, \n",
    "                                  max_iter=100, solver='sgd', learning_rate_init=learn_rate)\n",
    "            \n",
    "        elif type_model == 1: #https://scikit-learn.org/stable/modules/tree.html (see how tree can be visualised)\n",
    "            model = DecisionTreeClassifier(random_state=0, max_depth=tree_depth)\n",
    "            \n",
    "        elif type_model == 2:\n",
    "            model = RandomForestClassifier(n_estimators=100, max_depth=tree_depth, random_state=run_num)             \n",
    "\n",
    "    elif problem == 'regression':\n",
    "        if type_model == 0:   # SGD  \n",
    "            model = MLPRegressor(hidden_layer_sizes=(hidden*3,), random_state=run_num, \n",
    "                                 max_iter=500, solver='adam', learning_rate_init=learn_rate)\n",
    "            \n",
    "        elif type_model == 1:  \n",
    "            model = DecisionTreeRegressor(random_state=0, max_depth=tree_depth)\n",
    "            \n",
    "        elif type_model == 2: \n",
    "            model = RandomForestRegressor(n_estimators=100, max_depth=tree_depth, random_state=run_num)        \n",
    "   \n",
    "    # Train the model using the training sets\n",
    "    model.fit(x_train, y_train)   \n",
    "\n",
    "    if type_model == 1:\n",
    "        r = export_text(model)\n",
    "        print(r)\n",
    "\n",
    "    # Make predictions using the testing set\n",
    "    y_pred_test = model.predict(x_test)\n",
    "    y_pred_train = model.predict(x_train) \n",
    "\n",
    "    if problem == 'regression':\n",
    "        perf_test = np.sqrt(mean_squared_error(y_test, y_pred_test)) \n",
    "        perf_train = np.sqrt(mean_squared_error(y_train, y_pred_train)) \n",
    "\n",
    "    if problem == 'classifification': \n",
    "        perf_test = accuracy_score(y_pred_test, y_test) \n",
    "        perf_train = accuracy_score(y_pred_train, y_train) \n",
    "        cm = confusion_matrix(y_pred_test, y_test) \n",
    "        #print(cm, 'is confusion matrix')\n",
    "        #auc = roc_auc_score(y_pred, y_test, average=None) \n",
    "\n",
    "    return perf_test #,perf_train\n",
    "\n",
    "\n",
    "def main(): \n",
    "\n",
    "    max_expruns = 5\n",
    "\n",
    "    nn_all = np.zeros(max_expruns) \n",
    "    nnpca_all = np.zeros(max_expruns)   \n",
    "\n",
    "    learn_rate = 0.01\n",
    "    hidden = 8\n",
    "\n",
    "    prob = 'classifification' # classification  or regression \n",
    "    #prob = 'regression'     # classification  or regression \n",
    "\n",
    "    # classifcation accurary is reported for classification and RMSE for regression\n",
    "\n",
    "    print(prob, ' is our problem') \n",
    " \n",
    "    for run_num in range(0, max_expruns): \n",
    "\n",
    "        x_train, x_test, y_train, y_test = read_data(run_num, prob)   \n",
    "        \n",
    "        acc_nn = scipy_models(x_train, x_test, y_train, y_test, 0, hidden, learn_rate, run_num, prob) # SGD \n",
    "\n",
    "        [scores, k_max] = kmeans_clustering(x_train, x_test, 0)\n",
    "\n",
    "        # 0 is for PCA (you can try 1 for case of SVD)\n",
    "        [reduced_datatrain, reduced_datatest, variance_scoretrain, variance_scoretest] = dimen_reduction(x_train, x_test, 0)\n",
    "\n",
    "        #print(reduced_datatrain, variance_scoretrain, ' reduced_datatrain - variance_scoretrain')\n",
    "\n",
    "        #acc_nnpca = scipy_models(reduced_datatrain, reduced_datatest, y_train, y_test, 0, hidden, learn_rate, run_num, prob) #SGD  after PCA\n",
    "       \n",
    "        nn_all[run_num] = acc_nn\n",
    "        \n",
    "    print(nn_all,' nn_all')\n",
    "    print(np.mean(nn_all), ' mean nn_all')\n",
    "    print(np.std(nn_all), ' std nn_all') \n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "     main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb219b3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
