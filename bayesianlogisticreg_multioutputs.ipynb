{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16b95ee0",
   "metadata": {},
   "source": [
    "- Bayesian linear regression for a single step ahead (Sunspot time series) and multi-step ahead time series prediction (MMM stock market). Below you can see trace-plot and histogram of the posterior distribution. Note that Bayesian multinomial regression can be developed using this code for multi-class classification problems. \n",
    "\n",
    "- We note that in multi-step time series prediction, 5 step-ahead would mean 5 output neurons. We use sigmoid units in the output layer. Note that gradients are not used and you can compare the results with SGD which is present in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6f3bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.54000e-04 3.73900e-03 1.98500e-03 ... 4.20000e-03 1.06200e-03\n",
      "  3.97000e-03]\n",
      " [1.98500e-03 0.00000e+00 2.30800e-03 ... 3.97000e-03 7.84700e-03\n",
      "  1.12160e-02]\n",
      " [2.30800e-03 4.29300e-03 1.84600e-03 ... 1.12160e-02 1.05240e-02\n",
      "  1.03390e-02]\n",
      " ...\n",
      " [6.93515e-01 7.01704e-01 6.94112e-01 ... 6.94491e-01 6.76650e-01\n",
      "  6.92593e-01]\n",
      " [6.94112e-01 7.09296e-01 6.94166e-01 ... 6.92593e-01 6.84730e-01\n",
      "  6.97528e-01]\n",
      " [6.94166e-01 6.92539e-01 6.96552e-01 ... 6.97528e-01 7.05500e-01\n",
      "  7.06259e-01]]\n",
      "[[ 0.19501245 -0.42787433 -0.44500019 -0.42245869  0.08695229]\n",
      " [-0.09754217  0.12578379 -0.29525026  0.30677844  0.00785604]\n",
      " [ 0.17812435  0.15360126  0.35716324 -0.45325319 -0.3546039 ]\n",
      " [ 0.24864554  0.37547593  0.13433643 -0.49453747 -0.13222017]\n",
      " [-0.1182563  -0.04220437 -0.23727212  0.14013973  0.26251846]]  self.w init\n",
      "[ 0.43389103 -0.4185525  -0.37356592  0.37067382  0.27667912]  self.b init\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\di_ma\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:87: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "C:\\Users\\di_ma\\AppData\\Local\\Temp/ipykernel_11208/1386866908.py:40: RuntimeWarning: overflow encountered in square\n",
      "  return  np.sum(np.square(prediction - actual))/prediction.shape[0]   # to cater more in one output/class\n",
      "C:\\Users\\di_ma\\AppData\\Local\\Temp/ipykernel_11208/1386866908.py:62: RuntimeWarning: overflow encountered in add\n",
      "  self.b += -1 * self.learn_rate * self.out_delta[y]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 nan  class_perf, rmse\n",
      "0.0 nan  class_perf, rmse\n",
      "0.0 0.0 nan nan  train_perc, test_perc, rmse_train, rmse_test using SGD\n",
      "[[ 0.36494371  0.36991867 -0.05431822 -0.11429268 -0.45060512]\n",
      " [-0.28451835 -0.45220988 -0.16419805  0.16286393  0.41036738]\n",
      " [-0.1481562   0.07515119 -0.21526428  0.12937607 -0.00149895]\n",
      " [ 0.43686832 -0.49307152 -0.38069026  0.43669857 -0.34674241]\n",
      " [ 0.16502611  0.02880121 -0.47588164 -0.27124769  0.1523828 ]]  self.w init\n",
      "[-0.42062668  0.0803283  -0.02793981  0.11972065  0.30663819]  self.b init\n",
      "0.12950681444079562  tau_pro\n",
      "-3242.734288225582  initial likelihood\n",
      "0 -3072.1276649638476 -24.452419437721623 0.4736727126357085 0.4740308078279326  accepted\n",
      "1 -3037.944415359088 -24.45641673134471 0.47085775527130047 0.471353658631431  accepted\n",
      "4 -2935.1105244659816 -24.443571168926482 0.4640402258635376 0.46488326531968516  accepted\n",
      "5 -2884.7337295813973 -24.407031552608256 0.4606842637141537 0.4636974540567485  accepted\n",
      "6 -2648.410621407719 -24.384593757862312 0.44399037811822717 0.4417288488935568  accepted\n",
      "7 -2612.3735706582597 -24.346313097483367 0.44053361628030724 0.4482499701666503  accepted\n",
      "10 -2534.0471895337255 -24.357825959578115 0.43494434407475074 0.4455924292226048  accepted\n",
      "16 -2455.1391534788145 -24.34895698304069 0.42903567323705327 0.44197603239210875  accepted\n",
      "17 -2328.210021775698 -24.36120819431903 0.42038790798485887 0.44013241557429167  accepted\n",
      "19 -2065.8976525637136 -24.359459382182884 0.4001925215720044 0.4125915898895539  accepted\n",
      "23 -2025.668546462024 -24.394494846783765 0.39738784363509494 0.4079280398615232  accepted\n",
      "25 -1798.1884287677772 -24.365718636151673 0.3779951762753305 0.3623537242367204  accepted\n",
      "27 -1730.2768145895873 -24.340013754825982 0.3717989932315745 0.3564560114594062  accepted\n",
      "29 -1727.4870763827673 -24.346030761272843 0.3715097103200206 0.33849083154045323  accepted\n",
      "32 -1587.287626606731 -24.304136262725805 0.35911014280818176 0.3297225984893882  accepted\n",
      "33 -1573.9907637073607 -24.294133057525265 0.3579287121668938 0.33441230401750205  accepted\n",
      "34 -1544.9772383086136 -24.28649361043341 0.35531613233118486 0.31870598024543995  accepted\n",
      "39 -1502.9054820362549 -24.277849003600245 0.3516226040349617 0.30580947764589256  accepted\n",
      "40 -1483.1736824582679 -24.30411997965495 0.3497996039383439 0.29953680609804895  accepted\n",
      "41 -1476.1138179726804 -24.291095071196416 0.34916680621513735 0.2956620945178628  accepted\n",
      "42 -1365.868546817629 -24.24748346830655 0.33935168023752577 0.29782862476586935  accepted\n",
      "43 -1351.3525383712426 -24.260841851668886 0.33795239476315797 0.30013566551322063  accepted\n",
      "47 -1340.8727203925464 -24.250532268129803 0.3370780996133957 0.30647297867451184  accepted\n",
      "48 -1234.6944415599469 -24.27651677465584 0.3268877329397979 0.270254305211727  accepted\n",
      "50 -1143.3988139115463 -24.29349616467808 0.3182444994183363 0.2756124298763857  accepted\n",
      "51 -1070.8333402309127 -24.283264605569833 0.3113123381179984 0.25815248935988677  accepted\n",
      "53 -988.8002267482959 -24.287622805695634 0.3033065958289579 0.24973847224138795  accepted\n",
      "54 -984.1610493663338 -24.27398749055471 0.30310522988344485 0.2528080020605528  accepted\n",
      "55 -972.9644985987659 -24.279141741191566 0.3022435589189839 0.24100306041580433  accepted\n",
      "57 -949.6009552329363 -24.25627308666496 0.30075826827039903 0.2478695716590394  accepted\n",
      "58 -911.5544144361281 -24.275821518886154 0.2967734701799672 0.23955568653928114  accepted\n",
      "59 -873.2852978852848 -24.295811434768158 0.29241208308009264 0.22432456202084733  accepted\n",
      "60 -858.1369066731343 -24.306528346580972 0.29008713091128774 0.22367497192247707  accepted\n",
      "62 -837.789568991913 -24.294367542705416 0.28728919706731787 0.22278572915946276  accepted\n",
      "64 -796.1745127419362 -24.258443374673046 0.2828175431533594 0.2172535597891638  accepted\n",
      "72 -712.2994558197023 -24.243805923210882 0.27234880734131106 0.19885773174094554  accepted\n",
      "76 -709.9790780157666 -24.247632379138444 0.27283895667967967 0.19424166092585776  accepted\n",
      "77 -683.4916309348162 -24.23538078460796 0.27158533230136783 0.18481983152506826  accepted\n",
      "87 -673.1228749349639 -24.23032045659272 0.27082809799822993 0.1869866168606751  accepted\n",
      "89 -667.5620652861537 -24.21649683524616 0.2690909426773523 0.18952823867594962  accepted\n",
      "90 -656.3640815077648 -24.204167405833015 0.26856743632651103 0.21191777712001553  accepted\n",
      "91 -629.3446487139656 -24.220219320852884 0.2654319461206015 0.21751379112961675  accepted\n",
      "92 -608.8164424971756 -24.22267083829694 0.2617438521925607 0.215085090773697  accepted\n",
      "93 -567.7605830260622 -24.215711206159426 0.2583882683482469 0.23397669766818113  accepted\n",
      "95 -502.2551675468415 -24.205550616614424 0.2535660439518823 0.2224132128699136  accepted\n",
      "96 -470.8956171219684 -24.197433202683996 0.25072728346214734 0.20662305226041178  accepted\n",
      "99 -423.96973255870046 -24.207726352128162 0.24512671397006802 0.2143366496203254  accepted\n",
      "101 -356.75115629858203 -24.201221788464558 0.23742168696877214 0.19159513107398138  accepted\n",
      "102 -260.4641004430848 -24.18260073020678 0.22639363623129674 0.17584317758851717  accepted\n",
      "106 -203.06195419106146 -24.179128077272473 0.2217996909184887 0.15669429235268809  accepted\n",
      "109 -181.0503356977316 -24.179353816396805 0.2218084931566953 0.17967222716910966  accepted\n",
      "112 -142.4778331934648 -24.178468326484246 0.21587265158669586 0.14741231727667295  accepted\n",
      "115 -123.26031756468149 -24.178289398468834 0.2156498483529487 0.15083274563544058  accepted\n",
      "116 -119.37965955694861 -24.185043730579295 0.21471188889642728 0.16869785596234133  accepted\n",
      "118 -70.41507403310409 -24.212849935841753 0.20906205781438383 0.1838160069327449  accepted\n",
      "121 -45.10957331233274 -24.229363538068696 0.20626688013223693 0.17631077869129294  accepted\n",
      "123 -34.06286552086071 -24.25511499274791 0.2064949340489316 0.17707974325498743  accepted\n",
      "128 -29.769965502013008 -24.230184396341713 0.20835590061336612 0.18798123279499715  accepted\n",
      "129 1.5758735345208947 -24.1942945900225 0.20702999787500845 0.19313584773031686  accepted\n",
      "132 30.18421993587313 -24.19755880110033 0.20369161090654278 0.18160353845713487  accepted\n",
      "133 61.25245425468768 -24.18859475844772 0.20125800222908508 0.1864344902162434  accepted\n",
      "134 86.57458192877775 -24.19175550381414 0.1992693648810001 0.16198588244033207  accepted\n",
      "141 91.12806365417231 -24.221691452529047 0.19848658802773048 0.1392190169885525  accepted\n",
      "148 122.13217274562845 -24.212992863135867 0.19441946850388164 0.1608402317329977  accepted\n",
      "152 181.1238142417733 -24.168427944982604 0.18979226358679357 0.15852317625787568  accepted\n",
      "154 184.00072810319296 -24.15768982703301 0.18975723941106803 0.15936553310993617  accepted\n",
      "155 206.3199877537244 -24.150172185369314 0.18504817276065955 0.14979283685213332  accepted\n",
      "157 216.1981028272511 -24.136074506938403 0.18230656302088646 0.15377910617846308  accepted\n",
      "158 231.99709139509918 -24.109014033580436 0.18225769504664838 0.149602685462198  accepted\n",
      "161 252.31730681909767 -24.108002369211253 0.17926366313198813 0.14845167688600275  accepted\n",
      "165 297.1476196948293 -24.115474483217426 0.1747946005330639 0.14098952898438247  accepted\n",
      "166 334.45469939066925 -24.151253071688608 0.16807428858128515 0.13300856775593625  accepted\n",
      "167 340.2988892190026 -24.163230921201848 0.16919368975514015 0.1344239681919429  accepted\n",
      "169 365.6386916034189 -24.179682336089176 0.1646463262018344 0.1453584148366205  accepted\n",
      "171 413.9123959424969 -24.15253435024728 0.15804918820512034 0.12468617709773654  accepted\n",
      "175 430.46320020887117 -24.143260393992705 0.15708160265161336 0.11417948589133801  accepted\n",
      "176 448.87612039320106 -24.139874539619292 0.16217537270993884 0.11384166365797438  accepted\n",
      "178 466.74114927375365 -24.120754129272292 0.16002766488751788 0.1019177634805323  accepted\n",
      "187 505.65669206604025 -24.098259238306984 0.15567432264772116 0.0947589538617451  accepted\n",
      "194 547.3409208356268 -24.083143629689996 0.1507896357287611 0.12032451746552188  accepted\n",
      "197 556.9125244217046 -24.097685950763672 0.1513024987245374 0.111660495085726  accepted\n",
      "200 583.0843449043952 -24.08554916758094 0.14703294070457168 0.10677127029118799  accepted\n",
      "203 597.490122317866 -24.077329111140358 0.1455921927147679 0.09283281804219227  accepted\n",
      "204 603.3539161049893 -24.05814169375508 0.14375098918442283 0.08143110965216614  accepted\n",
      "206 616.9165764138494 -24.021980204972092 0.1444294088184239 0.08278446044346187  accepted\n",
      "207 646.0155141533168 -24.049779980395698 0.13724251642239466 0.08519324506335729  accepted\n",
      "208 661.5919331098066 -24.03723862678334 0.13319652513606536 0.08824604936505147  accepted\n",
      "210 679.0677106071148 -24.02061977730964 0.12948580274651364 0.08964308148086793  accepted\n",
      "213 679.2019982435883 -24.015986925857483 0.12976626070167374 0.0982419306285014  accepted\n",
      "220 680.5721359002102 -24.036656457373134 0.1310714201138934 0.11896095513406588  accepted\n",
      "223 692.0132579735293 -24.040216240154823 0.13029941221449443 0.10897150274851278  accepted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225 703.2304189473792 -24.029301796789746 0.1236247207600117 0.11451147100408145  accepted\n",
      "227 738.1304116458753 -23.970605869020556 0.12157009782089798 0.12554769415200662  accepted\n",
      "232 748.648463489893 -23.967313064192115 0.11903433913603018 0.10932014695538975  accepted\n",
      "240 795.2643792482075 -23.939407625956697 0.11489317876052581 0.1169390821332097  accepted\n",
      "242 803.570857855781 -23.951014808387825 0.11400848652678452 0.12068736576153585  accepted\n",
      "251 822.3673579245306 -23.9419835422665 0.11427859941877232 0.12266064366048418  accepted\n",
      "255 843.6418512775073 -23.933982300381512 0.10923780250476448 0.1182789573545593  accepted\n",
      "257 856.2406682546011 -23.937780181871627 0.10551906631021177 0.10668891315269087  accepted\n",
      "258 901.9898941210105 -23.934692945464665 0.0958477342060205 0.09883116397022153  accepted\n",
      "259 939.7050744254129 -23.90898093889886 0.09496385136493467 0.08661669988686257  accepted\n",
      "261 962.459184591617 -23.908069086682076 0.09697926928845933 0.07838034475043366  accepted\n",
      "266 974.0053071354497 -23.89073709189127 0.09499498911772707 0.06714821556158408  accepted\n",
      "271 1007.7080939182256 -23.884226367311385 0.09017610684374296 0.08332053045821766  accepted\n",
      "273 1019.1928957313914 -23.880417039177797 0.09041550382206574 0.08438984815174064  accepted\n",
      "276 1033.1044121220511 -23.877332727458313 0.09207243202021179 0.09967452299969207  accepted\n",
      "284 1043.6284185124089 -23.8543483890841 0.09545271962472161 0.11496996407109532  accepted\n",
      "287 1108.206848514259 -23.844714212129077 0.08944241075311009 0.10405372572059804  accepted\n",
      "288 1116.705034308256 -23.862107205571366 0.08963605320328492 0.09602417944159423  accepted\n",
      "289 1140.6171233667299 -23.854092275523236 0.0867536331616938 0.10131227605110799  accepted\n",
      "290 1138.950354784254 -23.832664082943783 0.09553269243366659 0.1172032092617559  accepted\n",
      "291 1140.4840823366192 -23.829940670818523 0.09849013365061762 0.11322557264878674  accepted\n",
      "294 1168.0343391397678 -23.822998577718288 0.0930185818947248 0.10641116076516338  accepted\n",
      "295 1171.1647415846064 -23.850888692555824 0.08688427738194837 0.08609609426808046  accepted\n",
      "304 1182.0121673679398 -23.855719748198236 0.0847447757275562 0.09879551502700704  accepted\n",
      "306 1194.3116651772996 -23.87687980485428 0.08286389891549718 0.0929030374708911  accepted\n",
      "308 1202.978368287418 -23.873331268261733 0.08386840114144714 0.10148399235373053  accepted\n",
      "310 1207.0201318281343 -23.870605091744093 0.08416958218685153 0.09472758000841144  accepted\n",
      "311 1236.2264722121831 -23.893831186123474 0.08282070036607986 0.10121071331895003  accepted\n",
      "313 1281.31633403385 -23.88932045590859 0.07918588914110854 0.09010545350780347  accepted\n",
      "314 1311.8104465984218 -23.88828806443454 0.06829689388207785 0.08938944020077808  accepted\n",
      "316 1337.0249503986038 -23.88843289334362 0.06703269385304486 0.08839583780771672  accepted\n",
      "317 1357.1037699080994 -23.891028650507575 0.06837042245198806 0.08957461964621333  accepted\n",
      "320 1399.6672698054085 -23.92385471220279 0.06868748391060894 0.08526823296190489  accepted\n",
      "324 1425.0087443691868 -23.927756368741697 0.06846609601690982 0.08612608681979829  accepted\n",
      "326 1425.589014437453 -23.964265780038165 0.07255804782123693 0.102239727752222  accepted\n",
      "330 1452.266516585329 -23.967493278305252 0.07001395967951639 0.09508826868409806  accepted\n",
      "335 1463.5041478238081 -23.994473066498426 0.06934451555756749 0.08368845005755574  accepted\n",
      "339 1485.7534845311116 -23.977434125125548 0.07330842324965645 0.09352684235813895  accepted\n",
      "341 1502.791751736453 -23.98817711789713 0.06812720981707028 0.08689644548245733  accepted\n",
      "346 1505.8991252181156 -23.963985395841295 0.06946756837672556 0.08554364005061332  accepted\n",
      "347 1507.5297248208226 -23.95297087233777 0.06836553149259955 0.0859209837082166  accepted\n",
      "350 1509.1072299221023 -23.972390994964663 0.07051293677108947 0.08983082957809187  accepted\n",
      "352 1520.5537183090935 -23.938162612020513 0.07606059147903332 0.09942480736772621  accepted\n",
      "353 1534.5225722775958 -23.942217370763412 0.07537814510527274 0.08834871170129378  accepted\n",
      "357 1542.1916846910074 -23.981904385207873 0.07573489981743205 0.09065743223039141  accepted\n",
      "360 1562.8280410690147 -23.955938759665546 0.0795802565426576 0.0905985743556741  accepted\n",
      "363 1622.016520372148 -23.949414573721793 0.07507688838467504 0.08797045326410323  accepted\n",
      "366 1637.5850073089464 -23.95186299772953 0.06848000976146017 0.07171247848412209  accepted\n",
      "377 1648.1784076304612 -23.96698400374863 0.06868039699101638 0.0705401535568811  accepted\n",
      "379 1667.4160679767797 -23.967753831226748 0.07188899554168374 0.08392825509352277  accepted\n",
      "381 1679.1123310748994 -23.974310772133247 0.062421584771616634 0.07062238781047735  accepted\n",
      "382 1683.115773720385 -23.996128289412876 0.0647307801254406 0.07746659588945963  accepted\n",
      "383 1684.1909516056865 -24.002467916998413 0.06519884451518551 0.07476958137116889  accepted\n",
      "384 1716.129253977416 -23.994930592745167 0.06772098421287219 0.0715091336407522  accepted\n",
      "396 1738.4717660615445 -23.9506164922962 0.07018194286135827 0.0743222477725003  accepted\n",
      "397 1774.5525593421676 -23.96266181216976 0.06354023097827556 0.07301659061417809  accepted\n",
      "403 1789.8015246567516 -23.945521104067357 0.06688487922501328 0.08196247708187349  accepted\n",
      "406 1808.759706998872 -23.89267546428427 0.06912161281354219 0.09449318800791787  accepted\n",
      "416 1862.0091436263551 -23.91225594601533 0.05907189476131219 0.07304915995228171  accepted\n",
      "419 1864.8982193328168 -23.881593210644787 0.06924497844956617 0.07951990918399741  accepted\n",
      "423 1881.6474292849525 -23.885537234090837 0.06332316596620617 0.06583654196999779  accepted\n",
      "427 1894.9697361124677 -23.84212578846241 0.06481834997112153 0.07278147735499745  accepted\n",
      "430 1926.8299754986251 -23.8273503479301 0.06655082674118178 0.08135370352570209  accepted\n",
      "432 1950.7845674549549 -23.796955189940714 0.06741157629995612 0.07585623783510001  accepted\n",
      "449 1972.2564469188005 -23.79538504287208 0.06580978044111002 0.07663866368878361  accepted\n",
      "451 2017.7094974525753 -23.795389549329272 0.058937672401536065 0.06315209893305088  accepted\n",
      "453 2020.5606431001888 -23.785258986124692 0.060031714433652045 0.06586122719227312  accepted\n",
      "454 2025.7572994494003 -23.74998993454507 0.06504758480142298 0.06357956313948353  accepted\n",
      "455 2047.5815895686137 -23.75222336372938 0.06624128806608068 0.06118898026402659  accepted\n",
      "458 2065.0083958426453 -23.742439406567353 0.06240385407743277 0.06210830253237536  accepted\n",
      "462 2097.7855421611384 -23.730997734757754 0.05966259576772273 0.06417720079031418  accepted\n",
      "466 2107.9956583151215 -23.705241588373394 0.06044816732031169 0.06098480128191374  accepted\n",
      "468 2123.4577359696596 -23.685585380666268 0.05807202419948466 0.05391223186441967  accepted\n",
      "470 2152.549123413798 -23.65913329412036 0.058953853597794245 0.05611652883521997  accepted\n",
      "471 2159.7830904515167 -23.61656353848042 0.06296914921590538 0.05335919319717986  accepted\n",
      "481 2159.587853473975 -23.60852792298421 0.06665182734976005 0.06197282974242946  accepted\n",
      "486 2207.0834170578405 -23.599910587656815 0.05779487218111925 0.053520958236891444  accepted\n",
      "489 2239.8726243242736 -23.55238614177264 0.05947987082170232 0.053744323042091584  accepted\n",
      "490 2244.062974870923 -23.577852667598187 0.059693671405549105 0.062235145540094745  accepted\n",
      "499 2298.1581681148896 -23.59851044796988 0.050243957186392935 0.05216384992240107  accepted\n",
      "508 2320.872673394657 -23.585079887409556 0.05300861233606753 0.053451258357224765  accepted\n",
      "531 2338.2567514380407 -23.601356069043053 0.052901225333340365 0.048429476030640545  accepted\n",
      "543 2346.502474521654 -23.635726621517513 0.050742738365618836 0.05073138890472106  accepted\n",
      "549 2371.199474480798 -23.645124921492368 0.050462009722854854 0.05972669676457451  accepted\n",
      "553 2379.5928343451924 -23.626009631675018 0.05221921468844018 0.06993606331485532  accepted\n",
      "556 2391.129611433542 -23.606377876341696 0.05341128035630025 0.0713417386193047  accepted\n",
      "562 2391.6458986882703 -23.625127152464444 0.05196825085797886 0.06845061827521041  accepted\n",
      "566 2396.2798777057405 -23.592642259390907 0.05731542927160501 0.07504048070157726  accepted\n",
      "573 2403.6282457961697 -23.605307628812067 0.05419601420553056 0.07017186109434667  accepted\n",
      "575 2442.6322989139135 -23.589550121017993 0.05393811494642308 0.06677614905708908  accepted\n",
      "579 2454.7062583742527 -23.589799918833247 0.05048949657861323 0.048594622748637746  accepted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "584 2479.9563323867715 -23.594684271631614 0.04390528031733105 0.05168763276178361  accepted\n",
      "586 2515.557325414311 -23.559449156660197 0.04212776773235903 0.05297352028351965  accepted\n",
      "604 2519.1060012467833 -23.568001181495227 0.042257212614804814 0.053739994483334025  accepted\n",
      "608 2555.320704179697 -23.53577978831263 0.040005052793810954 0.05187940651356146  accepted\n",
      "616 2565.8001419351417 -23.53624494249825 0.035374608997595036 0.04925512630611083  accepted\n",
      "629 2575.727398838475 -23.566972109820114 0.03610754162204258 0.050697095367515974  accepted\n",
      "640 2593.5155033497385 -23.55979928813301 0.0372973515314413 0.05496435426536699  accepted\n",
      "651 2607.348046113092 -23.602662508117167 0.03292958989669953 0.0493703648594076  accepted\n",
      "663 2619.9151968480955 -23.589843326517705 0.03922034588566611 0.06175309325032651  accepted\n",
      "691 2641.386799230987 -23.56274983072723 0.040591788358361865 0.06169001944084971  accepted\n",
      "692 2643.3966814403398 -23.54067411271912 0.04408789418116465 0.06160814122231911  accepted\n",
      "698 2642.196931580048 -23.54452899178317 0.04366348219623202 0.06267801872403887  accepted\n",
      "701 2657.7507249918617 -23.545288606720185 0.0426048289345558 0.05902913811088021  accepted\n",
      "706 2677.745940864324 -23.551180288308725 0.04631468740900548 0.06243721063579324  accepted\n",
      "712 2715.4560788319923 -23.51210231435916 0.04326017604622637 0.058515798933105365  accepted\n",
      "717 2757.6586200169545 -23.489707769306314 0.0399564364586849 0.04713501188435218  accepted\n",
      "718 2762.6402869092103 -23.508884856200424 0.03622829728285386 0.04900644925305963  accepted\n",
      "729 2816.7921034650744 -23.504411213231993 0.03306572791979188 0.047828232652775624  accepted\n",
      "730 2819.9138823012277 -23.505530900606455 0.039406865027167115 0.052332235590963457  accepted\n",
      "732 2829.740423933471 -23.510268081564643 0.03613803474832723 0.05411853172327795  accepted\n",
      "735 2858.6584778283623 -23.51675758303903 0.03921319938121793 0.05992171760949158  accepted\n",
      "737 2886.332078649726 -23.519205594343813 0.03235146171907217 0.05221201226890764  accepted\n",
      "749 2913.4198341097745 -23.498096713639335 0.032863138910248134 0.049921542411285234  accepted\n",
      "762 2913.700141440174 -23.474030649784105 0.03774428600696563 0.05048023969805143  accepted\n",
      "776 2921.711204669588 -23.456853824411567 0.03909556244860542 0.054222053348105474  accepted\n",
      "795 2924.6695536377724 -23.489290758330903 0.0403613219579444 0.05980790608501895  accepted\n",
      "796 2929.2503593368688 -23.489727158884747 0.03977859096684967 0.05540888222940667  accepted\n",
      "800 2956.144356011733 -23.45564453044588 0.041920630146907646 0.04610770483578998  accepted\n",
      "802 2964.0425751622447 -23.46682732839389 0.037460768100709065 0.05572700818005286  accepted\n",
      "809 2974.080782072318 -23.431344202327395 0.037602421590178736 0.059949712385041334  accepted\n",
      "810 2982.0710237301864 -23.457629491902985 0.04125848867629239 0.05191993247966299  accepted\n",
      "813 2999.5956884310635 -23.430672419870934 0.04174903614156377 0.05107654978726807  accepted\n",
      "819 3018.009721876815 -23.43874797010701 0.041378343834979875 0.05984161311277305  accepted\n",
      "824 3042.339350871887 -23.45221880146206 0.04287086113177797 0.06601768890649592  accepted\n",
      "834 3047.506623065902 -23.455828902772858 0.039337937885123964 0.06256181699008424  accepted\n",
      "839 3064.3895429284494 -23.449619680239614 0.0353341184519958 0.058285140942562756  accepted\n",
      "849 3108.9493749393437 -23.464107123023858 0.03422601173204421 0.05072852412741361  accepted\n",
      "867 3111.4807939211364 -23.43844203118686 0.03759694227159436 0.05092226319215196  accepted\n",
      "877 3122.4968801222376 -23.41659313480681 0.03504427217357877 0.054684339245958176  accepted\n",
      "880 3134.604424837575 -23.413689138512545 0.032240105076066464 0.048968056829234434  accepted\n",
      "892 3163.9809089925047 -23.361214331077207 0.03200541461073931 0.04942286866133931  accepted\n",
      "893 3164.8745558299547 -23.337306500960295 0.03154824085077372 0.05082197360240492  accepted\n",
      "895 3165.190334024349 -23.366330321687716 0.031861401690973165 0.053664971623728484  accepted\n",
      "898 3167.864126900116 -23.363500950025788 0.03282873663769841 0.052194188949732455  accepted\n",
      "903 3172.099280272412 -23.322522200135026 0.03536725970080175 0.05327540181479772  accepted\n",
      "906 3185.912890224297 -23.29578841268448 0.03929202341452849 0.050690648821864984  accepted\n",
      "908 3200.689664360186 -23.292355881096483 0.03787551787958829 0.055875097141942436  accepted\n",
      "918 3230.805307409799 -23.28567002645469 0.037688392507433964 0.06005367644761549  accepted\n",
      "930 3274.6814745041574 -23.306610921724264 0.03396551474304155 0.05435237192863411  accepted\n",
      "939 3276.8255578781227 -23.248485333140444 0.04128163211146447 0.050835102452321396  accepted\n",
      "940 3281.1974143659245 -23.2624811795812 0.03825646912177233 0.04870744086201892  accepted\n",
      "947 3297.136858034938 -23.205401840184745 0.044156527384235415 0.05196446452479038  accepted\n",
      "949 3307.575974393352 -23.237944887921625 0.0401441172292566 0.04979508807030633  accepted\n",
      "951 3312.953185158016 -23.201068567225196 0.04134178255911844 0.05881761313197897  accepted\n",
      "958 3321.557165250353 -23.210960078815518 0.03893890432353918 0.05630428392464555  accepted\n",
      "968 3323.5222064643995 -23.17297891364171 0.044254861124206214 0.0633404355094301  accepted\n",
      "972 3339.7577864088616 -23.17931361146545 0.041155046134583075 0.06180162508193058  accepted\n",
      "980 3350.2721215823535 -23.20181668559506 0.04130028316700385 0.06205284935995834  accepted\n",
      "991 3378.4102452257603 -23.207436492272002 0.04076308215763226 0.061996820329801895  accepted\n",
      "995 3404.212270516693 -23.17384828232602 0.04199222855318114 0.05894530165787088  accepted\n",
      "996 3456.796148901326 -23.15550194796756 0.03673405611496757 0.054057174576792745  accepted\n",
      "1002 3466.056757787433 -23.154548143886817 0.03883231168861621 0.054280837727646046  accepted\n",
      "1007 3477.584431907057 -23.15598171213604 0.03578500147239824 0.05235228548209692  accepted\n",
      "1010 3482.0468854152687 -23.133007184521638 0.038366417439885385 0.05421079125851832  accepted\n",
      "1017 3483.7761399975825 -23.1396551240926 0.032968098243457575 0.045320263597702984  accepted\n",
      "1019 3488.3183476901377 -23.14910521583872 0.036003231064277476 0.04981980001564014  accepted\n",
      "1052 3524.7614327722144 -23.150304864054142 0.03229728564660387 0.047740337038399264  accepted\n",
      "1088 3538.5089493963155 -23.16235572755345 0.03215345638570432 0.04565725797585171  accepted\n",
      "1105 3537.734819325632 -23.142600095496494 0.03635037686130318 0.05282054474735297  accepted\n",
      "1106 3542.5267961580294 -23.16461636233815 0.03368358452763622 0.0518751901546415  accepted\n",
      "1117 3564.1118346495664 -23.18123310719675 0.031647323459955384 0.049547673417013736  accepted\n",
      "1118 3593.9067965044887 -23.157217268387075 0.03420137379547218 0.05400159598967408  accepted\n",
      "1121 3615.40980178733 -23.198421674172447 0.03173415636786196 0.05115918996597004  accepted\n",
      "1148 3633.807150875745 -23.16112976411445 0.029181511280850843 0.04994648155774457  accepted\n",
      "1149 3645.9705152619736 -23.143141340034617 0.03021718576006369 0.051473325326672155  accepted\n",
      "1152 3665.515968151018 -23.140876672895374 0.02754740516566738 0.048749603596742956  accepted\n",
      "1165 3679.760464781647 -23.129319567810224 0.03115221863674126 0.04665426978395774  accepted\n",
      "1173 3685.060566380934 -23.093675562235223 0.03038468692223377 0.04767703605527963  accepted\n",
      "1175 3694.3230284272704 -23.10909363247751 0.029507904197381057 0.04900595581336765  accepted\n",
      "1184 3695.1931319474215 -23.080550874408736 0.03404651872239013 0.04891678734678564  accepted\n",
      "1186 3704.1622599158563 -23.089664871997563 0.03362301313764461 0.05308589517501291  accepted\n",
      "1189 3726.6794256479566 -23.090295830898945 0.031499326411864315 0.049722157617756495  accepted\n",
      "1194 3727.8132359968804 -23.074522478371293 0.03964642124357455 0.0561426689900381  accepted\n",
      "1212 3763.4849487001784 -23.03242499411394 0.04084444061565117 0.05867304167416764  accepted\n",
      "1217 3780.7358088480687 -23.08210926320861 0.03447009349873839 0.0498263745950639  accepted\n",
      "1219 3786.7944645906364 -23.125004496977823 0.03969752091766501 0.055336870221194386  accepted\n",
      "1255 3800.106868673172 -23.097483422765922 0.03929251165363287 0.05032047873359876  accepted\n",
      "1275 3799.38494400284 -23.076637253345606 0.0379695108553437 0.052928531311188125  accepted\n",
      "1282 3820.8571984012697 -23.068325075846936 0.03678125432009214 0.05631316488061398  accepted\n",
      "1290 3836.118539946177 -23.01590155258079 0.03936795426860341 0.04765555677760829  accepted\n",
      "1291 3846.4676863667846 -22.973144106514507 0.037203404734043544 0.05073678010889517  accepted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1305 3854.99769321192 -22.984263041305688 0.038619060868909934 0.05637644445523082  accepted\n",
      "1310 3859.4655301921557 -23.006862880530967 0.03838450422116161 0.05263498008854491  accepted\n",
      "1339 3875.130228841692 -23.016912789684948 0.03871048576184786 0.055481009582146905  accepted\n",
      "1373 3878.1069012719877 -22.990753151677424 0.03967873658703745 0.05700482288795451  accepted\n",
      "1384 3892.429515952526 -22.996424535968288 0.03739499442132231 0.052478552784774105  accepted\n",
      "1396 3918.133375479514 -22.97945527303722 0.0352451448638767 0.05522892027878755  accepted\n",
      "1399 3923.9131602937796 -22.967449906937777 0.037322078050085104 0.057319890007435745  accepted\n",
      "1403 3950.060459645799 -22.96519204364226 0.03447362829530647 0.055290768034863304  accepted\n",
      "1427 3954.4587759494116 -22.97557740486744 0.03810316877156344 0.05400550840238203  accepted\n",
      "1434 3980.808701470977 -22.952648884129403 0.03758414932624473 0.059463008607172914  accepted\n",
      "1450 3994.201920880667 -22.91642786944951 0.03691941882305105 0.06085999071965486  accepted\n",
      "1454 4002.944676586055 -22.90886198778377 0.032044285944017474 0.05439637543244561  accepted\n",
      "1470 4004.611739158026 -22.875528169561978 0.034821842556326 0.0559294624500549  accepted\n",
      "1479 4020.9204974538625 -22.87649663660447 0.032487712210832874 0.04941346165896187  accepted\n",
      "1520 4036.0075723290065 -22.87683493779306 0.03534989259690089 0.05370123023871969  accepted\n",
      "1530 4087.9583713281936 -22.847364603684266 0.03560812886963692 0.056578799867958454  accepted\n",
      "1547 4088.407326180734 -22.826855974258798 0.03781984412163493 0.053524558892917386  accepted\n",
      "1578 4100.766080606918 -22.88725576698821 0.035947548543819506 0.05403133661594617  accepted\n",
      "1590 4113.916493400493 -22.88957721418637 0.036941408712815316 0.057866340988258276  accepted\n",
      "1626 4115.723644358344 -22.864301742677455 0.03816791196104594 0.05492189122222092  accepted\n",
      "1633 4138.843690414905 -22.822565311694568 0.03641340120639618 0.055995286152626875  accepted\n",
      "1636 4162.02702299507 -22.800610911534058 0.035611827579193955 0.057143003894943735  accepted\n",
      "1642 4168.599845259713 -22.81772802645772 0.03191060243325594 0.053912793153203666  accepted\n",
      "1655 4179.654994742083 -22.82236535303322 0.029267382250315353 0.04779527392946162  accepted\n",
      "1660 4194.900509242477 -22.806444273953385 0.029955031736472394 0.04583575717198769  accepted\n",
      "1786 4196.282170325412 -22.791151849048077 0.0365210006890761 0.05448558715800772  accepted\n",
      "1792 4204.367442277443 -22.768415068053795 0.03605515545309745 0.05312080025856786  accepted\n",
      "1811 4208.633715028211 -22.762825679317263 0.03853842342524532 0.05664626206214581  accepted\n",
      "1816 4212.754940961432 -22.741545343214852 0.03898513852159314 0.055561633265338924  accepted\n",
      "1818 4237.263267016989 -22.744229412382822 0.03416423719780186 0.04810693718545483  accepted\n",
      "1820 4279.262153113838 -22.78035122557564 0.02753848234129206 0.048050767322112074  accepted\n",
      "1821 4309.277823871145 -22.720641068687243 0.03005498642722725 0.049812697361804  accepted\n",
      "1835 4318.327871221296 -22.733458529530402 0.03338763720387138 0.04586079750316162  accepted\n",
      "1838 4324.675492026148 -22.703527223899744 0.03693864166393688 0.05343791615776196  accepted\n",
      "1843 4325.841218715664 -22.707529394284624 0.03689853987538412 0.05764796039904812  accepted\n",
      "1859 4345.578490546646 -22.717531868926045 0.03554327182402617 0.053078058018254395  accepted\n",
      "1910 4346.599956913138 -22.73762352141403 0.03575600075675707 0.049502197467832555  accepted\n",
      "1914 4379.549435342705 -22.700736816618416 0.03468051871551272 0.04699168839682167  accepted\n",
      "1967 4379.4691666078925 -22.720205469726594 0.0348303122834937 0.056502471304715654  accepted\n",
      "1972 4404.7036412024445 -22.69401762180659 0.03148747986636016 0.05087697710846664  accepted\n",
      "1974 4410.033259407505 -22.694153632330877 0.0344781123817384 0.05308618834417823  accepted\n",
      "1982 4421.184030307268 -22.72463913455421 0.03462036004142562 0.055451244885062145  accepted\n",
      "1983 4421.574985386815 -22.706533476532492 0.03334549674019647 0.053235894753319545  accepted\n",
      "2006 4423.907524036222 -22.68996052386377 0.030634562974015886 0.04945022897788136  accepted\n",
      "2007 4442.923408502322 -22.678712350819175 0.0317957534039894 0.05140952603233927  accepted\n",
      "2013 4460.387284046473 -22.675578176859915 0.028590599603174737 0.04820934453096984  accepted\n",
      "2029 4469.807095165145 -22.6839417788502 0.02975795709714476 0.04690722325137982  accepted\n",
      "2035 4481.008355011574 -22.66525736117954 0.029433084105144725 0.045582007328219196  accepted\n",
      "2044 4512.165854857656 -22.648107768874418 0.03079856742317449 0.04664963490069393  accepted\n",
      "2053 4530.534801001388 -22.610818488519566 0.03489219368823043 0.05166946636419309  accepted\n",
      "2056 4563.90527350767 -22.628341404077048 0.03362265205929047 0.05373453860839907  accepted\n",
      "2092 4589.263276329319 -22.616262452315432 0.031664936467405695 0.04883397722218025  accepted\n",
      "2106 4598.878886590064 -22.614847569178465 0.032451443814168125 0.05288215275803457  accepted\n",
      "2122 4613.215712354797 -22.611964267897164 0.03270792369671115 0.0483610685542409  accepted\n",
      "2134 4631.777803289988 -22.59118941926156 0.033658912349303106 0.04526329135481408  accepted\n",
      "2143 4666.192651862984 -22.591144251437655 0.029762764871556813 0.04688227377502608  accepted\n",
      "2184 4677.58307881125 -22.562270179978395 0.03221903976500798 0.053045479639364464  accepted\n",
      "2210 4692.4842450536835 -22.574529614077285 0.03132363129204868 0.04767945913784877  accepted\n",
      "2229 4734.854325297177 -22.523978289794368 0.02995220231715768 0.04774760332429054  accepted\n",
      "2239 4739.100401133816 -22.538984035702466 0.03237416532741633 0.05161828751402981  accepted\n",
      "2252 4755.105554399071 -22.52667846849384 0.032029449841654026 0.048970583148778585  accepted\n",
      "2260 4755.300353275579 -22.521907185771166 0.031989810086062796 0.04634022911565416  accepted\n",
      "2271 4758.612919660706 -22.512437186469885 0.031123759264724264 0.04969694872969498  accepted\n",
      "2274 4781.224124459527 -22.486475632336735 0.02913484582062921 0.04602794722730933  accepted\n",
      "2277 4791.137377493414 -22.466229700836685 0.028770878385317705 0.04707032796155577  accepted\n",
      "2302 4792.426481991848 -22.479148045312236 0.029500197471665393 0.045812871614356133  accepted\n",
      "2383 4804.40526418487 -22.48165111371867 0.030791862046675035 0.047803586340500956  accepted\n",
      "2393 4804.838405206969 -22.485817416098733 0.029071257412684688 0.04649741510375704  accepted\n",
      "2400 4813.813797272565 -22.52094211539792 0.030527665463613744 0.047882363195591075  accepted\n",
      "2413 4834.340929946091 -22.55393896136833 0.027903457637723787 0.0468807593518538  accepted\n",
      "2447 4861.8070199600825 -22.510883932261805 0.029227469015501523 0.048164215957367564  accepted\n",
      "2474 4866.532119321115 -22.526974044472045 0.030063089306319023 0.048585100173628104  accepted\n",
      "2498 4867.399141395625 -22.49378803546687 0.0326694043715597 0.050204115968764694  accepted\n",
      "2501 4877.756304238712 -22.490266141593516 0.0327242023824796 0.05033353918857728  accepted\n",
      "2513 4882.824319487673 -22.481811932445222 0.03184669362479099 0.048428217610923315  accepted\n",
      "2530 4896.731181387109 -22.495559819751918 0.0327937400032404 0.048601464771401516  accepted\n",
      "2534 4941.2292743010885 -22.484459049065183 0.027960826202103734 0.045013152256967705  accepted\n",
      "2544 4945.074873262423 -22.494600893578284 0.02897942331089252 0.046846358101180006  accepted\n",
      "2545 4957.585902162853 -22.50206374286383 0.026301903466717836 0.04410660889695573  accepted\n",
      "2568 4957.957755335069 -22.47071701274521 0.03217700158309751 0.048569299721234686  accepted\n",
      "2575 4972.3224555671295 -22.50898699663912 0.03063128882208843 0.046079064539769676  accepted\n",
      "2602 5001.822713604784 -22.50717605909604 0.028074802748752202 0.04569985577488648  accepted\n",
      "2621 5038.273619230848 -22.44729951179544 0.028864793724224787 0.047591364957383156  accepted\n",
      "2667 5074.5588184436 -22.44084463947222 0.028387104685029217 0.04646158936562976  accepted\n",
      "2719 5078.413399687974 -22.412472767029342 0.032888295741433676 0.04679025457184396  accepted\n",
      "2726 5080.046563369812 -22.438108894568 0.032990898778161484 0.049595567265107984  accepted\n",
      "2729 5141.855106118649 -22.407470455387248 0.02929677717120302 0.046594333186744746  accepted\n",
      "2730 5174.033348144289 -22.395186686483914 0.02826052775831657 0.049406334843484535  accepted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2826 5176.917887803002 -22.384823638698293 0.027919387469500706 0.04596568297776676  accepted\n",
      "2878 5192.6452561225915 -22.38909516589142 0.027492980887863238 0.04847999985533537  accepted\n",
      "3006 5205.943516849878 -22.34016742620654 0.028577135158069287 0.0469953401851691  accepted\n",
      "3030 5211.675992540131 -22.304776839004603 0.02735215694101639 0.04507340087045947  accepted\n",
      "3047 5216.196895517028 -22.293142462135677 0.02920277357298439 0.04698083909867471  accepted\n",
      "3049 5248.2499594209585 -22.26557561957023 0.029750284757534225 0.046031758010794635  accepted\n",
      "3050 5269.702184270237 -22.211834725924366 0.029447701262947566 0.04768124206139051  accepted\n",
      "3053 5331.007848105908 -22.243329649263472 0.026119441259764547 0.043975882610545254  accepted\n",
      "3230 5329.170496260678 -22.2345337119706 0.026998211522503642 0.046777728644893934  accepted\n",
      "3256 5343.184990160526 -22.206836482921133 0.02701580333659274 0.046428394684715726  accepted\n",
      "3263 5355.255602091729 -22.179955891251094 0.02732521879291697 0.042440420861827304  accepted\n",
      "3418 5385.8495224755925 -22.190070306993377 0.028801520952992914 0.043180561676033735  accepted\n",
      "3505 5391.017001743736 -22.182196383493903 0.03043886165372087 0.04350943116950853  accepted\n",
      "3512 5403.21059469363 -22.18131097564819 0.02825758007750071 0.04484077422468731  accepted\n",
      "3605 5413.0209910280055 -22.18838442768102 0.028751295614339704 0.0471955412421884  accepted\n",
      "3607 5417.445394327201 -22.17760450377447 0.030165970717387355 0.04564940342292291  accepted\n",
      "3614 5420.431978920672 -22.181396065793223 0.031082468924579186 0.04588591937486894  accepted\n",
      "3618 5424.046705043872 -22.12906256609413 0.0320208369723385 0.048873558863200654  accepted\n",
      "3622 5429.716196015813 -22.126491814735857 0.03254635413019896 0.050113245120066534  accepted\n",
      "3628 5436.478432592987 -22.108444191817433 0.03304092281791979 0.051037096189719  accepted\n",
      "3629 5501.325522661237 -22.117934284370943 0.03049839945867314 0.047724685424321824  accepted\n",
      "3643 5508.642603642274 -22.122485276573 0.031700936307083434 0.05202270516124525  accepted\n",
      "3656 5515.268344695464 -22.11249538482518 0.032746934718974205 0.0521315519142912  accepted\n",
      "3666 5565.446865485239 -22.09087000998733 0.029749536144638232 0.04947902874654215  accepted\n",
      "3694 5577.182275967596 -22.07224722337267 0.0315301750229593 0.05276068519007301  accepted\n",
      "3707 5662.660112805532 -22.062998698944753 0.024880023174820916 0.04366425275084193  accepted\n",
      "3813 5674.414576323766 -22.04306360238421 0.02637057472485474 0.04515841811299245  accepted\n",
      "3821 5710.949300414462 -22.05675605256612 0.025034440130184257 0.044182671657684816  accepted\n",
      "3824 5721.617667976812 -22.050433825141177 0.025473699988504555 0.04398728192705835  accepted\n",
      "3995 5724.904508845064 -22.023242068837998 0.02514098510668196 0.043719145554107275  accepted\n",
      "4039 5747.706796016748 -21.984941069361795 0.025512731028573488 0.042807626591469417  accepted\n",
      "4071 5767.067938407117 -21.975918326388914 0.0252068464513678 0.044548223815839005  accepted\n",
      "4189 5767.194761975654 -21.95647214527159 0.025687378960377608 0.043435429553715384  accepted\n",
      "4235 5766.769069133659 -21.975018386979457 0.026682466744230714 0.04443525305620692  accepted\n",
      "4238 5767.531068682549 -21.949220370442966 0.028214555153437168 0.045535633745662905  accepted\n",
      "4298 5767.974115342367 -21.929817777892882 0.027706854366655008 0.046196709122892206  accepted\n",
      "4334 5782.272510511521 -21.916771797231704 0.02707546083083043 0.04616605707777181  accepted\n",
      "4349 5836.779771897234 -21.89432971343311 0.027205805816576112 0.044407401026545265  accepted\n",
      "4365 5844.927781159388 -21.86827112252498 0.0266101180646945 0.04453095378112428  accepted\n",
      "4376 5856.139716545586 -21.87236824892617 0.026027077763971042 0.04393343209192035  accepted\n",
      "4580 5857.630278457709 -21.84951098092632 0.026155294654851002 0.044176312970090115  accepted\n",
      "4800 5864.775157854392 -21.829503518475825 0.025783166814325433 0.04386605029379324  accepted\n",
      "8.06 % was accepted\n",
      "0.02905627963939325 0.0037586952646761524 0.046889097554954864 0.0038996649294053276  rmse_tr, rmsetr_std, rmse_tes, rmsetest_std\n",
      "0 612.1025323551792 0.12609452035090338 0.011437966195776077 0.012105008711735319 0.004238036600508078  posterior test \n",
      "1 273.9474796079697 0.1453884979079629 0.009222426502476308 0.012105008711735319 0.004238036600508078  posterior test \n",
      "2 532.2878259532578 0.1361394911763025 0.013047649656415203 0.012105008711735319 0.004238036600508078  posterior test \n",
      "3 889.1724803264933 0.09260466368906052 0.014460259041621532 0.012105008711735319 0.004238036600508078  posterior test \n",
      "4 1002.3603620986161 0.07558948567481631 0.01370398099470117 0.012105008711735319 0.004238036600508078  posterior test \n",
      "5 1090.6849097021595 0.07965289685767689 0.005618764854440997 0.012105008711735319 0.004238036600508078  posterior test \n",
      "6 491.47082363834363 0.1246937565314688 0.007614866558860191 0.012105008711735319 0.004238036600508078  posterior test \n",
      "7 388.67363781877816 0.13088116066362238 0.007608416088956203 0.012105008711735319 0.004238036600508078  posterior test \n",
      "8 716.2752546078221 0.10615723990684083 0.0064137026092382175 0.012105008711735319 0.004238036600508078  posterior test \n",
      "9 959.0478428305878 0.0909215985768856 0.009578543532881411 0.012105008711735319 0.004238036600508078  posterior test \n",
      "0.11081233113355402 0.0237199557293947  is mean and std of accuracy rmse test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    }
   ],
   "source": [
    "# by R. Chandra\n",
    "# https://github.com/rohitash-chandra/Bayesianlogisticreg_multioutputs\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "from math import exp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class lin_model:\n",
    "\n",
    "    def __init__(self, num_epocs, train_data, test_data, num_features, learn_rate, activation):\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data \n",
    "        self.num_features = num_features\n",
    "        self.num_outputs = self.train_data.shape[1] - num_features \n",
    "        self.num_train = self.train_data.shape[0]\n",
    "\n",
    "        #self.w = np.random.uniform(-0.5, 0.5, num_features)  # in case one output class\n",
    "        self.w = np.random.uniform(-.5, .5, (num_features, self.num_outputs))  \n",
    "        self.b = np.random.uniform(-.5, .5, self.num_outputs) \n",
    "        self.learn_rate = learn_rate\n",
    "        self.max_epoch = num_epocs\n",
    "        self.use_sigmoid = activation    # SIGMOID: 1 -sigmoid, 2 -step, 3 -linear \n",
    "        self.out_delta = np.zeros(self.num_outputs)\n",
    "\n",
    "        print(self.w, ' self.w init') \n",
    "        print(self.b, ' self.b init')        \n",
    "\n",
    "    def activation_func(self,z_vec):\n",
    "        if self.use_sigmoid == True:\n",
    "            y = 1 / (1 + np.exp(z_vec))   # sigmoid/logistic \n",
    "        else:  \n",
    "            y = z_vec\n",
    "        return y\n",
    "    \n",
    "\n",
    "    def squared_error(self, prediction, actual):\n",
    "        return  np.sum(np.square(prediction - actual))/prediction.shape[0]   # to cater more in one output/class\n",
    "    \n",
    "\n",
    "    def predict(self, x_vec ):    # implementation using dot product\n",
    "        z_vec = x_vec.dot(self.w) - self.b \n",
    "        output = self.activation_func(z_vec)    # Output \n",
    "        return output\n",
    "    \n",
    "\n",
    "    def gradient(self, x_vec, output, actual):   \n",
    "        if self.use_sigmoid == True :\n",
    "            out_delta = (output - actual)  *(output * (1 - output)) \n",
    "        else:    # for linear activation\n",
    "            out_delta = output - actual\n",
    "        return out_delta  \n",
    "    \n",
    "\n",
    "    def update(self, x_vec, output, actual):    # implementation using for loops \n",
    "        for x in range(0, self.num_features):\n",
    "            for y in range(0, self.num_outputs):\n",
    "                self.w[x,y] += self.learn_rate * self.out_delta[y] * x_vec[x] \n",
    "        for y in range(0, self.num_outputs):\n",
    "            self.b += -1 * self.learn_rate * self.out_delta[y]\n",
    "            \n",
    "\n",
    "    def test_model(self, data, tolerance):  \n",
    "\n",
    "        num_instances = data.shape[0]\n",
    "\n",
    "        class_perf = 0\n",
    "        sum_sqer = 0   \n",
    "        for s in range(0, num_instances):\n",
    "            input_instance = self.train_data[s,0:self.num_features] \n",
    "            actual = self.train_data[s,self.num_features:]  \n",
    "            prediction = self.predict(input_instance) \n",
    "            pred_binary = np.zeros(prediction.shape[0])\n",
    "            sum_sqer += self.squared_error(prediction, actual)\n",
    "            index = np.argmax(prediction)\n",
    "            pred_binary[index] = 1    # i= for softmax  \n",
    "            #pred_binary = np.where(prediction > (1 - tolerance), 1, 0) # for sigmoid in case of classification\n",
    "\n",
    "            if (actual==pred_binary).all():\n",
    "                class_perf += 1   \n",
    "\n",
    "        rmse = np.sqrt(sum_sqer/num_instances)\n",
    "        percentage_correct = float(class_perf/num_instances) * 100 \n",
    "        print(percentage_correct, rmse,  ' class_perf, rmse')  \n",
    "\n",
    "        return (rmse, percentage_correct)\n",
    "\n",
    "\n",
    "    def SGD(self):   \n",
    "        epoch = 0 \n",
    "        shuffle = True\n",
    "\n",
    "        while epoch < self.max_epoch:\n",
    "            sum_sqer = 0\n",
    "            for s in range(0, self.num_train):\n",
    "                if shuffle==True:\n",
    "                    i = random.randint(0, self.num_train-1) \n",
    "                    input_instance = self.train_data[i, 0:self.num_features]  \n",
    "                    actual = self.train_data[i, self.num_features:]  \n",
    "                    prediction = self.predict(input_instance) \n",
    "                    sum_sqer += self.squared_error(prediction, actual)\n",
    "                    self.out_delta = self.gradient(input_instance, prediction, actual)\n",
    "                    # major difference when compared to GD \n",
    "                    self.update(input_instance, prediction, actual) \n",
    "\n",
    "            epoch += 1  \n",
    "\n",
    "        rmse_train, train_perc = self.test_model(self.train_data, 0.3) \n",
    "        rmse_test = 0\n",
    "        test_perc = 0\n",
    "        rmse_test, test_perc = self.test_model(self.test_data, 0.3)\n",
    "\n",
    "        return (train_perc, test_perc, rmse_train, rmse_test) \n",
    "\n",
    "    # new functions added for MCMC below\n",
    "\n",
    "    def encode(self, w):    # get the parameters and encode into the model\n",
    "        w_size = self.num_features * self.num_outputs\n",
    "        w_temp = w[0:w_size]\n",
    "        self.w = np.reshape(w_temp, (self.num_features, self.num_outputs))\n",
    "        self.b = w[w_size:w.shape[0]]\n",
    "        \n",
    "\n",
    "    def evaluate_proposal(self, data, w):   # BP with SGD (Stocastic BP)\n",
    "\n",
    "        self.encode(w)    # method to encode w and b\n",
    "        fx = np.zeros((data.shape[0], self.num_outputs))\n",
    "\n",
    "        for s in range(0, data.shape[0]):\n",
    "            i = s\n",
    "            #random.randint(0, data.shape[0]-1)  (we dont shuffle in this implementation)\n",
    "            input_instance = data[i,0:self.num_features]  \n",
    "            actual = data[i,self.num_features:]  \n",
    "            prediction = self.predict(input_instance)  \n",
    "            fx[s, :] = prediction \n",
    "\n",
    "        return fx\n",
    "\n",
    "#------------------------------------------------------------------\n",
    "\n",
    "class MCMC:\n",
    "    def __init__(self, samples, traindata, testdata, topology, regression):\n",
    "        self.samples = samples    # NN topology [input, hidden, output]\n",
    "        self.topology = topology  # max epocs\n",
    "        self.traindata = traindata\n",
    "        self.testdata = testdata\n",
    "        random.seed() \n",
    "        self.regression = regression # False means classification\n",
    "\n",
    "\n",
    "    def rmse(self, predictions, targets):\n",
    "        return np.sqrt(((predictions - targets) ** 2).mean())\n",
    "    \n",
    "\n",
    "    def likelihood_func(self, model, data, w, tausq):\n",
    "        y = data[:, self.topology[0]:]\n",
    "        fx = model.evaluate_proposal(data, w) \n",
    "        accuracy = self.rmse(fx, y)     # RMSE \n",
    "        loss = np.sum(-0.5 * np.log(2 * math.pi * tausq) - 0.5 * np.square(y-fx)/tausq)\n",
    "        return [loss, fx, accuracy]\n",
    "    \n",
    "\n",
    "    def prior_likelihood(self, sigma_squared, nu_1, nu_2, w, tausq): \n",
    "        param = (self.topology[0]  * self.topology[1]) + self.topology[1]   # number of parameters in model\n",
    "        part1 = -1 * (param / 2) * np.log(sigma_squared)\n",
    "        part2 = 1 / (2 * sigma_squared) * (sum(np.square(w)))\n",
    "        log_loss = part1 - part2 - (1 + nu_1) * np.log(tausq) - (nu_2 / tausq)\n",
    "        return log_loss\n",
    "    \n",
    "\n",
    "    def sampler(self):\n",
    "        # Initialize MCMC\n",
    "        testsize = self.testdata.shape[0]\n",
    "        trainsize = self.traindata.shape[0]\n",
    "        samples = self.samples\n",
    "\n",
    "        x_test = np.linspace(0, 1, num=testsize)\n",
    "        x_train = np.linspace(0, 1, num=trainsize)\n",
    "\n",
    "        #self.topology  # [input, output]\n",
    "        y_test = self.testdata[:, self.topology[0]:]\n",
    "        y_train = self.traindata[:, self.topology[0]:]\n",
    "\n",
    "        w_size = (self.topology[0] * self.topology[1]) + self.topology[1]  # num of weights and bias  \n",
    "\n",
    "        pos_w = np.ones((samples, w_size))  # posterior of all weights and bias over all samples\n",
    "        pos_tau = np.ones((samples, 1))\n",
    "\n",
    "        fxtrain_samples = np.ones((samples, trainsize, self.topology[1]))  # fx of train data over all samples\n",
    "        fxtest_samples = np.ones((samples, testsize, self.topology[1]))    # fx of test data over all samples\n",
    "        rmse_train = np.zeros(samples)\n",
    "        rmse_test = np.zeros(samples)\n",
    "\n",
    "        w = np.random.randn(w_size)\n",
    "\n",
    "        w_proposal = np.random.randn(w_size)\n",
    "\n",
    "        step_w = 0.02   # defines how much variation you need in changes to w\n",
    "        step_eta = 0.01  \n",
    "        # eta is an additional parameter to cater for noise in predictions (Gaussian likelihood). \n",
    "        # note eta is used as tau in the sampler to consider log scale. \n",
    "        # eta is not used in multinomial likelihood. \n",
    "\n",
    "        model = lin_model(0, self.traindata, self.testdata, self.topology[0], 0.1, self.regression) \n",
    "\n",
    "        pred_train = model.evaluate_proposal(self.traindata, w) \n",
    "        pred_test = model.evaluate_proposal(self.testdata, w)\n",
    "\n",
    "        eta = np.log(np.var(pred_train - y_train))    # this is to estimate var of eta that represents noise\n",
    "        tau_pro = np.exp(eta)\n",
    "\n",
    "        print(tau_pro, ' tau_pro')\n",
    "\n",
    "        sigma_squared = 5\n",
    "        # considered by looking at distribution of similar trained models - i.e. distribution of weights and bias\n",
    "        nu_1 = 0\n",
    "        nu_2 = 0\n",
    "\n",
    "        prior_likelihood = self.prior_likelihood(sigma_squared, nu_1, nu_2, w, tau_pro)  # takes care of the gradients\n",
    "\n",
    "        [likelihood, pred_train, rmsetrain] = self.likelihood_func(model, self.traindata, w, tau_pro)\n",
    "        print(likelihood, ' initial likelihood')\n",
    "        \n",
    "        [likelihood_ignore, pred_test, rmsetest] = self.likelihood_func(model, self.testdata, w, tau_pro)\n",
    "\n",
    "        naccept = 0  \n",
    "\n",
    "        for i in range(samples - 1):\n",
    "            \n",
    "            w_proposal = w + np.random.normal(0, step_w, w_size)\n",
    "\n",
    "            eta_pro = eta + np.random.normal(0, step_eta, 1)\n",
    "            tau_pro = math.exp(eta_pro)\n",
    "\n",
    "            [likelihood_proposal, pred_train, rmsetrain] = self.likelihood_func(model, self.traindata, w_proposal, tau_pro)\n",
    "            [likelihood_ignore, pred_test, rmsetest] = self.likelihood_func(model, self.testdata, w_proposal, tau_pro)\n",
    "\n",
    "            # likelihood_ignore  refers to parameter that will not be used in the alg.\n",
    "\n",
    "            prior_prop = self.prior_likelihood(sigma_squared, nu_1, nu_2, w_proposal, tau_pro)\n",
    "            # takes care of the gradients\n",
    "\n",
    "            diff_likelihood = likelihood_proposal - likelihood\n",
    "            # since we using log scale: based on https://www.rapidtables.com/math/algebra/Logarithm.html\n",
    "            diff_priorliklihood = prior_prop - prior_likelihood\n",
    "\n",
    "            mh_prob = min(1, math.exp(diff_likelihood + diff_priorliklihood))\n",
    "\n",
    "            u = random.uniform(0, 1)\n",
    "\n",
    "            if u < mh_prob:\n",
    "                # Update position\n",
    "                #print(i, ' is accepted sample')\n",
    "                naccept += 1\n",
    "                likelihood = likelihood_proposal\n",
    "                prior_likelihood = prior_prop\n",
    "                w = w_proposal\n",
    "                eta = eta_pro\n",
    "                rmse_train[i + 1,] = rmsetrain\n",
    "                rmse_test[i + 1,] = rmsetest\n",
    "\n",
    "                print (i, likelihood, prior_likelihood, rmsetrain, rmsetest, ' accepted')\n",
    "\n",
    "                pos_w[i + 1,] = w_proposal\n",
    "                pos_tau[i + 1,] = tau_pro\n",
    "                fxtrain_samples[i + 1,] = pred_train\n",
    "                fxtest_samples[i + 1,] = pred_test \n",
    "\n",
    "            else:\n",
    "                pos_w[i + 1,] = pos_w[i,]\n",
    "                pos_tau[i + 1,] = pos_tau[i,]\n",
    "                fxtrain_samples[i + 1,] = fxtrain_samples[i,]\n",
    "                fxtest_samples[i + 1,] = fxtest_samples[i,] \n",
    "                rmse_train[i + 1,] = rmse_train[i,]\n",
    "                rmse_test[i + 1,] = rmse_test[i,]\n",
    "\n",
    "        accept_ratio = naccept / (samples * 1.0) * 100\n",
    "\n",
    "        print(accept_ratio, '% was accepted')\n",
    "\n",
    "        burnin = 0.25 * samples  # use post burn in samples\n",
    "\n",
    "        pos_w = pos_w[int(burnin):, ]\n",
    "        pos_tau = pos_tau[int(burnin):, ] \n",
    "        rmse_train = rmse_train[int(burnin):]\n",
    "        rmse_test = rmse_test[int(burnin):]\n",
    "\n",
    "        rmse_tr = np.mean(rmse_train)\n",
    "        rmsetr_std = np.std(rmse_train)\n",
    "        rmse_tes = np.mean(rmse_test)\n",
    "        rmsetest_std = np.std(rmse_test)\n",
    "        print(rmse_tr, rmsetr_std, rmse_tes, rmsetest_std, ' rmse_tr, rmsetr_std, rmse_tes, rmsetest_std')\n",
    "\n",
    "        # let us next test the Bayesian model using the posterior distributions over n trials\n",
    "\n",
    "        num_trials = 10\n",
    "\n",
    "        accuracy = np.zeros(num_trials)\n",
    "\n",
    "        for i in range(num_trials):\n",
    "            #print(pos_w.mean(axis=0), pos_w.std(axis=0), ' pos w mean, pos w std')\n",
    "            w_drawn = np.random.normal(pos_w.mean(axis=0), pos_w.std(axis=0), w_size)\n",
    "            tausq_drawn = np.random.normal(pos_tau.mean(), pos_tau.std())\n",
    "            # a buf is present here - gives negative values at times\n",
    "\n",
    "            [loss, fx_, accuracy[i]] = self.likelihood_func(model, self.testdata, w_drawn, tausq_drawn)\n",
    "\n",
    "            print(i, loss, accuracy[i], tausq_drawn, pos_tau.mean(), pos_tau.std(), ' posterior test ')\n",
    "\n",
    "        print(accuracy.mean(), accuracy.std(), ' is mean and std of accuracy rmse test')\n",
    "\n",
    "        return (pos_w, pos_tau, fxtrain_samples, fxtest_samples, rmse_train, rmse_test, accept_ratio)\n",
    "    \n",
    "    \n",
    "def histogram_trace(pos_points, fname):    # this is function to plot (not part of class)\n",
    "    \n",
    "    size = 15\n",
    "\n",
    "    plt.tick_params(labelsize=size)\n",
    "    params = {'legend.fontsize': size, 'legend.handlelength': 2}\n",
    "    plt.rcParams.update(params)\n",
    "    plt.grid(alpha=0.75)\n",
    "\n",
    "    plt.hist(pos_points, bins = 20, color='#0504aa', alpha=0.7)   \n",
    "    plt.title(\"Posterior distribution \", fontsize = size)\n",
    "    plt.xlabel('Parameter value', fontsize = size)\n",
    "    plt.ylabel('Frequency', fontsize = size) \n",
    "    plt.tight_layout()  \n",
    "    plt.savefig(fname + '_posterior.png')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    plt.tick_params(labelsize=size)\n",
    "    params = {'legend.fontsize': size, 'legend.handlelength': 2}\n",
    "    plt.rcParams.update(params)\n",
    "    plt.grid(alpha=0.75) \n",
    "    plt.plot(pos_points)\n",
    "    plt.title(\"Parameter trace plot\", fontsize = size)\n",
    "    plt.xlabel('Number of Samples', fontsize = size)\n",
    "    plt.ylabel('Parameter value', fontsize = size)\n",
    "    plt.tight_layout()  \n",
    "    plt.savefig(fname + '_trace.png')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    \n",
    "def main():\n",
    "    \n",
    "    outres = open('datasets/results2.txt', 'w')\n",
    "    \n",
    "    for problem in range(2, 3):\n",
    "        \n",
    "        if problem == 1:    # Single step ahead prediction\n",
    "            traindata = np.loadtxt(\"datasets/Sunspot/train.txt\")\n",
    "            testdata = np.loadtxt(\"datasets/Sunspot/test.txt\")\n",
    "            features = 4\n",
    "            output = 1\n",
    "            activation = False     # true for sigmoid, false for linear\n",
    "\n",
    "        if problem == 2:\n",
    "            # Multi-step ahead prediction\n",
    "            # MMM stock market - 5 steps ahead predicton for closing stock price\n",
    "            # https://au.finance.yahoo.com/quote/mmm/\n",
    "            traindata = np.loadtxt(\"datasets/Stockmarket/train.txt\")\n",
    "            testdata = np.loadtxt(\"datasets/Stockmarket/test.txt\")\n",
    "            features = 5\n",
    "            output = 5\n",
    "            activation = False   # true for sigmoid, false for linear\n",
    "\n",
    "        print(traindata)\n",
    "\n",
    "        topology = [features, output]\n",
    "\n",
    "        model = lin_model(500, traindata, testdata, topology[0], 0.1, activation) \n",
    "\n",
    "        train_perc, test_perc, rmse_train, rmse_test = model.SGD()\n",
    "\n",
    "        print(train_perc, test_perc, rmse_train, rmse_test, ' train_perc, test_perc, rmse_train, rmse_test using SGD')\n",
    "\n",
    "        #--------------------------------------------------\n",
    "\n",
    "        MinCriteria = 0.005  # stop when RMSE reaches MinCriteria (problem dependent)\n",
    "\n",
    "        numSamples = 5000    # need to decide yourself\n",
    "\n",
    "        mcmc = MCMC(numSamples, traindata, testdata, topology, activation)  # declare class\n",
    "\n",
    "        [pos_w, pos_tau, fx_train, fx_test, rmse_train, rmse_test, accept_ratio] = mcmc.sampler()\n",
    "\n",
    "        fx_mu = fx_test.mean(axis=0)\n",
    "        fx_high = np.percentile(fx_test, 95, axis=0)\n",
    "        fx_low = np.percentile(fx_test, 5, axis=0)\n",
    "        fx_mu_tr = fx_train.mean(axis=0)\n",
    "        fx_high_tr = np.percentile(fx_train, 95, axis=0)\n",
    "        fx_low_tr = np.percentile(fx_train, 5, axis=0)\n",
    "\n",
    "        rmse_tr = np.mean(rmse_train)\n",
    "        rmsetr_std = np.std(rmse_train)\n",
    "        rmse_tes = np.mean(rmse_test)\n",
    "        rmsetest_std = np.std(rmse_test)\n",
    "\n",
    "        np.savetxt(outres, (rmse_tr, rmsetr_std, rmse_tes, rmsetest_std, accept_ratio), fmt='%1.5f')\n",
    "\n",
    "        ytestdata = testdata[:, features]\n",
    "        ytraindata = traindata[:, features]\n",
    "        x_test = np.linspace(0, 1, num=testdata.shape[0])\n",
    "        x_train = np.linspace(0, 1, num=traindata.shape[0])\n",
    "\n",
    "        '''plt.plot(x_test, ytestdata, label='actual')\n",
    "        plt.plot(x_test, fx_mu, label='pred.(mean)')\n",
    "        plt.plot(x_test, fx_low, label='pred.(5th percen.)')\n",
    "        plt.plot(x_test, fx_high, label='pred.(95th percen.)')\n",
    "        plt.fill_between(x_test, fx_low, fx_high, facecolor='g', alpha=0.4)\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.title(\"Test Data Uncertainty \")\n",
    "        plt.savefig('figures/mcmctest.png')\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "        \n",
    "        # -----------------------------------------\n",
    "        \n",
    "        plt.plot(x_train, ytraindata, label='actual')\n",
    "        plt.plot(x_train, fx_mu_tr, label='pred.(mean)')\n",
    "        plt.plot(x_train, fx_low_tr, label='pred.(5th percen.)')\n",
    "        plt.plot(x_train, fx_high_tr, label='pred.(95th percen.)')\n",
    "        plt.fill_between(x_train, fx_low_tr, fx_high_tr, facecolor='g', alpha=0.4)\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.title(\"Train Data Uncertainty \")\n",
    "        plt.savefig('figures/mcmctrain.png')\n",
    "        plt.show()\n",
    "        plt.clf()'''\n",
    "\n",
    "        mpl_fig = plt.figure()\n",
    "        ax = mpl_fig.add_subplot(111)\n",
    "        ax.boxplot(pos_w)\n",
    "        ax.set_xlabel('[w0] [w1] [w3] [b]')\n",
    "        ax.set_ylabel('Posterior')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.title(\"Posterior\")\n",
    "        plt.savefig('figures/w_pos.png')\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "\n",
    "        import os\n",
    "        folder = 'figures'\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "\n",
    "        #for i in range(pos_w.shape[1]):\n",
    "\n",
    "        for i in range(5):\n",
    "            histogram_trace(pos_w[:,i], folder+'/'+str(i))\n",
    "            \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f286add",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
