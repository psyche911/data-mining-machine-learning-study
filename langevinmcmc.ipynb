{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2504891c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !/usr/bin/python\n",
    "\n",
    "# MCMC Random# !/usr/bin/python\n",
    "# MCMC Random Walk for Feedforward Neural Network for One-Step-Ahead Chaotic Time Series Prediction\n",
    "\n",
    "# Data (Sunspot and Lazer). Taken' Theorem used for Data Reconstruction (Dimension = 4, Timelag = 2).\n",
    "# Data procesing file is included.\n",
    "\n",
    "# RMSE (Root Mean Squared Error)\n",
    "\n",
    "# based on: https://github.com/rohitash-chandra/FNN_TimeSeries\n",
    "# based on: https://github.com/rohitash-chandra/mcmc-randomwalk\n",
    "\n",
    "# Rohitash Chandra, Centre for Translational Data Science\n",
    "# University of Sydey, Sydney NSW, Australia.  2017 c.rohitash@gmail.conm\n",
    "# https://www.researchgate.net/profile/Rohitash_Chandra\n",
    "\n",
    "# Reference for publication for this code\n",
    "# [Chandra_ICONIP2017] R. Chandra, L. Azizi, S. Cripps, \n",
    "# 'Bayesian neural learning via Langevin dynamicsfor chaotic time series prediction', ICONIP 2017.\n",
    "# (to be addeded on https://www.researchgate.net/profile/Rohitash_Chandra)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.stats import norm\n",
    "import math\n",
    "import os\n",
    "\n",
    "# An example of a class\n",
    "class Network:\n",
    "    def __init__(self, Topo, Train, Test, learn_rate):\n",
    "        self.Top = Topo    # NN topology [input, hidden, output]\n",
    "        self.TrainData = Train\n",
    "        self.TestData = Test\n",
    "        np.random.seed()\n",
    "        self.lrate = learn_rate\n",
    "\n",
    "        self.W1 = np.random.randn(self.Top[0], self.Top[1]) / np.sqrt(self.Top[0])\n",
    "        self.B1 = np.random.randn(1, self.Top[1]) / np.sqrt(self.Top[1])    # bias first layer\n",
    "        self.W2 = np.random.randn(self.Top[1], self.Top[2]) / np.sqrt(self.Top[1])\n",
    "        self.B2 = np.random.randn(1, self.Top[2]) / np.sqrt(self.Top[1])    # bias second layer\n",
    "\n",
    "        self.hidout = np.zeros((1, self.Top[1]))  # output of first hidden layer\n",
    "        self.out = np.zeros((1, self.Top[2]))     # output last layer\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sampleEr(self, actualout):\n",
    "        error = np.subtract(self.out, actualout)\n",
    "        sqerror = np.sum(np.square(error)) / self.Top[2]\n",
    "        return sqerror\n",
    "\n",
    "    def ForwardPass(self, X):\n",
    "        z1 = X.dot(self.W1) - self.B1\n",
    "        self.hidout = self.sigmoid(z1)  # output of first hidden layer\n",
    "        z2 = self.hidout.dot(self.W2) - self.B2\n",
    "        self.out = self.sigmoid(z2)     # output second hidden layer\n",
    "\n",
    "    def BackwardPass(self, Input, desired):\n",
    "        out_delta = (desired - self.out) * (self.out * (1 - self.out))\n",
    "        hid_delta = out_delta.dot(self.W2.T) * (self.hidout * (1 - self.hidout))\n",
    "\n",
    "        #self.W2 += (self.hidout.T.dot(out_delta) * self.lrate)\n",
    "        #self.B2 += (-1 * self.lrate * out_delta)\n",
    "        #self.W1 += (Input.T.dot(hid_delta) * self.lrate)\n",
    "        #self.B1 += (-1 * self.lrate * hid_delta)\n",
    "\n",
    "        layer = 1  # hidden to output        \n",
    "        for x in range(0, self.Top[layer]):\n",
    "            for y in range(0, self.Top[layer + 1]):\n",
    "                self.W2[x, y] += self.lrate * out_delta[y] * self.hidout[x]                \n",
    "        for y in range(0, self.Top[layer + 1]):\n",
    "            self.B2[y] += -1 * self.lrate * out_delta[y]\n",
    "\n",
    "        layer = 0  # Input to Hidden\n",
    "        for x in range(0, self.Top[layer]):\n",
    "            for y in range(0, self.Top[layer + 1]):\n",
    "                self.W1[x, y] += self.lrate * hid_delta[y] * Input[x]\n",
    "        for y in range(0, self.Top[layer + 1]):\n",
    "            self.B1[y] += -1 * self.lrate * hid_delta[y]\n",
    "\n",
    "    def decode(self, w):\n",
    "        w_layer1size = self.Top[0] * self.Top[1]\n",
    "        w_layer2size = self.Top[1] * self.Top[2]\n",
    "\n",
    "        w_layer1 = w[0:w_layer1size]\n",
    "        self.W1 = np.reshape(w_layer1, (self.Top[0], self.Top[1]))\n",
    "\n",
    "        w_layer2 = w[w_layer1size:w_layer1size + w_layer2size]\n",
    "        self.W2 = np.reshape(w_layer2, (self.Top[1], self.Top[2]))\n",
    "        self.B1 = w[w_layer1size + w_layer2size:w_layer1size + w_layer2size + self.Top[1]]\n",
    "        self.B2 = w[w_layer1size + w_layer2size + self.Top[1]:w_layer1size + w_layer2size + self.Top[1] + self.Top[2]]\n",
    "\n",
    "    def encode(self):\n",
    "        w1 = self.W1.ravel()\n",
    "        w2 = self.W2.ravel()\n",
    "        w = np.concatenate([w1, w2, self.B1, self.B2])\n",
    "        return w\n",
    "\n",
    "    def langevin_gradient(self, data, w, depth):  # BP with SGD (Stocastic BP)\n",
    "\n",
    "        self.decode(w)      # method to decode w into W1, W2, B1, B2.\n",
    "        size = data.shape[0]\n",
    "\n",
    "        Input = np.zeros((1, self.Top[0]))  # temp hold input\n",
    "        Desired = np.zeros((1, self.Top[2]))\n",
    "        fx = np.zeros(size)\n",
    "\n",
    "        for i in range(0, depth):\n",
    "            for i in range(0, size):\n",
    "                pat = i\n",
    "                Input = data[pat, 0:self.Top[0]]\n",
    "                Desired = data[pat, self.Top[0]:]\n",
    "                self.ForwardPass(Input)\n",
    "                self.BackwardPass(Input, Desired)\n",
    "\n",
    "        w_updated = self.encode()\n",
    "\n",
    "        return  w_updated\n",
    "\n",
    "    def evaluate_proposal(self, data, w):  # BP with SGD (Stocastic BP)\n",
    "\n",
    "        self.decode(w)    # method to decode w into W1, W2, B1, B2.\n",
    "        size = data.shape[0]\n",
    "\n",
    "        Input = np.zeros((1, self.Top[0]))  # temp hold input\n",
    "        Desired = np.zeros((1, self.Top[2]))\n",
    "        fx = np.zeros(size)\n",
    "\n",
    "        for i in range(0, size):    # to see what fx is produced by your current weight update\n",
    "            Input = data[i, 0:self.Top[0]]\n",
    "            self.ForwardPass(Input)\n",
    "            fx[i] = self.out\n",
    "\n",
    "        return fx\n",
    "    \n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "class MCMC:\n",
    "    \n",
    "    def __init__(self, use_langevin_gradients, l_prob, learn_rate, samples, traindata, testdata, topology):\n",
    "        self.samples = samples    # NN topology [input, hidden, output]\n",
    "        self.topology = topology  # max epocs\n",
    "        self.traindata = traindata\n",
    "        self.testdata = testdata \n",
    "        self.use_langevin_gradients = use_langevin_gradients \n",
    "\n",
    "        self.l_prob = l_prob    # likelihood prob\n",
    "\n",
    "        self.learn_rate = learn_rate        \n",
    "\n",
    "    def rmse(self, predictions, targets):\n",
    "        return np.sqrt(((predictions - targets) ** 2).mean())\n",
    "\n",
    "    def likelihood_func(self, neuralnet, data, w, tausq):\n",
    "        y = data[:, self.topology[0]]\n",
    "        fx = neuralnet.evaluate_proposal(data, w)\n",
    "        rmse = self.rmse(fx, y)\n",
    "        #loss = -0.5 * np.log(2 * math.pi * tausq) - 0.5 * np.square(y - fx) / tausq\n",
    "\n",
    "        n = y.shape[0]\n",
    "\n",
    "        loss =(-(n/2) * np.log(2 * math.pi * tausq)) - ((1/(2*tausq)) * np.sum(np.square(y - fx)))\n",
    "        return [loss, fx, rmse]\n",
    "\n",
    "    def prior_likelihood(self, sigma_squared, nu_1, nu_2, w, tausq):\n",
    "        h = self.topology[1]    # number hidden neurons\n",
    "        d = self.topology[0]    # number input neurons\n",
    "        part1 = -1 * ((d * h + h + 2) / 2) * np.log(sigma_squared)\n",
    "        part2 = 1 / (2 * sigma_squared) * (sum(np.square(w)))\n",
    "        log_loss = part1 - part2 - (1 + nu_1) * np.log(tausq) - (nu_2 / tausq)\n",
    "        return log_loss\n",
    "\n",
    "    def sampler(self, w_limit, tau_limit):\n",
    "        # Initialize MCMC\n",
    "        testsize = self.testdata.shape[0]\n",
    "        trainsize = self.traindata.shape[0]\n",
    "        samples = self.samples\n",
    "\n",
    "        self.sgd_depth = 1\n",
    "\n",
    "        x_test = np.linspace(0, 1, num=testsize)\n",
    "        x_train = np.linspace(0, 1, num=trainsize)\n",
    "\n",
    "        netw = self.topology     # [input, hidden, output]\n",
    "        y_test = self.testdata[:, netw[0]]\n",
    "        y_train = self.traindata[:, netw[0]]\n",
    "        print(y_train.size)\n",
    "        print(y_test.size)\n",
    "\n",
    "        w_size = (netw[0] * netw[1]) + (netw[1] * netw[2]) + netw[1] + netw[2]  # num of weights and bias\n",
    "\n",
    "        pos_w = np.ones((samples, w_size))  # posterior of all weights and bias over all samples\n",
    "        pos_tau = np.ones((samples, 1))\n",
    "\n",
    "        fxtrain_samples = np.ones((samples, trainsize))  # fx of train data over all samples\n",
    "        fxtest_samples = np.ones((samples, testsize))    # fx of test data over all samples\n",
    "        rmse_train = np.zeros(samples)\n",
    "        rmse_test = np.zeros(samples)\n",
    "\n",
    "        w = np.random.randn(w_size)\n",
    "        w_proposal = np.random.randn(w_size)\n",
    "\n",
    "        #step_w = 0.05   # defines how much variation you need in changes to w\n",
    "        #step_eta = 0.2  # exp 0\n",
    "\n",
    "        step_w = w_limit      # defines how much variation you need in changes to w\n",
    "        step_eta = tau_limit  # exp 1\n",
    "        \n",
    "        # Declare FNN and initialize\n",
    "        neuralnet = Network(self.topology, self.traindata, self.testdata, self.learn_rate)\n",
    "        print('evaluate Initial w')\n",
    "\n",
    "        pred_train = neuralnet.evaluate_proposal(self.traindata, w)\n",
    "        pred_test = neuralnet.evaluate_proposal(self.testdata, w)\n",
    "\n",
    "        eta = np.log(np.var(pred_train - y_train))\n",
    "        tau_pro = np.exp(eta)\n",
    "\n",
    "        sigma_squared = 25\n",
    "        nu_1 = 0\n",
    "        nu_2 = 0\n",
    "\n",
    "        sigma_diagmat = np.zeros((w_size, w_size))  # for Equation 9 in Ref [Chandra_ICONIP2017]\n",
    "        np.fill_diagonal(sigma_diagmat, step_w)\n",
    "\n",
    "        delta_likelihood = 0.5 # an arbitrary position\n",
    "        \n",
    "        prior_current = self.prior_likelihood(sigma_squared, nu_1, nu_2, w, tau_pro)  # takes care of the gradients\n",
    "\n",
    "        [likelihood, pred_train, rmsetrain] = self.likelihood_func(neuralnet, self.traindata, w, tau_pro)\n",
    "        [likelihood_ignore, pred_test, rmsetest] = self.likelihood_func(neuralnet, self.testdata, w, tau_pro)\n",
    "\n",
    "        print(likelihood, ' Initial likelihood')\n",
    "\n",
    "        naccept = 0\n",
    "\n",
    "        langevin_count = 0\n",
    "\n",
    "        for i in range(samples - 1):\n",
    "\n",
    "            lx = np.random.uniform(0,1,1)\n",
    "\n",
    "            if (self.use_langevin_gradients is True) and (lx< self.l_prob):  \n",
    "                w_gd = neuralnet.langevin_gradient(self.traindata, w.copy(), self.sgd_depth)   # Eq 8\n",
    "                w_proposal = np.random.normal(w_gd, step_w, w_size)    # Eq 7\n",
    "                w_prop_gd = neuralnet.langevin_gradient(self.traindata, w_proposal.copy(), self.sgd_depth) \n",
    "                # first = np.log(multivariate_normal.pdf(w, w_prop_gd, sigma_diagmat)) \n",
    "                # second = np.log(multivariate_normal.pdf(w_proposal, w_gd , sigma_diagmat)) \n",
    "                # this gives numerical instability - hence we give a simple implementation next that takes out log \n",
    "\n",
    "                wc_delta = (w- w_prop_gd) \n",
    "                wp_delta = (w_proposal - w_gd)\n",
    "\n",
    "                sigma_sq = step_w * step_w\n",
    "\n",
    "                first = -0.5 * np.sum(wc_delta  *  wc_delta) / sigma_sq  # this is wc_delta.T * wc_delta /sigma_sq\n",
    "                second = -0.5 * np.sum(wp_delta * wp_delta) / sigma_sq\n",
    "\n",
    "                diff_prop = first - second  \n",
    "                langevin_count += 1\n",
    "\n",
    "            else:\n",
    "                diff_prop = 0\n",
    "                w_proposal = np.random.normal(w, step_w, w_size)\n",
    "\n",
    "            eta_pro = eta + np.random.normal(0, step_eta, 1)\n",
    "            tau_pro = math.exp(eta_pro)\n",
    "\n",
    "            [likelihood_proposal, pred_train, rmsetrain] = self.likelihood_func(neuralnet, self.traindata, \n",
    "                                                                                w_proposal, tau_pro)\n",
    "            \n",
    "            [likelihood_ignore, pred_test, rmsetest] = self.likelihood_func(neuralnet, self.testdata, \n",
    "                                                                            w_proposal, tau_pro) \n",
    "\n",
    "            prior_prop = self.prior_likelihood(sigma_squared, nu_1, nu_2, w_proposal, tau_pro)\n",
    "            # takes care of the gradients\n",
    "\n",
    "            diff_prior = prior_prop - prior_current\n",
    "\n",
    "            diff_likelihood = likelihood_proposal - likelihood\n",
    "\n",
    "            #mh_prob = min(1, math.exp(diff_likelihood + diff_prior + diff_prop))\n",
    "\n",
    "            try:\n",
    "                mh_prob = min(1, math.exp(diff_likelihood+diff_prior+ diff_prop))\n",
    "\n",
    "            except OverflowError as e:\n",
    "                mh_prob = 1\n",
    "\n",
    "            u = random.uniform(0, 1)\n",
    "\n",
    "            if u < mh_prob:\n",
    "                # Update position \n",
    "                naccept += 1\n",
    "                likelihood = likelihood_proposal\n",
    "                prior_current = prior_prop\n",
    "                w = w_proposal\n",
    "                eta = eta_pro\n",
    "                if i%10 ==0:\n",
    "                    print(i,likelihood, prior_current, diff_prop, rmsetrain, rmsetest, 'accepted')\n",
    "\n",
    "                pos_w[i + 1,] = w_proposal\n",
    "                pos_tau[i + 1,] = tau_pro\n",
    "                fxtrain_samples[i + 1,] = pred_train\n",
    "                fxtest_samples[i + 1,] = pred_test\n",
    "                rmse_train[i + 1,] = rmsetrain\n",
    "                rmse_test[i + 1,] = rmsetest\n",
    "\n",
    "                plt.plot(x_train, pred_train)\n",
    "\n",
    "            else:\n",
    "                pos_w[i + 1,] = pos_w[i,]\n",
    "                pos_tau[i + 1,] = pos_tau[i,]\n",
    "                fxtrain_samples[i + 1,] = fxtrain_samples[i,]\n",
    "                fxtest_samples[i + 1,] = fxtest_samples[i,]\n",
    "                rmse_train[i + 1,] = rmse_train[i,]\n",
    "                rmse_test[i + 1,] = rmse_test[i,]\n",
    "\n",
    "        print(naccept, ' num accepted')\n",
    "        print(naccept / (samples * 1.0), '% was accepted')\n",
    "        \n",
    "        accept_ratio = naccept / (samples * 1.0) * 100\n",
    "\n",
    "        print(langevin_count, ' langevin_count')\n",
    "\n",
    "        return (pos_w, pos_tau, fxtrain_samples, fxtest_samples, x_train, x_test, rmse_train, rmse_test, accept_ratio)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    for problem in range(2, 3): \n",
    "        hidden = 5\n",
    "        input = 4\n",
    "        output = 1\n",
    "\n",
    "        w_limit = 0.025  # step size for w\n",
    "        tau_limit = 0.2  # step size for eta\n",
    "\n",
    "        if problem == 1:\n",
    "            traindata = np.loadtxt(\"datasets/Lazer/train.txt\")\n",
    "            testdata = np.loadtxt(\"datasets/Lazer/test.txt\")\n",
    "            name = \"Lazer\"\n",
    "        if problem == 2:\n",
    "            traindata = np.loadtxt(\"datasets/Sunspot/train.txt\")\n",
    "            testdata = np.loadtxt(\"datasets/Sunspot/test.txt\")\n",
    "            name = \"Sunspot\"\n",
    "        if problem == 3:\n",
    "            traindata = np.loadtxt(\"datasets/Mackey/train.txt\")\n",
    "            testdata = np.loadtxt(\"datasets/Mackey/test.txt\")\n",
    "            name = \"Mackey\"\n",
    "\n",
    "        topology = [input, hidden, output]\n",
    "        random.seed(time.time())\n",
    "        numSamples = 5000      # need to decide yourself\n",
    "        use_langevin_gradients = True\n",
    "\n",
    "        l_prob = 0.5\n",
    "        learn_rate = 0.01\n",
    "\n",
    "        timer = time.time() \n",
    "        mcmc = MCMC(use_langevin_gradients, l_prob, learn_rate, numSamples, traindata, testdata, topology)\n",
    "        # declare class\n",
    "\n",
    "        [pos_w,pos_tau,fx_train,fx_test,x_train,x_test,rmse_train,rmse_test,accept_ratio]= mcmc.sampler(w_limit,tau_limit)\n",
    "        print('sucessfully sampled')\n",
    "\n",
    "        burnin = 0.5 * numSamples  # use post burn in samples\n",
    "\n",
    "        timer2 = time.time()\n",
    "\n",
    "        timetotal = (timer2 - timer) /60\n",
    "        print((timetotal), 'min taken')\n",
    "\n",
    "        pos_w = pos_w[int(burnin):, ]\n",
    "        pos_tau = pos_tau[int(burnin):, ]\n",
    "\n",
    "        fx_mu = fx_test.mean(axis=0)\n",
    "        fx_high = np.percentile(fx_test, 95, axis=0)\n",
    "        fx_low = np.percentile(fx_test, 5, axis=0)\n",
    "\n",
    "        fx_mu_tr = fx_train.mean(axis=0)\n",
    "        fx_high_tr = np.percentile(fx_train, 95, axis=0)\n",
    "        fx_low_tr = np.percentile(fx_train, 5, axis=0)\n",
    "\n",
    "        pos_w_mean = pos_w.mean(axis=0) \n",
    "\n",
    "        rmse_tr = np.mean(rmse_train[int(burnin):])\n",
    "        rmsetr_std = np.std(rmse_train[int(burnin):])\n",
    "        rmse_tes = np.mean(rmse_test[int(burnin):])\n",
    "        rmsetest_std = np.std(rmse_test[int(burnin):])\n",
    "        print(rmse_tr, rmsetr_std, rmse_tes, rmsetest_std)\n",
    "\n",
    "        outres_db = open('datasets/result.txt', \"a+\")\n",
    "\n",
    "        np.savetxt(outres_db, (use_langevin_gradients, learn_rate, rmse_tr, rmsetr_std, \n",
    "                               rmse_tes, rmsetest_std, accept_ratio, timetotal), fmt='%1.5f')\n",
    "\n",
    "        ytestdata = testdata[:, input]\n",
    "        ytraindata = traindata[:, input]\n",
    "\n",
    "        plt.plot(x_test, ytestdata, label='actual')\n",
    "        plt.plot(x_test, fx_mu, label='pred. (mean)')\n",
    "        plt.plot(x_test, fx_low, label='pred.(5th percen.)')\n",
    "        plt.plot(x_test, fx_high, label='pred.(95th percen.)')\n",
    "        plt.fill_between(x_test, fx_low, fx_high, facecolor='g', alpha=0.4)\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.title(\"Prediction  Uncertainty \")\n",
    "        plt.savefig('figures/mcmcrestest.png')\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "        \n",
    "        # -----------------------------------------\n",
    "        plt.plot(x_train, ytraindata, label='actual')\n",
    "        plt.plot(x_train, fx_mu_tr, label='pred. (mean)')\n",
    "        plt.plot(x_train, fx_low_tr, label='pred.(5th percen.)')\n",
    "        plt.plot(x_train, fx_high_tr, label='pred.(95th percen.)')\n",
    "        plt.fill_between(x_train, fx_low_tr, fx_high_tr, facecolor='g', alpha=0.4)\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.title(\"Prediction  Uncertainty\")\n",
    "        plt.savefig('figures/mcmcrestrain.png') \n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "\n",
    "        mpl_fig = plt.figure()\n",
    "        ax = mpl_fig.add_subplot(111)\n",
    "        ax.boxplot(pos_w)\n",
    "        ax.set_xlabel('[W1] [B1] [W2] [B2]')\n",
    "        ax.set_ylabel('Posterior')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.title(\"Boxplot of Posterior W (weights and biases)\")\n",
    "        plt.savefig('figures/w_pos.png')\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    main()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
